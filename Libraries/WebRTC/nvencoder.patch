diff --git a/report.xml b/report.xml
new file mode 100644
index 0000000..434f184
--- /dev/null
+++ b/report.xml
@@ -0,0 +1,16 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<testsuites tests="7" failures="0" disabled="0" errors="0" timestamp="2017-05-15T15:03:55" time="0.132" name="AllTests">
+  <testsuite name="H264SpropParameterSetsTest" tests="2" failures="0" disabled="0" errors="0" time="0.002">
+    <testcase name="Base64DecodeSprop" status="run" time="0" classname="H264SpropParameterSetsTest" />
+    <testcase name="InvalidData" status="run" time="0.001" classname="H264SpropParameterSetsTest" />
+  </testsuite>
+  <testsuite name="H264EncoderImplTest" tests="4" failures="0" disabled="0" errors="0" time="0.04">
+    <testcase name="CanInitializeWithDefaultParameters" status="run" time="0.01" classname="H264EncoderImplTest" />
+    <testcase name="CanInitializeWithNonInterleavedModeExplicitly" status="run" time="0.008" classname="H264EncoderImplTest" />
+    <testcase name="CanInitializeWithSingleNalUnitModeExplicitly" status="run" time="0.009" classname="H264EncoderImplTest" />
+    <testcase name="CanInitializeWithRemovedParameter" status="run" time="0.009" classname="H264EncoderImplTest" />
+  </testsuite>
+  <testsuite name="H264TestImpl" tests="1" failures="0" disabled="0" errors="0" time="0.088">
+    <testcase name="SoftwareEncodeDecode" status="run" time="0.086" classname="H264TestImpl" />
+  </testsuite>
+</testsuites>
diff --git a/webrtc/api/video/i420_buffer.cc b/webrtc/api/video/i420_buffer.cc
index 031b159..9ee100c 100644
--- a/webrtc/api/video/i420_buffer.cc
+++ b/webrtc/api/video/i420_buffer.cc
@@ -1,260 +1,261 @@
-/*
- *  Copyright (c) 2015 The WebRTC project authors. All Rights Reserved.
- *
- *  Use of this source code is governed by a BSD-style license
- *  that can be found in the LICENSE file in the root of the source
- *  tree. An additional intellectual property rights grant can be found
- *  in the file PATENTS.  All contributing project authors may
- *  be found in the AUTHORS file in the root of the source tree.
- */
-#include "webrtc/api/video/i420_buffer.h"
-
-#include <string.h>
-
-#include <algorithm>
-#include <utility>
-
-#include "webrtc/base/checks.h"
-#include "webrtc/base/keep_ref_until_done.h"
-#include "libyuv/convert.h"
-#include "libyuv/planar_functions.h"
-#include "libyuv/scale.h"
-
-// Aligning pointer to 64 bytes for improved performance, e.g. use SIMD.
-static const int kBufferAlignment = 64;
-
-namespace webrtc {
-
-namespace {
-
-int I420DataSize(int height, int stride_y, int stride_u, int stride_v) {
-  return stride_y * height + (stride_u + stride_v) * ((height + 1) / 2);
-}
-
-}  // namespace
-
-I420Buffer::I420Buffer(int width, int height)
-    : I420Buffer(width, height, width, (width + 1) / 2, (width + 1) / 2) {
-}
-
-I420Buffer::I420Buffer(int width,
-                       int height,
-                       int stride_y,
-                       int stride_u,
-                       int stride_v)
-    : width_(width),
-      height_(height),
-      stride_y_(stride_y),
-      stride_u_(stride_u),
-      stride_v_(stride_v),
-      data_(static_cast<uint8_t*>(AlignedMalloc(
-          I420DataSize(height, stride_y, stride_u, stride_v),
-          kBufferAlignment))) {
-  RTC_DCHECK_GT(width, 0);
-  RTC_DCHECK_GT(height, 0);
-  RTC_DCHECK_GE(stride_y, width);
-  RTC_DCHECK_GE(stride_u, (width + 1) / 2);
-  RTC_DCHECK_GE(stride_v, (width + 1) / 2);
-}
-
-I420Buffer::~I420Buffer() {
-}
-
-// static
-rtc::scoped_refptr<I420Buffer> I420Buffer::Create(int width, int height) {
-  return new rtc::RefCountedObject<I420Buffer>(width, height);
-}
-
-// static
-rtc::scoped_refptr<I420Buffer> I420Buffer::Create(int width,
-                                                  int height,
-                                                  int stride_y,
-                                                  int stride_u,
-                                                  int stride_v) {
-  return new rtc::RefCountedObject<I420Buffer>(
-      width, height, stride_y, stride_u, stride_v);
-}
-
-// static
-rtc::scoped_refptr<I420Buffer> I420Buffer::Copy(
-    const VideoFrameBuffer& source) {
-  return Copy(source.width(), source.height(),
-              source.DataY(), source.StrideY(),
-              source.DataU(), source.StrideU(),
-              source.DataV(), source.StrideV());
-}
-
-// static
-rtc::scoped_refptr<I420Buffer> I420Buffer::Copy(
-      int width, int height,
-      const uint8_t* data_y, int stride_y,
-      const uint8_t* data_u, int stride_u,
-      const uint8_t* data_v, int stride_v) {
-  // Note: May use different strides than the input data.
-  rtc::scoped_refptr<I420Buffer> buffer = Create(width, height);
-  RTC_CHECK_EQ(0, libyuv::I420Copy(data_y, stride_y,
-                                   data_u, stride_u,
-                                   data_v, stride_v,
-                                   buffer->MutableDataY(), buffer->StrideY(),
-                                   buffer->MutableDataU(), buffer->StrideU(),
-                                   buffer->MutableDataV(), buffer->StrideV(),
-                                   width, height));
-  return buffer;
-}
-
-// static
-rtc::scoped_refptr<I420Buffer> I420Buffer::Rotate(
-    const VideoFrameBuffer& src, VideoRotation rotation) {
-  RTC_CHECK(src.DataY());
-  RTC_CHECK(src.DataU());
-  RTC_CHECK(src.DataV());
-
-  int rotated_width = src.width();
-  int rotated_height = src.height();
-  if (rotation == webrtc::kVideoRotation_90 ||
-      rotation == webrtc::kVideoRotation_270) {
-    std::swap(rotated_width, rotated_height);
-  }
-
-  rtc::scoped_refptr<webrtc::I420Buffer> buffer =
-      I420Buffer::Create(rotated_width, rotated_height);
-
-  RTC_CHECK_EQ(0, libyuv::I420Rotate(
-      src.DataY(), src.StrideY(),
-      src.DataU(), src.StrideU(),
-      src.DataV(), src.StrideV(),
-      buffer->MutableDataY(), buffer->StrideY(), buffer->MutableDataU(),
-      buffer->StrideU(), buffer->MutableDataV(), buffer->StrideV(),
-      src.width(), src.height(),
-      static_cast<libyuv::RotationMode>(rotation)));
-
-  return buffer;
-}
-
-// static
-rtc::scoped_refptr<VideoFrameBuffer> I420Buffer::Rotate(
-    rtc::scoped_refptr<VideoFrameBuffer> src,
-    VideoRotation rotation) {
-  if (rotation == webrtc::kVideoRotation_0) {
-    return src;
-  } else {
-    return Rotate(*src, rotation);
-  }
-}
-
-void I420Buffer::InitializeData() {
-  memset(data_.get(), 0,
-         I420DataSize(height_, stride_y_, stride_u_, stride_v_));
-}
-
-int I420Buffer::width() const {
-  return width_;
-}
-
-int I420Buffer::height() const {
-  return height_;
-}
-
-const uint8_t* I420Buffer::DataY() const {
-  return data_.get();
-}
-const uint8_t* I420Buffer::DataU() const {
-  return data_.get() + stride_y_ * height_;
-}
-const uint8_t* I420Buffer::DataV() const {
-  return data_.get() + stride_y_ * height_ + stride_u_ * ((height_ + 1) / 2);
-}
-
-int I420Buffer::StrideY() const {
-  return stride_y_;
-}
-int I420Buffer::StrideU() const {
-  return stride_u_;
-}
-int I420Buffer::StrideV() const {
-  return stride_v_;
-}
-
-void* I420Buffer::native_handle() const {
-  return nullptr;
-}
-
-rtc::scoped_refptr<VideoFrameBuffer> I420Buffer::NativeToI420Buffer() {
-  RTC_NOTREACHED();
-  return nullptr;
-}
-
-uint8_t* I420Buffer::MutableDataY() {
-  return const_cast<uint8_t*>(DataY());
-}
-uint8_t* I420Buffer::MutableDataU() {
-  return const_cast<uint8_t*>(DataU());
-}
-uint8_t* I420Buffer::MutableDataV() {
-  return const_cast<uint8_t*>(DataV());
-}
-
-// static
-void I420Buffer::SetBlack(I420Buffer* buffer) {
-  RTC_CHECK(libyuv::I420Rect(buffer->MutableDataY(), buffer->StrideY(),
-                             buffer->MutableDataU(), buffer->StrideU(),
-                             buffer->MutableDataV(), buffer->StrideV(),
-                             0, 0, buffer->width(), buffer->height(),
-                             0, 128, 128) == 0);
-}
-
-void I420Buffer::CropAndScaleFrom(
-    const VideoFrameBuffer& src,
-    int offset_x,
-    int offset_y,
-    int crop_width,
-    int crop_height) {
-  RTC_CHECK_LE(crop_width, src.width());
-  RTC_CHECK_LE(crop_height, src.height());
-  RTC_CHECK_LE(crop_width + offset_x, src.width());
-  RTC_CHECK_LE(crop_height + offset_y, src.height());
-  RTC_CHECK_GE(offset_x, 0);
-  RTC_CHECK_GE(offset_y, 0);
-
-  // Make sure offset is even so that u/v plane becomes aligned.
-  const int uv_offset_x = offset_x / 2;
-  const int uv_offset_y = offset_y / 2;
-  offset_x = uv_offset_x * 2;
-  offset_y = uv_offset_y * 2;
-
-  const uint8_t* y_plane =
-      src.DataY() + src.StrideY() * offset_y + offset_x;
-  const uint8_t* u_plane =
-      src.DataU() + src.StrideU() * uv_offset_y + uv_offset_x;
-  const uint8_t* v_plane =
-      src.DataV() + src.StrideV() * uv_offset_y + uv_offset_x;
-  int res = libyuv::I420Scale(y_plane, src.StrideY(),
-                              u_plane, src.StrideU(),
-                              v_plane, src.StrideV(),
-                              crop_width, crop_height,
-                              MutableDataY(), StrideY(),
-                              MutableDataU(), StrideU(),
-                              MutableDataV(), StrideV(),
-                              width(), height(), libyuv::kFilterBox);
-
-  RTC_DCHECK_EQ(res, 0);
-}
-
-void I420Buffer::CropAndScaleFrom(
-    const VideoFrameBuffer& src) {
-  const int crop_width =
-      std::min(src.width(), width() * src.height() / height());
-  const int crop_height =
-      std::min(src.height(), height() * src.width() / width());
-
-  CropAndScaleFrom(
-      src,
-      (src.width() - crop_width) / 2, (src.height() - crop_height) / 2,
-      crop_width, crop_height);
-}
-
-void I420Buffer::ScaleFrom(const VideoFrameBuffer& src) {
-  CropAndScaleFrom(src, 0, 0, src.width(), src.height());
-}
-
-}  // namespace webrtc
+/*
+ *  Copyright (c) 2015 The WebRTC project authors. All Rights Reserved.
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ */
+#include "webrtc/api/video/i420_buffer.h"
+
+#include <string.h>
+
+#include <algorithm>
+#include <utility>
+
+#include "webrtc/base/checks.h"
+#include "webrtc/base/keep_ref_until_done.h"
+#include "libyuv/convert.h"
+#include "libyuv/planar_functions.h"
+#include "libyuv/scale.h"
+
+// Aligning pointer to 64 bytes for improved performance, e.g. use SIMD.
+static const int kBufferAlignment = 64;
+
+namespace webrtc {
+
+namespace {
+
+int I420DataSize(int height, int stride_y, int stride_u, int stride_v) {
+  return stride_y * height + (stride_u + stride_v) * ((height + 1) / 2);
+}
+
+}  // namespace
+
+I420Buffer::I420Buffer(int width, int height)
+    : I420Buffer(width, height, width, (width + 1) / 2, (width + 1) / 2) {
+}
+
+
+I420Buffer::I420Buffer(int width,
+                       int height,
+                       int stride_y,
+                       int stride_u,
+                       int stride_v)
+    : width_(width),
+      height_(height),
+      stride_y_(stride_y),
+      stride_u_(stride_u),
+      stride_v_(stride_v),
+      data_(static_cast<uint8_t*>(AlignedMalloc(
+		  I420DataSize(height, stride_y, stride_u, stride_v),
+          kBufferAlignment))) {
+  RTC_DCHECK_GT(width, 0);
+  RTC_DCHECK_GT(height, 0);
+  RTC_DCHECK_GE(stride_y, width);
+  RTC_DCHECK_GE(stride_u, (width + 1) / 2);
+  RTC_DCHECK_GE(stride_v, (width + 1) / 2);
+}
+
+I420Buffer::~I420Buffer() {
+}
+
+// static
+rtc::scoped_refptr<I420Buffer> I420Buffer::Create(int width, int height) {
+  return new rtc::RefCountedObject<I420Buffer>(width, height);
+}
+
+// static
+rtc::scoped_refptr<I420Buffer> I420Buffer::Create(int width,
+                                                  int height,
+                                                  int stride_y,
+                                                  int stride_u,
+                                                  int stride_v) {
+  return new rtc::RefCountedObject<I420Buffer>(
+      width, height, stride_y, stride_u, stride_v);
+}
+
+// static
+rtc::scoped_refptr<I420Buffer> I420Buffer::Copy(
+    const VideoFrameBuffer& source) {
+  return Copy(source.width(), source.height(),
+              source.DataY(), source.StrideY(),
+              source.DataU(), source.StrideU(),
+              source.DataV(), source.StrideV());
+}
+
+// static
+rtc::scoped_refptr<I420Buffer> I420Buffer::Copy(
+      int width, int height,
+      const uint8_t* data_y, int stride_y,
+      const uint8_t* data_u, int stride_u,
+      const uint8_t* data_v, int stride_v) {
+  // Note: May use different strides than the input data.
+  rtc::scoped_refptr<I420Buffer> buffer = Create(width, height);
+  RTC_CHECK_EQ(0, libyuv::I420Copy(data_y, stride_y,
+                                   data_u, stride_u,
+                                   data_v, stride_v,
+                                   buffer->MutableDataY(), buffer->StrideY(),
+                                   buffer->MutableDataU(), buffer->StrideU(),
+                                   buffer->MutableDataV(), buffer->StrideV(),
+                                   width, height));
+  return buffer;
+}
+
+// static
+rtc::scoped_refptr<I420Buffer> I420Buffer::Rotate(
+    const VideoFrameBuffer& src, VideoRotation rotation) {
+  RTC_CHECK(src.DataY());
+  RTC_CHECK(src.DataU());
+  RTC_CHECK(src.DataV());
+
+  int rotated_width = src.width();
+  int rotated_height = src.height();
+  if (rotation == webrtc::kVideoRotation_90 ||
+      rotation == webrtc::kVideoRotation_270) {
+    std::swap(rotated_width, rotated_height);
+  }
+
+  rtc::scoped_refptr<webrtc::I420Buffer> buffer =
+      I420Buffer::Create(rotated_width, rotated_height);
+
+  RTC_CHECK_EQ(0, libyuv::I420Rotate(
+      src.DataY(), src.StrideY(),
+      src.DataU(), src.StrideU(),
+      src.DataV(), src.StrideV(),
+      buffer->MutableDataY(), buffer->StrideY(), buffer->MutableDataU(),
+      buffer->StrideU(), buffer->MutableDataV(), buffer->StrideV(),
+      src.width(), src.height(),
+      static_cast<libyuv::RotationMode>(rotation)));
+
+  return buffer;
+}
+
+// static
+rtc::scoped_refptr<VideoFrameBuffer> I420Buffer::Rotate(
+    rtc::scoped_refptr<VideoFrameBuffer> src,
+    VideoRotation rotation) {
+  if (rotation == webrtc::kVideoRotation_0) {
+    return src;
+  } else {
+    return Rotate(*src, rotation);
+  }
+}
+
+void I420Buffer::InitializeData() {
+  memset(data_.get(), 0,
+         I420DataSize(height_, stride_y_, stride_u_, stride_v_));
+}
+
+int I420Buffer::width() const {
+  return width_;
+}
+
+int I420Buffer::height() const {
+  return height_;
+}
+
+const uint8_t* I420Buffer::DataY() const {
+  return data_.get();
+}
+const uint8_t* I420Buffer::DataU() const {
+  return data_.get() + stride_y_ * height_;
+}
+const uint8_t* I420Buffer::DataV() const {
+  return data_.get() + stride_y_ * height_ + stride_u_ * ((height_ + 1) / 2);
+}
+
+int I420Buffer::StrideY() const {
+  return stride_y_;
+}
+int I420Buffer::StrideU() const {
+  return stride_u_;
+}
+int I420Buffer::StrideV() const {
+  return stride_v_;
+}
+
+void* I420Buffer::native_handle() const {
+  return nullptr;
+}
+
+rtc::scoped_refptr<VideoFrameBuffer> I420Buffer::NativeToI420Buffer() {
+  RTC_NOTREACHED();
+  return nullptr;
+}
+
+uint8_t* I420Buffer::MutableDataY() {
+  return const_cast<uint8_t*>(DataY());
+}
+uint8_t* I420Buffer::MutableDataU() {
+  return const_cast<uint8_t*>(DataU());
+}
+uint8_t* I420Buffer::MutableDataV() {
+  return const_cast<uint8_t*>(DataV());
+}
+
+// static
+void I420Buffer::SetBlack(I420Buffer* buffer) {
+  RTC_CHECK(libyuv::I420Rect(buffer->MutableDataY(), buffer->StrideY(),
+                             buffer->MutableDataU(), buffer->StrideU(),
+                             buffer->MutableDataV(), buffer->StrideV(),
+                             0, 0, buffer->width(), buffer->height(),
+                             0, 128, 128) == 0);
+}
+
+void I420Buffer::CropAndScaleFrom(
+    const VideoFrameBuffer& src,
+    int offset_x,
+    int offset_y,
+    int crop_width,
+    int crop_height) {
+  RTC_CHECK_LE(crop_width, src.width());
+  RTC_CHECK_LE(crop_height, src.height());
+  RTC_CHECK_LE(crop_width + offset_x, src.width());
+  RTC_CHECK_LE(crop_height + offset_y, src.height());
+  RTC_CHECK_GE(offset_x, 0);
+  RTC_CHECK_GE(offset_y, 0);
+
+  // Make sure offset is even so that u/v plane becomes aligned.
+  const int uv_offset_x = offset_x / 2;
+  const int uv_offset_y = offset_y / 2;
+  offset_x = uv_offset_x * 2;
+  offset_y = uv_offset_y * 2;
+
+  const uint8_t* y_plane =
+      src.DataY() + src.StrideY() * offset_y + offset_x;
+  const uint8_t* u_plane =
+      src.DataU() + src.StrideU() * uv_offset_y + uv_offset_x;
+  const uint8_t* v_plane =
+      src.DataV() + src.StrideV() * uv_offset_y + uv_offset_x;
+  int res = libyuv::I420Scale(y_plane, src.StrideY(),
+                              u_plane, src.StrideU(),
+                              v_plane, src.StrideV(),
+                              crop_width, crop_height,
+                              MutableDataY(), StrideY(),
+                              MutableDataU(), StrideU(),
+                              MutableDataV(), StrideV(),
+                              width(), height(), libyuv::kFilterBox);
+
+  RTC_DCHECK_EQ(res, 0);
+}
+
+void I420Buffer::CropAndScaleFrom(
+    const VideoFrameBuffer& src) {
+  const int crop_width =
+      std::min(src.width(), width() * src.height() / height());
+  const int crop_height =
+      std::min(src.height(), height() * src.width() / width());
+
+  CropAndScaleFrom(
+      src,
+      (src.width() - crop_width) / 2, (src.height() - crop_height) / 2,
+      crop_width, crop_height);
+}
+
+void I420Buffer::ScaleFrom(const VideoFrameBuffer& src) {
+  CropAndScaleFrom(src, 0, 0, src.width(), src.height());
+}
+
+}  // namespace webrtc
diff --git a/webrtc/api/video/i420_buffer.h b/webrtc/api/video/i420_buffer.h
index 388a3dd..de3f982 100644
--- a/webrtc/api/video/i420_buffer.h
+++ b/webrtc/api/video/i420_buffer.h
@@ -1,116 +1,116 @@
-/*
- *  Copyright (c) 2015 The WebRTC project authors. All Rights Reserved.
- *
- *  Use of this source code is governed by a BSD-style license
- *  that can be found in the LICENSE file in the root of the source
- *  tree. An additional intellectual property rights grant can be found
- *  in the file PATENTS.  All contributing project authors may
- *  be found in the AUTHORS file in the root of the source tree.
- */
-
-#ifndef WEBRTC_API_VIDEO_I420_BUFFER_H_
-#define WEBRTC_API_VIDEO_I420_BUFFER_H_
-
-#include <memory>
-
-#include "webrtc/api/video/video_rotation.h"
-#include "webrtc/api/video/video_frame_buffer.h"
-#include "webrtc/system_wrappers/include/aligned_malloc.h"
-
-namespace webrtc {
-
-// Plain I420 buffer in standard memory.
-class I420Buffer : public VideoFrameBuffer {
- public:
-  static rtc::scoped_refptr<I420Buffer> Create(int width, int height);
-  static rtc::scoped_refptr<I420Buffer> Create(int width,
-                                               int height,
-                                               int stride_y,
-                                               int stride_u,
-                                               int stride_v);
-
-  // Create a new buffer and copy the pixel data.
-  static rtc::scoped_refptr<I420Buffer> Copy(const VideoFrameBuffer& buffer);
-
-  static rtc::scoped_refptr<I420Buffer> Copy(
-      int width, int height,
-      const uint8_t* data_y, int stride_y,
-      const uint8_t* data_u, int stride_u,
-      const uint8_t* data_v, int stride_v);
-
-  // Returns a rotated copy of |src|.
-  static rtc::scoped_refptr<I420Buffer> Rotate(const VideoFrameBuffer& src,
-                                               VideoRotation rotation);
-
-  // Sets the buffer to all black.
-  static void SetBlack(I420Buffer* buffer);
-
-  // Sets all three planes to all zeros. Used to work around for
-  // quirks in memory checkers
-  // (https://bugs.chromium.org/p/libyuv/issues/detail?id=377) and
-  // ffmpeg (http://crbug.com/390941).
-  // TODO(nisse): Deprecated. Should be deleted if/when those issues
-  // are resolved in a better way. Or in the mean time, use SetBlack.
-  void InitializeData();
-
-  // TODO(nisse): Deprecated, use static method instead.
-  void SetToBlack() { SetBlack(this); }
-
-  int width() const override;
-  int height() const override;
-  const uint8_t* DataY() const override;
-  const uint8_t* DataU() const override;
-  const uint8_t* DataV() const override;
-
-  int StrideY() const override;
-  int StrideU() const override;
-  int StrideV() const override;
-
-  void* native_handle() const override;
-  rtc::scoped_refptr<VideoFrameBuffer> NativeToI420Buffer() override;
-
-  uint8_t* MutableDataY();
-  uint8_t* MutableDataU();
-  uint8_t* MutableDataV();
-
-  // Scale the cropped area of |src| to the size of |this| buffer, and
-  // write the result into |this|.
-  void CropAndScaleFrom(const VideoFrameBuffer& src,
-                        int offset_x,
-                        int offset_y,
-                        int crop_width,
-                        int crop_height);
-
-  // The common case of a center crop, when needed to adjust the
-  // aspect ratio without distorting the image.
-  void CropAndScaleFrom(const VideoFrameBuffer& src);
-
-  // Scale all of |src| to the size of |this| buffer, with no cropping.
-  void ScaleFrom(const VideoFrameBuffer& src);
-
-  // TODO(nisse): Deprecated, delete once downstream applications are updated.
-  // Returns a rotated versions of |src|. Native buffers are not
-  // supported. The reason this function doesn't return an I420Buffer,
-  // is that it returns |src| unchanged in case |rotation| is zero.
-  static rtc::scoped_refptr<VideoFrameBuffer> Rotate(
-      rtc::scoped_refptr<VideoFrameBuffer> src,
-      VideoRotation rotation);
-
- protected:
-  I420Buffer(int width, int height);
-  I420Buffer(int width, int height, int stride_y, int stride_u, int stride_v);
-
-  ~I420Buffer() override;
-
- private:
-  const int width_;
-  const int height_;
-  const int stride_y_;
-  const int stride_u_;
-  const int stride_v_;
-  const std::unique_ptr<uint8_t, AlignedFreeDeleter> data_;
-};
-
-}  // namespace webrtc
-
-#endif  // WEBRTC_API_VIDEO_I420_BUFFER_H_
+/*
+ *  Copyright (c) 2015 The WebRTC project authors. All Rights Reserved.
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ */
+
+#ifndef WEBRTC_API_VIDEO_I420_BUFFER_H_
+#define WEBRTC_API_VIDEO_I420_BUFFER_H_
+
+#include <memory>
+
+#include "webrtc/api/video/video_rotation.h"
+#include "webrtc/api/video/video_frame_buffer.h"
+#include "webrtc/system_wrappers/include/aligned_malloc.h"
+
+namespace webrtc {
+
+// Plain I420 buffer in standard memory.
+class I420Buffer : public VideoFrameBuffer {
+ public:
+  static rtc::scoped_refptr<I420Buffer> Create(int width, int height);
+  static rtc::scoped_refptr<I420Buffer> Create(int width,
+                                               int height,
+                                               int stride_y,
+                                               int stride_u,
+                                               int stride_v);
+
+  // Create a new buffer and copy the pixel data.
+  static rtc::scoped_refptr<I420Buffer> Copy(const VideoFrameBuffer& buffer);
+
+  static rtc::scoped_refptr<I420Buffer> Copy(
+      int width, int height,
+      const uint8_t* data_y, int stride_y,
+      const uint8_t* data_u, int stride_u,
+      const uint8_t* data_v, int stride_v);
+
+  // Returns a rotated copy of |src|.
+  static rtc::scoped_refptr<I420Buffer> Rotate(const VideoFrameBuffer& src,
+                                               VideoRotation rotation);
+
+  // Sets the buffer to all black.
+  static void SetBlack(I420Buffer* buffer);
+
+  // Sets all three planes to all zeros. Used to work around for
+  // quirks in memory checkers
+  // (https://bugs.chromium.org/p/libyuv/issues/detail?id=377) and
+  // ffmpeg (http://crbug.com/390941).
+  // TODO(nisse): Deprecated. Should be deleted if/when those issues
+  // are resolved in a better way. Or in the mean time, use SetBlack.
+  void InitializeData();
+
+  // TODO(nisse): Deprecated, use static method instead.
+  void SetToBlack() { SetBlack(this); }
+
+  int width() const override;
+  int height() const override;
+  const uint8_t* DataY() const override;
+  const uint8_t* DataU() const override;
+  const uint8_t* DataV() const override;
+
+  int StrideY() const override;
+  int StrideU() const override;
+  int StrideV() const override;
+
+  void* native_handle() const override;
+  rtc::scoped_refptr<VideoFrameBuffer> NativeToI420Buffer() override;
+
+  uint8_t* MutableDataY();
+  uint8_t* MutableDataU();
+  uint8_t* MutableDataV();
+
+  // Scale the cropped area of |src| to the size of |this| buffer, and
+  // write the result into |this|.
+  void CropAndScaleFrom(const VideoFrameBuffer& src,
+                        int offset_x,
+                        int offset_y,
+                        int crop_width,
+                        int crop_height);
+
+  // The common case of a center crop, when needed to adjust the
+  // aspect ratio without distorting the image.
+  void CropAndScaleFrom(const VideoFrameBuffer& src);
+
+  // Scale all of |src| to the size of |this| buffer, with no cropping.
+  void ScaleFrom(const VideoFrameBuffer& src);
+
+  // TODO(nisse): Deprecated, delete once downstream applications are updated.
+  // Returns a rotated versions of |src|. Native buffers are not
+  // supported. The reason this function doesn't return an I420Buffer,
+  // is that it returns |src| unchanged in case |rotation| is zero.
+  static rtc::scoped_refptr<VideoFrameBuffer> Rotate(
+      rtc::scoped_refptr<VideoFrameBuffer> src,
+      VideoRotation rotation);
+
+ protected:
+  I420Buffer(int width, int height);
+  I420Buffer(int width, int height, int stride_y, int stride_u, int stride_v);
+
+  ~I420Buffer() override;
+
+ private:
+  const int width_;
+  const int height_;
+  const int stride_y_;
+  const int stride_u_;
+  const int stride_v_;
+  const std::unique_ptr<uint8_t, AlignedFreeDeleter> data_;
+};
+
+}  // namespace webrtc
+
+#endif  // WEBRTC_API_VIDEO_I420_BUFFER_H_
diff --git a/webrtc/api/video/video_frame.h b/webrtc/api/video/video_frame.h
index 8840782..439cfbb 100644
--- a/webrtc/api/video/video_frame.h
+++ b/webrtc/api/video/video_frame.h
@@ -12,7 +12,8 @@
 #define WEBRTC_API_VIDEO_VIDEO_FRAME_H_
 
 #include <stdint.h>
-
+
+#include <d3d11.h>
 #include "webrtc/api/video/video_rotation.h"
 #include "webrtc/api/video/video_frame_buffer.h"
 
@@ -104,6 +105,16 @@ class VideoFrame {
   // Return true if the frame is stored in a texture.
   bool is_texture() const {
     return video_frame_buffer()->native_handle() != nullptr;
+  }
+
+  void SetID3D11Texture2D(ID3D11Texture2D* texture)
+  {
+	  m_stagingFrameBuffer = texture;
+  }
+
+  ID3D11Texture2D* GetID3D11Texture2D() const
+  {
+	  return m_stagingFrameBuffer;
   }
 
  private:
@@ -112,7 +123,8 @@ class VideoFrame {
   uint32_t timestamp_rtp_;
   int64_t ntp_time_ms_;
   int64_t timestamp_us_;
-  VideoRotation rotation_;
+  VideoRotation rotation_;
+  ID3D11Texture2D* m_stagingFrameBuffer;
 };
 
 }  // namespace webrtc
diff --git a/webrtc/api/video/video_frame_buffer.h b/webrtc/api/video/video_frame_buffer.h
index c8c2e5d..d7621ad 100644
--- a/webrtc/api/video/video_frame_buffer.h
+++ b/webrtc/api/video/video_frame_buffer.h
@@ -1,55 +1,55 @@
-/*
- *  Copyright (c) 2015 The WebRTC project authors. All Rights Reserved.
- *
- *  Use of this source code is governed by a BSD-style license
- *  that can be found in the LICENSE file in the root of the source
- *  tree. An additional intellectual property rights grant can be found
- *  in the file PATENTS.  All contributing project authors may
- *  be found in the AUTHORS file in the root of the source tree.
- */
-
-#ifndef WEBRTC_API_VIDEO_VIDEO_FRAME_BUFFER_H_
-#define WEBRTC_API_VIDEO_VIDEO_FRAME_BUFFER_H_
-
-#include <stdint.h>
-
-#include "webrtc/base/refcount.h"
-#include "webrtc/base/scoped_ref_ptr.h"
-
-namespace webrtc {
-
-// Interface of a simple frame buffer containing pixel data. This interface does
-// not contain any frame metadata such as rotation, timestamp, pixel_width, etc.
-class VideoFrameBuffer : public rtc::RefCountInterface {
- public:
-  // The resolution of the frame in pixels. For formats where some planes are
-  // subsampled, this is the highest-resolution plane.
-  virtual int width() const = 0;
-  virtual int height() const = 0;
-
-  // Returns pointer to the pixel data for a given plane. The memory is owned by
-  // the VideoFrameBuffer object and must not be freed by the caller.
-  virtual const uint8_t* DataY() const = 0;
-  virtual const uint8_t* DataU() const = 0;
-  virtual const uint8_t* DataV() const = 0;
-
-  // Returns the number of bytes between successive rows for a given plane.
-  virtual int StrideY() const = 0;
-  virtual int StrideU() const = 0;
-  virtual int StrideV() const = 0;
-
-  // Return the handle of the underlying video frame. This is used when the
-  // frame is backed by a texture.
-  virtual void* native_handle() const = 0;
-
-  // Returns a new memory-backed frame buffer converted from this buffer's
-  // native handle.
-  virtual rtc::scoped_refptr<VideoFrameBuffer> NativeToI420Buffer() = 0;
-
- protected:
-  ~VideoFrameBuffer() override {}
-};
-
-}  // namespace webrtc
-
-#endif  // WEBRTC_API_VIDEO_VIDEO_FRAME_BUFFER_H_
+/*
+ *  Copyright (c) 2015 The WebRTC project authors. All Rights Reserved.
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ */
+
+#ifndef WEBRTC_API_VIDEO_VIDEO_FRAME_BUFFER_H_
+#define WEBRTC_API_VIDEO_VIDEO_FRAME_BUFFER_H_
+
+#include <stdint.h>
+
+#include "webrtc/base/refcount.h"
+#include "webrtc/base/scoped_ref_ptr.h"
+
+namespace webrtc {
+
+// Interface of a simple frame buffer containing pixel data. This interface does
+// not contain any frame metadata such as rotation, timestamp, pixel_width, etc.
+class VideoFrameBuffer : public rtc::RefCountInterface {
+ public:
+  // The resolution of the frame in pixels. For formats where some planes are
+  // subsampled, this is the highest-resolution plane.
+  virtual int width() const = 0;
+  virtual int height() const = 0;
+
+  // Returns pointer to the pixel data for a given plane. The memory is owned by
+  // the VideoFrameBuffer object and must not be freed by the caller.
+  virtual const uint8_t* DataY() const = 0;
+  virtual const uint8_t* DataU() const = 0;
+  virtual const uint8_t* DataV() const = 0;
+
+  // Returns the number of bytes between successive rows for a given plane.
+  virtual int StrideY() const = 0;
+  virtual int StrideU() const = 0;
+  virtual int StrideV() const = 0;
+
+  // Return the handle of the underlying video frame. This is used when the
+  // frame is backed by a texture.
+  virtual void* native_handle() const = 0;
+
+  // Returns a new memory-backed frame buffer converted from this buffer's
+  // native handle.
+  virtual rtc::scoped_refptr<VideoFrameBuffer> NativeToI420Buffer() = 0;
+
+ protected:
+  ~VideoFrameBuffer() override {}
+};
+
+}  // namespace webrtc
+
+#endif  // WEBRTC_API_VIDEO_VIDEO_FRAME_BUFFER_H_
diff --git a/webrtc/base/BUILD.gn b/webrtc/base/BUILD.gn
index eb9361b..89b2d4b 100644
--- a/webrtc/base/BUILD.gn
+++ b/webrtc/base/BUILD.gn
@@ -647,7 +647,7 @@ rtc_source_set("gtest_prod") {
   ]
 }
 
-if (rtc_include_tests) {
+if (true) {
   config("rtc_base_tests_utils_exported_config") {
     defines = [ "GTEST_RELATIVE_PATH" ]
   }
diff --git a/webrtc/common_video/include/video_frame_buffer.h b/webrtc/common_video/include/video_frame_buffer.h
index dfdd480..a6db3e1 100644
--- a/webrtc/common_video/include/video_frame_buffer.h
+++ b/webrtc/common_video/include/video_frame_buffer.h
@@ -1,92 +1,92 @@
-/*
- *  Copyright (c) 2015 The WebRTC project authors. All Rights Reserved.
- *
- *  Use of this source code is governed by a BSD-style license
- *  that can be found in the LICENSE file in the root of the source
- *  tree. An additional intellectual property rights grant can be found
- *  in the file PATENTS.  All contributing project authors may
- *  be found in the AUTHORS file in the root of the source tree.
- */
-
-#ifndef WEBRTC_COMMON_VIDEO_INCLUDE_VIDEO_FRAME_BUFFER_H_
-#define WEBRTC_COMMON_VIDEO_INCLUDE_VIDEO_FRAME_BUFFER_H_
-
-#include <memory>
-
-#include "webrtc/api/video/video_frame_buffer.h"
-// TODO(nisse): For backwards compatibility, files including this file
-// expect it to declare I420Buffer. Delete after callers are updated.
-#include "webrtc/api/video/i420_buffer.h"
-#include "webrtc/base/callback.h"
-#include "webrtc/base/scoped_ref_ptr.h"
-
-namespace webrtc {
-
-// Base class for native-handle buffer is a wrapper around a |native_handle|.
-// This is used for convenience as most native-handle implementations can share
-// many VideoFrame implementations, but need to implement a few others (such
-// as their own destructors or conversion methods back to software I420).
-class NativeHandleBuffer : public VideoFrameBuffer {
- public:
-  NativeHandleBuffer(void* native_handle, int width, int height);
-
-  int width() const override;
-  int height() const override;
-  const uint8_t* DataY() const override;
-  const uint8_t* DataU() const override;
-  const uint8_t* DataV() const override;
-  int StrideY() const override;
-  int StrideU() const override;
-  int StrideV() const override;
-
-  void* native_handle() const override;
-
- protected:
-  void* native_handle_;
-  const int width_;
-  const int height_;
-};
-
-class WrappedI420Buffer : public webrtc::VideoFrameBuffer {
- public:
-  WrappedI420Buffer(int width,
-                    int height,
-                    const uint8_t* y_plane,
-                    int y_stride,
-                    const uint8_t* u_plane,
-                    int u_stride,
-                    const uint8_t* v_plane,
-                    int v_stride,
-                    const rtc::Callback0<void>& no_longer_used);
-  int width() const override;
-  int height() const override;
-
-  const uint8_t* DataY() const override;
-  const uint8_t* DataU() const override;
-  const uint8_t* DataV() const override;
-  int StrideY() const override;
-  int StrideU() const override;
-  int StrideV() const override;
-
-  void* native_handle() const override;
-
-  rtc::scoped_refptr<VideoFrameBuffer> NativeToI420Buffer() override;
-
- private:
-  friend class rtc::RefCountedObject<WrappedI420Buffer>;
-  ~WrappedI420Buffer() override;
-
-  const int width_;
-  const int height_;
-  const uint8_t* const y_plane_;
-  const uint8_t* const u_plane_;
-  const uint8_t* const v_plane_;
-  const int y_stride_;
-  const int u_stride_;
-  const int v_stride_;
-  rtc::Callback0<void> no_longer_used_cb_;
-};
-
-}  // namespace webrtc
-
-#endif  // WEBRTC_COMMON_VIDEO_INCLUDE_VIDEO_FRAME_BUFFER_H_
+/*
+ *  Copyright (c) 2015 The WebRTC project authors. All Rights Reserved.
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ */
+
+#ifndef WEBRTC_COMMON_VIDEO_INCLUDE_VIDEO_FRAME_BUFFER_H_
+#define WEBRTC_COMMON_VIDEO_INCLUDE_VIDEO_FRAME_BUFFER_H_
+
+#include <memory>
+
+#include "webrtc/api/video/video_frame_buffer.h"
+// TODO(nisse): For backwards compatibility, files including this file
+// expect it to declare I420Buffer. Delete after callers are updated.
+#include "webrtc/api/video/i420_buffer.h"
+#include "webrtc/base/callback.h"
+#include "webrtc/base/scoped_ref_ptr.h"
+
+namespace webrtc {
+
+// Base class for native-handle buffer is a wrapper around a |native_handle|.
+// This is used for convenience as most native-handle implementations can share
+// many VideoFrame implementations, but need to implement a few others (such
+// as their own destructors or conversion methods back to software I420).
+class NativeHandleBuffer : public VideoFrameBuffer {
+ public:
+  NativeHandleBuffer(void* native_handle, int width, int height);
+
+  int width() const override;
+  int height() const override;
+  const uint8_t* DataY() const override;
+  const uint8_t* DataU() const override;
+  const uint8_t* DataV() const override;
+  int StrideY() const override;
+  int StrideU() const override;
+  int StrideV() const override;
+
+  void* native_handle() const override;
+
+ protected:
+  void* native_handle_;
+  const int width_;
+  const int height_;
+};
+
+class WrappedI420Buffer : public webrtc::VideoFrameBuffer {
+ public:
+  WrappedI420Buffer(int width,
+                    int height,
+                    const uint8_t* y_plane,
+                    int y_stride,
+                    const uint8_t* u_plane,
+                    int u_stride,
+                    const uint8_t* v_plane,
+                    int v_stride,
+                    const rtc::Callback0<void>& no_longer_used);
+  int width() const override;
+  int height() const override;
+
+  const uint8_t* DataY() const override;
+  const uint8_t* DataU() const override;
+  const uint8_t* DataV() const override;
+  int StrideY() const override;
+  int StrideU() const override;
+  int StrideV() const override;
+
+  void* native_handle() const override;
+
+  rtc::scoped_refptr<VideoFrameBuffer> NativeToI420Buffer() override;
+
+ private:
+  friend class rtc::RefCountedObject<WrappedI420Buffer>;
+  ~WrappedI420Buffer() override;
+
+  const int width_;
+  const int height_;
+  const uint8_t* const y_plane_;
+  const uint8_t* const u_plane_;
+  const uint8_t* const v_plane_;
+  const int y_stride_;
+  const int u_stride_;
+  const int v_stride_;
+  rtc::Callback0<void> no_longer_used_cb_;
+};
+
+}  // namespace webrtc
+
+#endif  // WEBRTC_COMMON_VIDEO_INCLUDE_VIDEO_FRAME_BUFFER_H_
diff --git a/webrtc/common_video/video_frame_buffer.cc b/webrtc/common_video/video_frame_buffer.cc
index 4646bf4..49206dc 100644
--- a/webrtc/common_video/video_frame_buffer.cc
+++ b/webrtc/common_video/video_frame_buffer.cc
@@ -1,132 +1,132 @@
-/*
- *  Copyright (c) 2015 The WebRTC project authors. All Rights Reserved.
- *
- *  Use of this source code is governed by a BSD-style license
- *  that can be found in the LICENSE file in the root of the source
- *  tree. An additional intellectual property rights grant can be found
- *  in the file PATENTS.  All contributing project authors may
- *  be found in the AUTHORS file in the root of the source tree.
- */
-#include "webrtc/common_video/include/video_frame_buffer.h"
-
-#include <string.h>
-
-#include <algorithm>
-
-#include "webrtc/base/checks.h"
-#include "webrtc/base/keep_ref_until_done.h"
-#include "libyuv/convert.h"
-#include "libyuv/planar_functions.h"
-#include "libyuv/scale.h"
-
-namespace webrtc {
-
-NativeHandleBuffer::NativeHandleBuffer(void* native_handle,
-                                       int width,
-                                       int height)
-    : native_handle_(native_handle), width_(width), height_(height) {
-  RTC_DCHECK(native_handle != nullptr);
-  RTC_DCHECK_GT(width, 0);
-  RTC_DCHECK_GT(height, 0);
-}
-
-int NativeHandleBuffer::width() const {
-  return width_;
-}
-
-int NativeHandleBuffer::height() const {
-  return height_;
-}
-
-const uint8_t* NativeHandleBuffer::DataY() const {
-  RTC_NOTREACHED();  // Should not be called.
-  return nullptr;
-}
-const uint8_t* NativeHandleBuffer::DataU() const {
-  RTC_NOTREACHED();  // Should not be called.
-  return nullptr;
-}
-const uint8_t* NativeHandleBuffer::DataV() const {
-  RTC_NOTREACHED();  // Should not be called.
-  return nullptr;
-}
-
-int NativeHandleBuffer::StrideY() const {
-  RTC_NOTREACHED();  // Should not be called.
-  return 0;
-}
-int NativeHandleBuffer::StrideU() const {
-  RTC_NOTREACHED();  // Should not be called.
-  return 0;
-}
-int NativeHandleBuffer::StrideV() const {
-  RTC_NOTREACHED();  // Should not be called.
-  return 0;
-}
-
-void* NativeHandleBuffer::native_handle() const {
-  return native_handle_;
-}
-
-WrappedI420Buffer::WrappedI420Buffer(int width,
-                                     int height,
-                                     const uint8_t* y_plane,
-                                     int y_stride,
-                                     const uint8_t* u_plane,
-                                     int u_stride,
-                                     const uint8_t* v_plane,
-                                     int v_stride,
-                                     const rtc::Callback0<void>& no_longer_used)
-    : width_(width),
-      height_(height),
-      y_plane_(y_plane),
-      u_plane_(u_plane),
-      v_plane_(v_plane),
-      y_stride_(y_stride),
-      u_stride_(u_stride),
-      v_stride_(v_stride),
-      no_longer_used_cb_(no_longer_used) {
-}
-
-WrappedI420Buffer::~WrappedI420Buffer() {
-  no_longer_used_cb_();
-}
-
-int WrappedI420Buffer::width() const {
-  return width_;
-}
-
-int WrappedI420Buffer::height() const {
-  return height_;
-}
-
-const uint8_t* WrappedI420Buffer::DataY() const {
-  return y_plane_;
-}
-const uint8_t* WrappedI420Buffer::DataU() const {
-  return u_plane_;
-}
-const uint8_t* WrappedI420Buffer::DataV() const {
-  return v_plane_;
-}
-
-int WrappedI420Buffer::StrideY() const {
-  return y_stride_;
-}
-int WrappedI420Buffer::StrideU() const {
-  return u_stride_;
-}
-int WrappedI420Buffer::StrideV() const {
-  return v_stride_;
-}
-
-void* WrappedI420Buffer::native_handle() const {
-  return nullptr;
-}
-
-rtc::scoped_refptr<VideoFrameBuffer> WrappedI420Buffer::NativeToI420Buffer() {
-  RTC_NOTREACHED();
-  return nullptr;
-}
-
-}  // namespace webrtc
+/*
+ *  Copyright (c) 2015 The WebRTC project authors. All Rights Reserved.
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ */
+#include "webrtc/common_video/include/video_frame_buffer.h"
+
+#include <string.h>
+
+#include <algorithm>
+
+#include "webrtc/base/checks.h"
+#include "webrtc/base/keep_ref_until_done.h"
+#include "libyuv/convert.h"
+#include "libyuv/planar_functions.h"
+#include "libyuv/scale.h"
+
+namespace webrtc {
+
+NativeHandleBuffer::NativeHandleBuffer(void* native_handle,
+                                       int width,
+                                       int height)
+    : native_handle_(native_handle), width_(width), height_(height) {
+  RTC_DCHECK(native_handle != nullptr);
+  RTC_DCHECK_GT(width, 0);
+  RTC_DCHECK_GT(height, 0);
+}
+
+int NativeHandleBuffer::width() const {
+  return width_;
+}
+
+int NativeHandleBuffer::height() const {
+  return height_;
+}
+
+const uint8_t* NativeHandleBuffer::DataY() const {
+  RTC_NOTREACHED();  // Should not be called.
+  return nullptr;
+}
+const uint8_t* NativeHandleBuffer::DataU() const {
+  RTC_NOTREACHED();  // Should not be called.
+  return nullptr;
+}
+const uint8_t* NativeHandleBuffer::DataV() const {
+  RTC_NOTREACHED();  // Should not be called.
+  return nullptr;
+}
+
+int NativeHandleBuffer::StrideY() const {
+  RTC_NOTREACHED();  // Should not be called.
+  return 0;
+}
+int NativeHandleBuffer::StrideU() const {
+  RTC_NOTREACHED();  // Should not be called.
+  return 0;
+}
+int NativeHandleBuffer::StrideV() const {
+  RTC_NOTREACHED();  // Should not be called.
+  return 0;
+}
+
+void* NativeHandleBuffer::native_handle() const {
+  return native_handle_;
+}
+
+WrappedI420Buffer::WrappedI420Buffer(int width,
+                                     int height,
+                                     const uint8_t* y_plane,
+                                     int y_stride,
+                                     const uint8_t* u_plane,
+                                     int u_stride,
+                                     const uint8_t* v_plane,
+                                     int v_stride,
+                                     const rtc::Callback0<void>& no_longer_used)
+    : width_(width),
+      height_(height),
+      y_plane_(y_plane),
+      u_plane_(u_plane),
+      v_plane_(v_plane),
+      y_stride_(y_stride),
+      u_stride_(u_stride),
+      v_stride_(v_stride),
+      no_longer_used_cb_(no_longer_used) {
+}
+
+WrappedI420Buffer::~WrappedI420Buffer() {
+  no_longer_used_cb_();
+}
+
+int WrappedI420Buffer::width() const {
+  return width_;
+}
+
+int WrappedI420Buffer::height() const {
+  return height_;
+}
+
+const uint8_t* WrappedI420Buffer::DataY() const {
+  return y_plane_;
+}
+const uint8_t* WrappedI420Buffer::DataU() const {
+  return u_plane_;
+}
+const uint8_t* WrappedI420Buffer::DataV() const {
+  return v_plane_;
+}
+
+int WrappedI420Buffer::StrideY() const {
+  return y_stride_;
+}
+int WrappedI420Buffer::StrideU() const {
+  return u_stride_;
+}
+int WrappedI420Buffer::StrideV() const {
+  return v_stride_;
+}
+
+void* WrappedI420Buffer::native_handle() const {
+  return nullptr;
+}
+
+rtc::scoped_refptr<VideoFrameBuffer> WrappedI420Buffer::NativeToI420Buffer() {
+  RTC_NOTREACHED();
+  return nullptr;
+}
+
+}  // namespace webrtc
diff --git a/webrtc/media/base/mediaconstants.cc b/webrtc/media/base/mediaconstants.cc
index 0d8512b..76dcb1c 100644
--- a/webrtc/media/base/mediaconstants.cc
+++ b/webrtc/media/base/mediaconstants.cc
@@ -108,6 +108,7 @@ const char kH264FmtpLevelAsymmetryAllowed[] = "level-asymmetry-allowed";
 const char kH264FmtpPacketizationMode[] = "packetization-mode";
 const char kH264FmtpSpropParameterSets[] = "sprop-parameter-sets";
 const char kH264ProfileLevelConstrainedBaseline[] = "42e01f";
+const char kH264UseHWNvencode[] = "use-hw-nvencode";
 
 const int kDefaultVideoMaxFramerate = 60;
 }  // namespace cricket
diff --git a/webrtc/media/base/mediaconstants.h b/webrtc/media/base/mediaconstants.h
index 44d8c7e..b592198 100644
--- a/webrtc/media/base/mediaconstants.h
+++ b/webrtc/media/base/mediaconstants.h
@@ -130,6 +130,7 @@ extern const char kH264FmtpLevelAsymmetryAllowed[];
 extern const char kH264FmtpPacketizationMode[];
 extern const char kH264FmtpSpropParameterSets[];
 extern const char kH264ProfileLevelConstrainedBaseline[];
+extern const char kH264UseHWNvencode[];
 
 extern const int kDefaultVideoMaxFramerate;
 }  // namespace cricket
diff --git a/webrtc/media/engine/webrtcvideoengine2.cc b/webrtc/media/engine/webrtcvideoengine2.cc
index 3639472..b61e548 100644
--- a/webrtc/media/engine/webrtcvideoengine2.cc
+++ b/webrtc/media/engine/webrtcvideoengine2.cc
@@ -1,2552 +1,2561 @@
-/*
- *  Copyright (c) 2014 The WebRTC project authors. All Rights Reserved.
- *
- *  Use of this source code is governed by a BSD-style license
- *  that can be found in the LICENSE file in the root of the source
- *  tree. An additional intellectual property rights grant can be found
- *  in the file PATENTS.  All contributing project authors may
- *  be found in the AUTHORS file in the root of the source tree.
- */
-
-#include "webrtc/media/engine/webrtcvideoengine2.h"
-
-#include <stdio.h>
-#include <algorithm>
-#include <set>
-#include <string>
-#include <utility>
-
-#include "webrtc/api/video/i420_buffer.h"
-#include "webrtc/base/copyonwritebuffer.h"
-#include "webrtc/base/logging.h"
-#include "webrtc/base/stringutils.h"
-#include "webrtc/base/timeutils.h"
-#include "webrtc/base/trace_event.h"
-#include "webrtc/call/call.h"
-#include "webrtc/common_video/h264/profile_level_id.h"
-#include "webrtc/media/engine/constants.h"
-#include "webrtc/media/engine/internalencoderfactory.h"
-#include "webrtc/media/engine/internaldecoderfactory.h"
-#include "webrtc/media/engine/simulcast.h"
-#include "webrtc/media/engine/videoencodersoftwarefallbackwrapper.h"
-#include "webrtc/media/engine/videodecodersoftwarefallbackwrapper.h"
-#include "webrtc/media/engine/webrtcmediaengine.h"
-#include "webrtc/media/engine/webrtcvideoencoderfactory.h"
-#include "webrtc/media/engine/webrtcvoiceengine.h"
-#include "webrtc/modules/video_coding/codecs/vp8/simulcast_encoder_adapter.h"
-#include "webrtc/system_wrappers/include/field_trial.h"
-#include "webrtc/video_decoder.h"
-#include "webrtc/video_encoder.h"
-
-namespace cricket {
-namespace {
-
-// If this field trial is enabled, we will enable sending FlexFEC and disable
-// sending ULPFEC whenever the former has been negotiated. Receiving FlexFEC
-// is enabled whenever FlexFEC has been negotiated.
-bool IsFlexfecFieldTrialEnabled() {
-  return webrtc::field_trial::FindFullName("WebRTC-FlexFEC-03") == "Enabled";
-}
-
-// Wrap cricket::WebRtcVideoEncoderFactory as a webrtc::VideoEncoderFactory.
-class EncoderFactoryAdapter : public webrtc::VideoEncoderFactory {
- public:
-  // EncoderFactoryAdapter doesn't take ownership of |factory|, which is owned
-  // by e.g. PeerConnectionFactory.
-  explicit EncoderFactoryAdapter(cricket::WebRtcVideoEncoderFactory* factory)
-      : factory_(factory) {}
-  virtual ~EncoderFactoryAdapter() {}
-
-  // Implement webrtc::VideoEncoderFactory.
-  webrtc::VideoEncoder* Create() override {
-    return factory_->CreateVideoEncoder(VideoCodec(kVp8CodecName));
-  }
-
-  void Destroy(webrtc::VideoEncoder* encoder) override {
-    return factory_->DestroyVideoEncoder(encoder);
-  }
-
- private:
-  cricket::WebRtcVideoEncoderFactory* const factory_;
-};
-
-// An encoder factory that wraps Create requests for simulcastable codec types
-// with a webrtc::SimulcastEncoderAdapter. Non simulcastable codec type
-// requests are just passed through to the contained encoder factory.
-class WebRtcSimulcastEncoderFactory
-    : public cricket::WebRtcVideoEncoderFactory {
- public:
-  // WebRtcSimulcastEncoderFactory doesn't take ownership of |factory|, which is
-  // owned by e.g. PeerConnectionFactory.
-  explicit WebRtcSimulcastEncoderFactory(
-      cricket::WebRtcVideoEncoderFactory* factory)
-      : factory_(factory) {}
-
-  static bool UseSimulcastEncoderFactory(
-      const std::vector<cricket::VideoCodec>& codecs) {
-    // If any codec is VP8, use the simulcast factory. If asked to create a
-    // non-VP8 codec, we'll just return a contained factory encoder directly.
-    for (const auto& codec : codecs) {
-      if (CodecNamesEq(codec.name.c_str(), kVp8CodecName)) {
-        return true;
-      }
-    }
-    return false;
-  }
-
-  webrtc::VideoEncoder* CreateVideoEncoder(
-      const cricket::VideoCodec& codec) override {
-    RTC_DCHECK(factory_ != NULL);
-    // If it's a codec type we can simulcast, create a wrapped encoder.
-    if (CodecNamesEq(codec.name.c_str(), kVp8CodecName)) {
-      return new webrtc::SimulcastEncoderAdapter(
-          new EncoderFactoryAdapter(factory_));
-    }
-    webrtc::VideoEncoder* encoder = factory_->CreateVideoEncoder(codec);
-    if (encoder) {
-      non_simulcast_encoders_.push_back(encoder);
-    }
-    return encoder;
-  }
-
-  const std::vector<cricket::VideoCodec>& supported_codecs() const override {
-    return factory_->supported_codecs();
-  }
-
-  bool EncoderTypeHasInternalSource(
-      webrtc::VideoCodecType type) const override {
-    return factory_->EncoderTypeHasInternalSource(type);
-  }
-
-  void DestroyVideoEncoder(webrtc::VideoEncoder* encoder) override {
-    // Check first to see if the encoder wasn't wrapped in a
-    // SimulcastEncoderAdapter. In that case, ask the factory to destroy it.
-    if (std::remove(non_simulcast_encoders_.begin(),
-                    non_simulcast_encoders_.end(),
-                    encoder) != non_simulcast_encoders_.end()) {
-      factory_->DestroyVideoEncoder(encoder);
-      return;
-    }
-
-    // Otherwise, SimulcastEncoderAdapter can be deleted directly, and will call
-    // DestroyVideoEncoder on the factory for individual encoder instances.
-    delete encoder;
-  }
-
- private:
-  // Disable overloaded virtual function warning. TODO(magjed): Remove once
-  // http://crbug/webrtc/6402 is fixed.
-  using cricket::WebRtcVideoEncoderFactory::CreateVideoEncoder;
-
-  cricket::WebRtcVideoEncoderFactory* factory_;
-  // A list of encoders that were created without being wrapped in a
-  // SimulcastEncoderAdapter.
-  std::vector<webrtc::VideoEncoder*> non_simulcast_encoders_;
-};
-
-void AddDefaultFeedbackParams(VideoCodec* codec) {
-  codec->AddFeedbackParam(FeedbackParam(kRtcpFbParamCcm, kRtcpFbCcmParamFir));
-  codec->AddFeedbackParam(FeedbackParam(kRtcpFbParamNack, kParamValueEmpty));
-  codec->AddFeedbackParam(FeedbackParam(kRtcpFbParamNack, kRtcpFbNackParamPli));
-  codec->AddFeedbackParam(FeedbackParam(kRtcpFbParamRemb, kParamValueEmpty));
-  codec->AddFeedbackParam(
-      FeedbackParam(kRtcpFbParamTransportCc, kParamValueEmpty));
-}
-
-static std::string CodecVectorToString(const std::vector<VideoCodec>& codecs) {
-  std::stringstream out;
-  out << '{';
-  for (size_t i = 0; i < codecs.size(); ++i) {
-    out << codecs[i].ToString();
-    if (i != codecs.size() - 1) {
-      out << ", ";
-    }
-  }
-  out << '}';
-  return out.str();
-}
-
-static bool ValidateCodecFormats(const std::vector<VideoCodec>& codecs) {
-  bool has_video = false;
-  for (size_t i = 0; i < codecs.size(); ++i) {
-    if (!codecs[i].ValidateCodecFormat()) {
-      return false;
-    }
-    if (codecs[i].GetCodecType() == VideoCodec::CODEC_VIDEO) {
-      has_video = true;
-    }
-  }
-  if (!has_video) {
-    LOG(LS_ERROR) << "Setting codecs without a video codec is invalid: "
-                  << CodecVectorToString(codecs);
-    return false;
-  }
-  return true;
-}
-
-static bool ValidateStreamParams(const StreamParams& sp) {
-  if (sp.ssrcs.empty()) {
-    LOG(LS_ERROR) << "No SSRCs in stream parameters: " << sp.ToString();
-    return false;
-  }
-
-  std::vector<uint32_t> primary_ssrcs;
-  sp.GetPrimarySsrcs(&primary_ssrcs);
-  std::vector<uint32_t> rtx_ssrcs;
-  sp.GetFidSsrcs(primary_ssrcs, &rtx_ssrcs);
-  for (uint32_t rtx_ssrc : rtx_ssrcs) {
-    bool rtx_ssrc_present = false;
-    for (uint32_t sp_ssrc : sp.ssrcs) {
-      if (sp_ssrc == rtx_ssrc) {
-        rtx_ssrc_present = true;
-        break;
-      }
-    }
-    if (!rtx_ssrc_present) {
-      LOG(LS_ERROR) << "RTX SSRC '" << rtx_ssrc
-                    << "' missing from StreamParams ssrcs: " << sp.ToString();
-      return false;
-    }
-  }
-  if (!rtx_ssrcs.empty() && primary_ssrcs.size() != rtx_ssrcs.size()) {
-    LOG(LS_ERROR)
-        << "RTX SSRCs exist, but don't cover all SSRCs (unsupported): "
-        << sp.ToString();
-    return false;
-  }
-
-  return true;
-}
-
-// Returns true if the given codec is disallowed from doing simulcast.
-bool IsCodecBlacklistedForSimulcast(const std::string& codec_name) {
-  return CodecNamesEq(codec_name, kH264CodecName) ||
-         CodecNamesEq(codec_name, kVp9CodecName);
-}
-
-// The selected thresholds for QVGA and VGA corresponded to a QP around 10.
-// The change in QP declined above the selected bitrates.
-static int GetMaxDefaultVideoBitrateKbps(int width, int height) {
-  if (width * height <= 320 * 240) {
-    return 600;
-  } else if (width * height <= 640 * 480) {
-    return 1700;
-  } else if (width * height <= 960 * 540) {
-    return 2000;
-  } else {
-    return 2500;
-  }
-}
-
-bool GetVp9LayersFromFieldTrialGroup(int* num_spatial_layers,
-                                     int* num_temporal_layers) {
-  std::string group = webrtc::field_trial::FindFullName("WebRTC-SupportVP9SVC");
-  if (group.empty())
-    return false;
-
-  if (sscanf(group.c_str(), "EnabledByFlag_%dSL%dTL", num_spatial_layers,
-             num_temporal_layers) != 2) {
-    return false;
-  }
-  const int kMaxSpatialLayers = 2;
-  if (*num_spatial_layers > kMaxSpatialLayers || *num_spatial_layers < 1)
-    return false;
-
-  const int kMaxTemporalLayers = 3;
-  if (*num_temporal_layers > kMaxTemporalLayers || *num_temporal_layers < 1)
-    return false;
-
-  return true;
-}
-
-int GetDefaultVp9SpatialLayers() {
-  int num_sl;
-  int num_tl;
-  if (GetVp9LayersFromFieldTrialGroup(&num_sl, &num_tl)) {
-    return num_sl;
-  }
-  return 1;
-}
-
-int GetDefaultVp9TemporalLayers() {
-  int num_sl;
-  int num_tl;
-  if (GetVp9LayersFromFieldTrialGroup(&num_sl, &num_tl)) {
-    return num_tl;
-  }
-  return 1;
-}
-
-class EncoderStreamFactory
-    : public webrtc::VideoEncoderConfig::VideoStreamFactoryInterface {
- public:
-  EncoderStreamFactory(std::string codec_name,
-                       int max_qp,
-                       int max_framerate,
-                       bool is_screencast,
-                       bool conference_mode)
-      : codec_name_(codec_name),
-        max_qp_(max_qp),
-        max_framerate_(max_framerate),
-        is_screencast_(is_screencast),
-        conference_mode_(conference_mode) {}
-
- private:
-  std::vector<webrtc::VideoStream> CreateEncoderStreams(
-      int width,
-      int height,
-      const webrtc::VideoEncoderConfig& encoder_config) override {
-    if (is_screencast_ &&
-        (!conference_mode_ || !cricket::UseSimulcastScreenshare())) {
-      RTC_DCHECK_EQ(1, encoder_config.number_of_streams);
-    }
-    if (encoder_config.number_of_streams > 1 ||
-        (CodecNamesEq(codec_name_, kVp8CodecName) && is_screencast_ &&
-         conference_mode_)) {
-      return GetSimulcastConfig(encoder_config.number_of_streams, width, height,
-                                encoder_config.max_bitrate_bps, max_qp_,
-                                max_framerate_, is_screencast_);
-    }
-
-    // For unset max bitrates set default bitrate for non-simulcast.
-    int max_bitrate_bps =
-        (encoder_config.max_bitrate_bps > 0)
-            ? encoder_config.max_bitrate_bps
-            : GetMaxDefaultVideoBitrateKbps(width, height) * 1000;
-
-    webrtc::VideoStream stream;
-    stream.width = width;
-    stream.height = height;
-    stream.max_framerate = max_framerate_;
-    stream.min_bitrate_bps = kMinVideoBitrateKbps * 1000;
-    stream.target_bitrate_bps = stream.max_bitrate_bps = max_bitrate_bps;
-    stream.max_qp = max_qp_;
-
-    if (CodecNamesEq(codec_name_, kVp9CodecName) && !is_screencast_) {
-      stream.temporal_layer_thresholds_bps.resize(
-          GetDefaultVp9TemporalLayers() - 1);
-    }
-
-    std::vector<webrtc::VideoStream> streams;
-    streams.push_back(stream);
-    return streams;
-  }
-
-  const std::string codec_name_;
-  const int max_qp_;
-  const int max_framerate_;
-  const bool is_screencast_;
-  const bool conference_mode_;
-};
-
-}  // namespace
-
-// Constants defined in webrtc/media/engine/constants.h
-// TODO(pbos): Move these to a separate constants.cc file.
-const int kMinVideoBitrateKbps = 30;
-
-const int kVideoMtu = 1200;
-const int kVideoRtpBufferSize = 65536;
-
-// This constant is really an on/off, lower-level configurable NACK history
-// duration hasn't been implemented.
-static const int kNackHistoryMs = 1000;
-
-static const int kDefaultQpMax = 56;
-
-static const int kDefaultRtcpReceiverReportSsrc = 1;
-
-// Minimum time interval for logging stats.
-static const int64_t kStatsLogIntervalMs = 10000;
-
-static std::vector<VideoCodec> GetSupportedCodecs(
-    const WebRtcVideoEncoderFactory* external_encoder_factory);
-
-rtc::scoped_refptr<webrtc::VideoEncoderConfig::EncoderSpecificSettings>
-WebRtcVideoChannel2::WebRtcVideoSendStream::ConfigureVideoEncoderSettings(
-    const VideoCodec& codec) {
-  RTC_DCHECK_RUN_ON(&thread_checker_);
-  bool is_screencast = parameters_.options.is_screencast.value_or(false);
-  // No automatic resizing when using simulcast or screencast.
-  bool automatic_resize =
-      !is_screencast && parameters_.config.rtp.ssrcs.size() == 1;
-  bool frame_dropping = !is_screencast;
-  bool denoising;
-  bool codec_default_denoising = false;
-  if (is_screencast) {
-    denoising = false;
-  } else {
-    // Use codec default if video_noise_reduction is unset.
-    codec_default_denoising = !parameters_.options.video_noise_reduction;
-    denoising = parameters_.options.video_noise_reduction.value_or(false);
-  }
-
-  if (CodecNamesEq(codec.name, kH264CodecName)) {
-    webrtc::VideoCodecH264 h264_settings =
-        webrtc::VideoEncoder::GetDefaultH264Settings();
-    h264_settings.frameDroppingOn = frame_dropping;
-    return new rtc::RefCountedObject<
-        webrtc::VideoEncoderConfig::H264EncoderSpecificSettings>(h264_settings);
-  }
-  if (CodecNamesEq(codec.name, kVp8CodecName)) {
-    webrtc::VideoCodecVP8 vp8_settings =
-        webrtc::VideoEncoder::GetDefaultVp8Settings();
-    vp8_settings.automaticResizeOn = automatic_resize;
-    // VP8 denoising is enabled by default.
-    vp8_settings.denoisingOn = codec_default_denoising ? true : denoising;
-    vp8_settings.frameDroppingOn = frame_dropping;
-    return new rtc::RefCountedObject<
-        webrtc::VideoEncoderConfig::Vp8EncoderSpecificSettings>(vp8_settings);
-  }
-  if (CodecNamesEq(codec.name, kVp9CodecName)) {
-    webrtc::VideoCodecVP9 vp9_settings =
-        webrtc::VideoEncoder::GetDefaultVp9Settings();
-    if (is_screencast) {
-      // TODO(asapersson): Set to 2 for now since there is a DCHECK in
-      // VideoSendStream::ReconfigureVideoEncoder.
-      vp9_settings.numberOfSpatialLayers = 2;
-    } else {
-      vp9_settings.numberOfSpatialLayers = GetDefaultVp9SpatialLayers();
-    }
-    // VP9 denoising is disabled by default.
-    vp9_settings.denoisingOn = codec_default_denoising ? false : denoising;
-    vp9_settings.frameDroppingOn = frame_dropping;
-    return new rtc::RefCountedObject<
-        webrtc::VideoEncoderConfig::Vp9EncoderSpecificSettings>(vp9_settings);
-  }
-  return nullptr;
-}
-
-DefaultUnsignalledSsrcHandler::DefaultUnsignalledSsrcHandler()
-    : default_recv_ssrc_(0), default_sink_(NULL) {}
-
-UnsignalledSsrcHandler::Action DefaultUnsignalledSsrcHandler::OnUnsignalledSsrc(
-    WebRtcVideoChannel2* channel,
-    uint32_t ssrc) {
-  if (default_recv_ssrc_ != 0) {  // Already one default stream, so replace it.
-    channel->RemoveRecvStream(default_recv_ssrc_);
-    default_recv_ssrc_ = 0;
-  }
-
-  StreamParams sp;
-  sp.ssrcs.push_back(ssrc);
-  LOG(LS_INFO) << "Creating default receive stream for SSRC=" << ssrc << ".";
-  if (!channel->AddRecvStream(sp, true)) {
-    LOG(LS_WARNING) << "Could not create default receive stream.";
-  }
-
-  channel->SetSink(ssrc, default_sink_);
-  default_recv_ssrc_ = ssrc;
-  return kDeliverPacket;
-}
-
-rtc::VideoSinkInterface<webrtc::VideoFrame>*
-DefaultUnsignalledSsrcHandler::GetDefaultSink() const {
-  return default_sink_;
-}
-
-void DefaultUnsignalledSsrcHandler::SetDefaultSink(
-    VideoMediaChannel* channel,
-    rtc::VideoSinkInterface<webrtc::VideoFrame>* sink) {
-  default_sink_ = sink;
-  if (default_recv_ssrc_ != 0) {
-    channel->SetSink(default_recv_ssrc_, default_sink_);
-  }
-}
-
-WebRtcVideoEngine2::WebRtcVideoEngine2()
-    : initialized_(false),
-      external_decoder_factory_(NULL),
-      external_encoder_factory_(NULL) {
-  LOG(LS_INFO) << "WebRtcVideoEngine2::WebRtcVideoEngine2()";
-}
-
-WebRtcVideoEngine2::~WebRtcVideoEngine2() {
-  LOG(LS_INFO) << "WebRtcVideoEngine2::~WebRtcVideoEngine2";
-}
-
-void WebRtcVideoEngine2::Init() {
-  LOG(LS_INFO) << "WebRtcVideoEngine2::Init";
-  initialized_ = true;
-}
-
-WebRtcVideoChannel2* WebRtcVideoEngine2::CreateChannel(
-    webrtc::Call* call,
-    const MediaConfig& config,
-    const VideoOptions& options) {
-  RTC_DCHECK(initialized_);
-  LOG(LS_INFO) << "CreateChannel. Options: " << options.ToString();
-  return new WebRtcVideoChannel2(call, config, options,
-                                 external_encoder_factory_,
-                                 external_decoder_factory_);
-}
-
-std::vector<VideoCodec> WebRtcVideoEngine2::codecs() const {
-  return GetSupportedCodecs(external_encoder_factory_);
-}
-
-RtpCapabilities WebRtcVideoEngine2::GetCapabilities() const {
-  RtpCapabilities capabilities;
-  capabilities.header_extensions.push_back(
-      webrtc::RtpExtension(webrtc::RtpExtension::kTimestampOffsetUri,
-                           webrtc::RtpExtension::kTimestampOffsetDefaultId));
-  capabilities.header_extensions.push_back(
-      webrtc::RtpExtension(webrtc::RtpExtension::kAbsSendTimeUri,
-                           webrtc::RtpExtension::kAbsSendTimeDefaultId));
-  capabilities.header_extensions.push_back(
-      webrtc::RtpExtension(webrtc::RtpExtension::kVideoRotationUri,
-                           webrtc::RtpExtension::kVideoRotationDefaultId));
-  capabilities.header_extensions.push_back(webrtc::RtpExtension(
-      webrtc::RtpExtension::kTransportSequenceNumberUri,
-      webrtc::RtpExtension::kTransportSequenceNumberDefaultId));
-  capabilities.header_extensions.push_back(
-      webrtc::RtpExtension(webrtc::RtpExtension::kPlayoutDelayUri,
-                           webrtc::RtpExtension::kPlayoutDelayDefaultId));
-  return capabilities;
-}
-
-void WebRtcVideoEngine2::SetExternalDecoderFactory(
-    WebRtcVideoDecoderFactory* decoder_factory) {
-  RTC_DCHECK(!initialized_);
-  external_decoder_factory_ = decoder_factory;
-}
-
-void WebRtcVideoEngine2::SetExternalEncoderFactory(
-    WebRtcVideoEncoderFactory* encoder_factory) {
-  RTC_DCHECK(!initialized_);
-  if (external_encoder_factory_ == encoder_factory)
-    return;
-
-  // No matter what happens we shouldn't hold on to a stale
-  // WebRtcSimulcastEncoderFactory.
-  simulcast_encoder_factory_.reset();
-
-  if (encoder_factory &&
-      WebRtcSimulcastEncoderFactory::UseSimulcastEncoderFactory(
-          encoder_factory->supported_codecs())) {
-    simulcast_encoder_factory_.reset(
-        new WebRtcSimulcastEncoderFactory(encoder_factory));
-    encoder_factory = simulcast_encoder_factory_.get();
-  }
-  external_encoder_factory_ = encoder_factory;
-}
-
-// This is a helper function for AppendVideoCodecs below. It will return the
-// first unused dynamic payload type (in the range [96, 127]), or nothing if no
-// payload type is unused.
-static rtc::Optional<int> NextFreePayloadType(
-    const std::vector<VideoCodec>& codecs) {
-  static const int kFirstDynamicPayloadType = 96;
-  static const int kLastDynamicPayloadType = 127;
-  bool is_payload_used[1 + kLastDynamicPayloadType - kFirstDynamicPayloadType] =
-      {false};
-  for (const VideoCodec& codec : codecs) {
-    if (kFirstDynamicPayloadType <= codec.id &&
-        codec.id <= kLastDynamicPayloadType) {
-      is_payload_used[codec.id - kFirstDynamicPayloadType] = true;
-    }
-  }
-  for (int i = kFirstDynamicPayloadType; i <= kLastDynamicPayloadType; ++i) {
-    if (!is_payload_used[i - kFirstDynamicPayloadType])
-      return rtc::Optional<int>(i);
-  }
-  // No free payload type.
-  return rtc::Optional<int>();
-}
-
-// This is a helper function for GetSupportedCodecs below. It will append new
-// unique codecs from |input_codecs| to |unified_codecs|. It will add default
-// feedback params to the codecs and will also add an associated RTX codec for
-// recognized codecs (VP8, VP9, H264, and RED).
-static void AppendVideoCodecs(const std::vector<VideoCodec>& input_codecs,
-                              std::vector<VideoCodec>* unified_codecs) {
-  for (VideoCodec codec : input_codecs) {
-    const rtc::Optional<int> payload_type =
-        NextFreePayloadType(*unified_codecs);
-    if (!payload_type)
-      return;
-    codec.id = *payload_type;
-    // TODO(magjed): Move the responsibility of setting these parameters to the
-    // encoder factories instead.
-    if (codec.name != kRedCodecName && codec.name != kUlpfecCodecName &&
-        codec.name != kFlexfecCodecName)
-      AddDefaultFeedbackParams(&codec);
-    // Don't add same codec twice.
-    if (FindMatchingCodec(*unified_codecs, codec))
-      continue;
-
-    unified_codecs->push_back(codec);
-
-    // Add associated RTX codec for recognized codecs.
-    // TODO(deadbeef): Should we add RTX codecs for external codecs whose names
-    // we don't recognize?
-    if (CodecNamesEq(codec.name, kVp8CodecName) ||
-        CodecNamesEq(codec.name, kVp9CodecName) ||
-        CodecNamesEq(codec.name, kH264CodecName) ||
-        CodecNamesEq(codec.name, kRedCodecName)) {
-      const rtc::Optional<int> rtx_payload_type =
-          NextFreePayloadType(*unified_codecs);
-      if (!rtx_payload_type)
-        return;
-      unified_codecs->push_back(
-          VideoCodec::CreateRtxCodec(*rtx_payload_type, codec.id));
-    }
-  }
-}
-
-static std::vector<VideoCodec> GetSupportedCodecs(
-    const WebRtcVideoEncoderFactory* external_encoder_factory) {
-  const std::vector<VideoCodec> internal_codecs =
-      InternalEncoderFactory().supported_codecs();
-  LOG(LS_INFO) << "Internally supported codecs: "
-               << CodecVectorToString(internal_codecs);
-
-  std::vector<VideoCodec> unified_codecs;
-  AppendVideoCodecs(internal_codecs, &unified_codecs);
-
-  if (external_encoder_factory != nullptr) {
-    const std::vector<VideoCodec>& external_codecs =
-        external_encoder_factory->supported_codecs();
-    AppendVideoCodecs(external_codecs, &unified_codecs);
-    LOG(LS_INFO) << "Codecs supported by the external encoder factory: "
-                 << CodecVectorToString(external_codecs);
-  }
-
-  return unified_codecs;
-}
-
-WebRtcVideoChannel2::WebRtcVideoChannel2(
-    webrtc::Call* call,
-    const MediaConfig& config,
-    const VideoOptions& options,
-    WebRtcVideoEncoderFactory* external_encoder_factory,
-    WebRtcVideoDecoderFactory* external_decoder_factory)
-    : VideoMediaChannel(config),
-      call_(call),
-      unsignalled_ssrc_handler_(&default_unsignalled_ssrc_handler_),
-      video_config_(config.video),
-      external_encoder_factory_(external_encoder_factory),
-      external_decoder_factory_(external_decoder_factory),
-      default_send_options_(options),
-      last_stats_log_ms_(-1) {
-  RTC_DCHECK(thread_checker_.CalledOnValidThread());
-
-  rtcp_receiver_report_ssrc_ = kDefaultRtcpReceiverReportSsrc;
-  sending_ = false;
-  recv_codecs_ = MapCodecs(GetSupportedCodecs(external_encoder_factory));
-}
-
-WebRtcVideoChannel2::~WebRtcVideoChannel2() {
-  for (auto& kv : send_streams_)
-    delete kv.second;
-  for (auto& kv : receive_streams_)
-    delete kv.second;
-}
-
-rtc::Optional<WebRtcVideoChannel2::VideoCodecSettings>
-WebRtcVideoChannel2::SelectSendVideoCodec(
-    const std::vector<VideoCodecSettings>& remote_mapped_codecs) const {
-  const std::vector<VideoCodec> local_supported_codecs =
-      GetSupportedCodecs(external_encoder_factory_);
-  // Select the first remote codec that is supported locally.
-  for (const VideoCodecSettings& remote_mapped_codec : remote_mapped_codecs) {
-    // For H264, we will limit the encode level to the remote offered level
-    // regardless if level asymmetry is allowed or not. This is strictly not
-    // following the spec in https://tools.ietf.org/html/rfc6184#section-8.2.2
-    // since we should limit the encode level to the lower of local and remote
-    // level when level asymmetry is not allowed.
-    if (FindMatchingCodec(local_supported_codecs, remote_mapped_codec.codec))
-      return rtc::Optional<VideoCodecSettings>(remote_mapped_codec);
-  }
-  // No remote codec was supported.
-  return rtc::Optional<VideoCodecSettings>();
-}
-
-bool WebRtcVideoChannel2::ReceiveCodecsHaveChanged(
-    std::vector<VideoCodecSettings> before,
-    std::vector<VideoCodecSettings> after) {
-  if (before.size() != after.size()) {
-    return true;
-  }
-  // The receive codec order doesn't matter, so we sort the codecs before
-  // comparing. This is necessary because currently the
-  // only way to change the send codec is to munge SDP, which causes
-  // the receive codec list to change order, which causes the streams
-  // to be recreates which causes a "blink" of black video.  In order
-  // to support munging the SDP in this way without recreating receive
-  // streams, we ignore the order of the received codecs so that
-  // changing the order doesn't cause this "blink".
-  auto comparison =
-      [](const VideoCodecSettings& codec1, const VideoCodecSettings& codec2) {
-        return codec1.codec.id > codec2.codec.id;
-      };
-  std::sort(before.begin(), before.end(), comparison);
-  std::sort(after.begin(), after.end(), comparison);
-  return before != after;
-}
-
-bool WebRtcVideoChannel2::GetChangedSendParameters(
-    const VideoSendParameters& params,
-    ChangedSendParameters* changed_params) const {
-  if (!ValidateCodecFormats(params.codecs) ||
-      !ValidateRtpExtensions(params.extensions)) {
-    return false;
-  }
-
-  // Select one of the remote codecs that will be used as send codec.
-  const rtc::Optional<VideoCodecSettings> selected_send_codec =
-      SelectSendVideoCodec(MapCodecs(params.codecs));
-
-  if (!selected_send_codec) {
-    LOG(LS_ERROR) << "No video codecs supported.";
-    return false;
-  }
-
-  if (!send_codec_ || *selected_send_codec != *send_codec_)
-    changed_params->codec = selected_send_codec;
-
-  // Handle RTP header extensions.
-  std::vector<webrtc::RtpExtension> filtered_extensions = FilterRtpExtensions(
-      params.extensions, webrtc::RtpExtension::IsSupportedForVideo, true);
-  if (!send_rtp_extensions_ || (*send_rtp_extensions_ != filtered_extensions)) {
-    changed_params->rtp_header_extensions =
-        rtc::Optional<std::vector<webrtc::RtpExtension>>(filtered_extensions);
-  }
-
-  // Handle max bitrate.
-  if (params.max_bandwidth_bps != send_params_.max_bandwidth_bps &&
-      params.max_bandwidth_bps >= 0) {
-    // 0 uncaps max bitrate (-1).
-    changed_params->max_bandwidth_bps = rtc::Optional<int>(
-        params.max_bandwidth_bps == 0 ? -1 : params.max_bandwidth_bps);
-  }
-
-  // Handle conference mode.
-  if (params.conference_mode != send_params_.conference_mode) {
-    changed_params->conference_mode =
-        rtc::Optional<bool>(params.conference_mode);
-  }
-
-  // Handle RTCP mode.
-  if (params.rtcp.reduced_size != send_params_.rtcp.reduced_size) {
-    changed_params->rtcp_mode = rtc::Optional<webrtc::RtcpMode>(
-        params.rtcp.reduced_size ? webrtc::RtcpMode::kReducedSize
-                                 : webrtc::RtcpMode::kCompound);
-  }
-
-  return true;
-}
-
-rtc::DiffServCodePoint WebRtcVideoChannel2::PreferredDscp() const {
-  return rtc::DSCP_AF41;
-}
-
-bool WebRtcVideoChannel2::SetSendParameters(const VideoSendParameters& params) {
-  TRACE_EVENT0("webrtc", "WebRtcVideoChannel2::SetSendParameters");
-  LOG(LS_INFO) << "SetSendParameters: " << params.ToString();
-  ChangedSendParameters changed_params;
-  if (!GetChangedSendParameters(params, &changed_params)) {
-    return false;
-  }
-
-  if (changed_params.codec) {
-    const VideoCodecSettings& codec_settings = *changed_params.codec;
-    send_codec_ = rtc::Optional<VideoCodecSettings>(codec_settings);
-    LOG(LS_INFO) << "Using codec: " << codec_settings.codec.ToString();
-  }
-
-  if (changed_params.rtp_header_extensions) {
-    send_rtp_extensions_ = changed_params.rtp_header_extensions;
-  }
-
-  if (changed_params.codec || changed_params.max_bandwidth_bps) {
-    if (send_codec_) {
-      // TODO(holmer): Changing the codec parameters shouldn't necessarily mean
-      // that we change the min/max of bandwidth estimation. Reevaluate this.
-      bitrate_config_ = GetBitrateConfigForCodec(send_codec_->codec);
-      if (!changed_params.codec) {
-        // If the codec isn't changing, set the start bitrate to -1 which means
-        // "unchanged" so that BWE isn't affected.
-        bitrate_config_.start_bitrate_bps = -1;
-      }
-    }
-    if (params.max_bandwidth_bps >= 0) {
-      // Note that max_bandwidth_bps intentionally takes priority over the
-      // bitrate config for the codec. This allows FEC to be applied above the
-      // codec target bitrate.
-      // TODO(pbos): Figure out whether b=AS means max bitrate for this
-      // WebRtcVideoChannel2 (in which case we're good), or per sender (SSRC),
-      // in which case this should not set a Call::BitrateConfig but rather
-      // reconfigure all senders.
-      bitrate_config_.max_bitrate_bps =
-          params.max_bandwidth_bps == 0 ? -1 : params.max_bandwidth_bps;
-    }
-    call_->SetBitrateConfig(bitrate_config_);
-  }
-
-  {
-    rtc::CritScope stream_lock(&stream_crit_);
-    for (auto& kv : send_streams_) {
-      kv.second->SetSendParameters(changed_params);
-    }
-    if (changed_params.codec || changed_params.rtcp_mode) {
-      // Update receive feedback parameters from new codec or RTCP mode.
-      LOG(LS_INFO)
-          << "SetFeedbackOptions on all the receive streams because the send "
-             "codec or RTCP mode has changed.";
-      for (auto& kv : receive_streams_) {
-        RTC_DCHECK(kv.second != nullptr);
-        kv.second->SetFeedbackParameters(
-            HasNack(send_codec_->codec), HasRemb(send_codec_->codec),
-            HasTransportCc(send_codec_->codec),
-            params.rtcp.reduced_size ? webrtc::RtcpMode::kReducedSize
-                                     : webrtc::RtcpMode::kCompound);
-      }
-    }
-  }
-  send_params_ = params;
-  return true;
-}
-
-webrtc::RtpParameters WebRtcVideoChannel2::GetRtpSendParameters(
-    uint32_t ssrc) const {
-  rtc::CritScope stream_lock(&stream_crit_);
-  auto it = send_streams_.find(ssrc);
-  if (it == send_streams_.end()) {
-    LOG(LS_WARNING) << "Attempting to get RTP send parameters for stream "
-                    << "with ssrc " << ssrc << " which doesn't exist.";
-    return webrtc::RtpParameters();
-  }
-
-  webrtc::RtpParameters rtp_params = it->second->GetRtpParameters();
-  // Need to add the common list of codecs to the send stream-specific
-  // RTP parameters.
-  for (const VideoCodec& codec : send_params_.codecs) {
-    rtp_params.codecs.push_back(codec.ToCodecParameters());
-  }
-  return rtp_params;
-}
-
-bool WebRtcVideoChannel2::SetRtpSendParameters(
-    uint32_t ssrc,
-    const webrtc::RtpParameters& parameters) {
-  TRACE_EVENT0("webrtc", "WebRtcVideoChannel2::SetRtpSendParameters");
-  rtc::CritScope stream_lock(&stream_crit_);
-  auto it = send_streams_.find(ssrc);
-  if (it == send_streams_.end()) {
-    LOG(LS_ERROR) << "Attempting to set RTP send parameters for stream "
-                  << "with ssrc " << ssrc << " which doesn't exist.";
-    return false;
-  }
-
-  // TODO(deadbeef): Handle setting parameters with a list of codecs in a
-  // different order (which should change the send codec).
-  webrtc::RtpParameters current_parameters = GetRtpSendParameters(ssrc);
-  if (current_parameters.codecs != parameters.codecs) {
-    LOG(LS_ERROR) << "Using SetParameters to change the set of codecs "
-                  << "is not currently supported.";
-    return false;
-  }
-
-  return it->second->SetRtpParameters(parameters);
-}
-
-webrtc::RtpParameters WebRtcVideoChannel2::GetRtpReceiveParameters(
-    uint32_t ssrc) const {
-  rtc::CritScope stream_lock(&stream_crit_);
-  auto it = receive_streams_.find(ssrc);
-  if (it == receive_streams_.end()) {
-    LOG(LS_WARNING) << "Attempting to get RTP receive parameters for stream "
-                    << "with ssrc " << ssrc << " which doesn't exist.";
-    return webrtc::RtpParameters();
-  }
-
-  // TODO(deadbeef): Return stream-specific parameters.
-  webrtc::RtpParameters rtp_params = CreateRtpParametersWithOneEncoding();
-  for (const VideoCodec& codec : recv_params_.codecs) {
-    rtp_params.codecs.push_back(codec.ToCodecParameters());
-  }
-  rtp_params.encodings[0].ssrc = it->second->GetFirstPrimarySsrc();
-  return rtp_params;
-}
-
-bool WebRtcVideoChannel2::SetRtpReceiveParameters(
-    uint32_t ssrc,
-    const webrtc::RtpParameters& parameters) {
-  TRACE_EVENT0("webrtc", "WebRtcVideoChannel2::SetRtpReceiveParameters");
-  rtc::CritScope stream_lock(&stream_crit_);
-  auto it = receive_streams_.find(ssrc);
-  if (it == receive_streams_.end()) {
-    LOG(LS_ERROR) << "Attempting to set RTP receive parameters for stream "
-                  << "with ssrc " << ssrc << " which doesn't exist.";
-    return false;
-  }
-
-  webrtc::RtpParameters current_parameters = GetRtpReceiveParameters(ssrc);
-  if (current_parameters != parameters) {
-    LOG(LS_ERROR) << "Changing the RTP receive parameters is currently "
-                  << "unsupported.";
-    return false;
-  }
-  return true;
-}
-
-bool WebRtcVideoChannel2::GetChangedRecvParameters(
-    const VideoRecvParameters& params,
-    ChangedRecvParameters* changed_params) const {
-  if (!ValidateCodecFormats(params.codecs) ||
-      !ValidateRtpExtensions(params.extensions)) {
-    return false;
-  }
-
-  // Handle receive codecs.
-  const std::vector<VideoCodecSettings> mapped_codecs =
-      MapCodecs(params.codecs);
-  if (mapped_codecs.empty()) {
-    LOG(LS_ERROR) << "SetRecvParameters called without any video codecs.";
-    return false;
-  }
-
-  // Verify that every mapped codec is supported locally.
-  const std::vector<VideoCodec> local_supported_codecs =
-      GetSupportedCodecs(external_encoder_factory_);
-  for (const VideoCodecSettings& mapped_codec : mapped_codecs) {
-    if (!FindMatchingCodec(local_supported_codecs, mapped_codec.codec)) {
-      LOG(LS_ERROR) << "SetRecvParameters called with unsupported video codec: "
-                    << mapped_codec.codec.ToString();
-      return false;
-    }
-  }
-
-  if (ReceiveCodecsHaveChanged(recv_codecs_, mapped_codecs)) {
-    changed_params->codec_settings =
-        rtc::Optional<std::vector<VideoCodecSettings>>(mapped_codecs);
-  }
-
-  // Handle RTP header extensions.
-  std::vector<webrtc::RtpExtension> filtered_extensions = FilterRtpExtensions(
-      params.extensions, webrtc::RtpExtension::IsSupportedForVideo, false);
-  if (filtered_extensions != recv_rtp_extensions_) {
-    changed_params->rtp_header_extensions =
-        rtc::Optional<std::vector<webrtc::RtpExtension>>(filtered_extensions);
-  }
-
-  return true;
-}
-
-bool WebRtcVideoChannel2::SetRecvParameters(const VideoRecvParameters& params) {
-  TRACE_EVENT0("webrtc", "WebRtcVideoChannel2::SetRecvParameters");
-  LOG(LS_INFO) << "SetRecvParameters: " << params.ToString();
-  ChangedRecvParameters changed_params;
-  if (!GetChangedRecvParameters(params, &changed_params)) {
-    return false;
-  }
-  if (changed_params.rtp_header_extensions) {
-    recv_rtp_extensions_ = *changed_params.rtp_header_extensions;
-  }
-  if (changed_params.codec_settings) {
-    LOG(LS_INFO) << "Changing recv codecs from "
-                 << CodecSettingsVectorToString(recv_codecs_) << " to "
-                 << CodecSettingsVectorToString(*changed_params.codec_settings);
-    recv_codecs_ = *changed_params.codec_settings;
-  }
-
-  {
-    rtc::CritScope stream_lock(&stream_crit_);
-    for (auto& kv : receive_streams_) {
-      kv.second->SetRecvParameters(changed_params);
-    }
-  }
-  recv_params_ = params;
-  return true;
-}
-
-std::string WebRtcVideoChannel2::CodecSettingsVectorToString(
-    const std::vector<VideoCodecSettings>& codecs) {
-  std::stringstream out;
-  out << '{';
-  for (size_t i = 0; i < codecs.size(); ++i) {
-    out << codecs[i].codec.ToString();
-    if (i != codecs.size() - 1) {
-      out << ", ";
-    }
-  }
-  out << '}';
-  return out.str();
-}
-
-bool WebRtcVideoChannel2::GetSendCodec(VideoCodec* codec) {
-  if (!send_codec_) {
-    LOG(LS_VERBOSE) << "GetSendCodec: No send codec set.";
-    return false;
-  }
-  *codec = send_codec_->codec;
-  return true;
-}
-
-bool WebRtcVideoChannel2::SetSend(bool send) {
-  TRACE_EVENT0("webrtc", "WebRtcVideoChannel2::SetSend");
-  LOG(LS_VERBOSE) << "SetSend: " << (send ? "true" : "false");
-  if (send && !send_codec_) {
-    LOG(LS_ERROR) << "SetSend(true) called before setting codec.";
-    return false;
-  }
-  {
-    rtc::CritScope stream_lock(&stream_crit_);
-    for (const auto& kv : send_streams_) {
-      kv.second->SetSend(send);
-    }
-  }
-  sending_ = send;
-  return true;
-}
-
-// TODO(nisse): The enable argument was used for mute logic which has
-// been moved to VideoBroadcaster. So remove the argument from this
-// method.
-bool WebRtcVideoChannel2::SetVideoSend(
-    uint32_t ssrc,
-    bool enable,
-    const VideoOptions* options,
-    rtc::VideoSourceInterface<webrtc::VideoFrame>* source) {
-  TRACE_EVENT0("webrtc", "SetVideoSend");
-  RTC_DCHECK(ssrc != 0);
-  LOG(LS_INFO) << "SetVideoSend (ssrc= " << ssrc << ", enable = " << enable
-               << ", options: " << (options ? options->ToString() : "nullptr")
-               << ", source = " << (source ? "(source)" : "nullptr") << ")";
-
-  rtc::CritScope stream_lock(&stream_crit_);
-  const auto& kv = send_streams_.find(ssrc);
-  if (kv == send_streams_.end()) {
-    // Allow unknown ssrc only if source is null.
-    RTC_CHECK(source == nullptr);
-    LOG(LS_ERROR) << "No sending stream on ssrc " << ssrc;
-    return false;
-  }
-
-  return kv->second->SetVideoSend(enable, options, source);
-}
-
-bool WebRtcVideoChannel2::ValidateSendSsrcAvailability(
-    const StreamParams& sp) const {
-  for (uint32_t ssrc : sp.ssrcs) {
-    if (send_ssrcs_.find(ssrc) != send_ssrcs_.end()) {
-      LOG(LS_ERROR) << "Send stream with SSRC '" << ssrc << "' already exists.";
-      return false;
-    }
-  }
-  return true;
-}
-
-bool WebRtcVideoChannel2::ValidateReceiveSsrcAvailability(
-    const StreamParams& sp) const {
-  for (uint32_t ssrc : sp.ssrcs) {
-    if (receive_ssrcs_.find(ssrc) != receive_ssrcs_.end()) {
-      LOG(LS_ERROR) << "Receive stream with SSRC '" << ssrc
-                    << "' already exists.";
-      return false;
-    }
-  }
-  return true;
-}
-
-bool WebRtcVideoChannel2::AddSendStream(const StreamParams& sp) {
-  LOG(LS_INFO) << "AddSendStream: " << sp.ToString();
-  if (!ValidateStreamParams(sp))
-    return false;
-
-  rtc::CritScope stream_lock(&stream_crit_);
-
-  if (!ValidateSendSsrcAvailability(sp))
-    return false;
-
-  for (uint32_t used_ssrc : sp.ssrcs)
-    send_ssrcs_.insert(used_ssrc);
-
-  webrtc::VideoSendStream::Config config(this);
-  config.suspend_below_min_bitrate = video_config_.suspend_below_min_bitrate;
-  config.periodic_alr_bandwidth_probing =
-      video_config_.periodic_alr_bandwidth_probing;
-  WebRtcVideoSendStream* stream = new WebRtcVideoSendStream(
-      call_, sp, std::move(config), default_send_options_,
-      external_encoder_factory_, video_config_.enable_cpu_overuse_detection,
-      bitrate_config_.max_bitrate_bps, send_codec_, send_rtp_extensions_,
-      send_params_);
-
-  uint32_t ssrc = sp.first_ssrc();
-  RTC_DCHECK(ssrc != 0);
-  send_streams_[ssrc] = stream;
-
-  if (rtcp_receiver_report_ssrc_ == kDefaultRtcpReceiverReportSsrc) {
-    rtcp_receiver_report_ssrc_ = ssrc;
-    LOG(LS_INFO) << "SetLocalSsrc on all the receive streams because we added "
-                    "a send stream.";
-    for (auto& kv : receive_streams_)
-      kv.second->SetLocalSsrc(ssrc);
-  }
-  if (sending_) {
-    stream->SetSend(true);
-  }
-
-  return true;
-}
-
-bool WebRtcVideoChannel2::RemoveSendStream(uint32_t ssrc) {
-  LOG(LS_INFO) << "RemoveSendStream: " << ssrc;
-
-  WebRtcVideoSendStream* removed_stream;
-  {
-    rtc::CritScope stream_lock(&stream_crit_);
-    std::map<uint32_t, WebRtcVideoSendStream*>::iterator it =
-        send_streams_.find(ssrc);
-    if (it == send_streams_.end()) {
-      return false;
-    }
-
-    for (uint32_t old_ssrc : it->second->GetSsrcs())
-      send_ssrcs_.erase(old_ssrc);
-
-    removed_stream = it->second;
-    send_streams_.erase(it);
-
-    // Switch receiver report SSRCs, the one in use is no longer valid.
-    if (rtcp_receiver_report_ssrc_ == ssrc) {
-      rtcp_receiver_report_ssrc_ = send_streams_.empty()
-                                       ? kDefaultRtcpReceiverReportSsrc
-                                       : send_streams_.begin()->first;
-      LOG(LS_INFO) << "SetLocalSsrc on all the receive streams because the "
-                      "previous local SSRC was removed.";
-
-      for (auto& kv : receive_streams_) {
-        kv.second->SetLocalSsrc(rtcp_receiver_report_ssrc_);
-      }
-    }
-  }
-
-  delete removed_stream;
-
-  return true;
-}
-
-void WebRtcVideoChannel2::DeleteReceiveStream(
-    WebRtcVideoChannel2::WebRtcVideoReceiveStream* stream) {
-  for (uint32_t old_ssrc : stream->GetSsrcs())
-    receive_ssrcs_.erase(old_ssrc);
-  delete stream;
-}
-
-bool WebRtcVideoChannel2::AddRecvStream(const StreamParams& sp) {
-  return AddRecvStream(sp, false);
-}
-
-bool WebRtcVideoChannel2::AddRecvStream(const StreamParams& sp,
-                                        bool default_stream) {
-  RTC_DCHECK(thread_checker_.CalledOnValidThread());
-
-  LOG(LS_INFO) << "AddRecvStream" << (default_stream ? " (default stream)" : "")
-               << ": " << sp.ToString();
-  if (!ValidateStreamParams(sp))
-    return false;
-
-  uint32_t ssrc = sp.first_ssrc();
-  RTC_DCHECK(ssrc != 0);  // TODO(pbos): Is this ever valid?
-
-  rtc::CritScope stream_lock(&stream_crit_);
-  // Remove running stream if this was a default stream.
-  const auto& prev_stream = receive_streams_.find(ssrc);
-  if (prev_stream != receive_streams_.end()) {
-    if (default_stream || !prev_stream->second->IsDefaultStream()) {
-      LOG(LS_ERROR) << "Receive stream for SSRC '" << ssrc
-                    << "' already exists.";
-      return false;
-    }
-    DeleteReceiveStream(prev_stream->second);
-    receive_streams_.erase(prev_stream);
-  }
-
-  if (!ValidateReceiveSsrcAvailability(sp))
-    return false;
-
-  for (uint32_t used_ssrc : sp.ssrcs)
-    receive_ssrcs_.insert(used_ssrc);
-
-  webrtc::VideoReceiveStream::Config config(this);
-  webrtc::FlexfecReceiveStream::Config flexfec_config(this);
-  ConfigureReceiverRtp(&config, &flexfec_config, sp);
-
-  config.disable_prerenderer_smoothing =
-      video_config_.disable_prerenderer_smoothing;
-  config.sync_group = sp.sync_label;
-
-  receive_streams_[ssrc] = new WebRtcVideoReceiveStream(
-      call_, sp, std::move(config), external_decoder_factory_, default_stream,
-      recv_codecs_, flexfec_config);
-
-  return true;
-}
-
-void WebRtcVideoChannel2::ConfigureReceiverRtp(
-    webrtc::VideoReceiveStream::Config* config,
-    webrtc::FlexfecReceiveStream::Config* flexfec_config,
-    const StreamParams& sp) const {
-  uint32_t ssrc = sp.first_ssrc();
-
-  config->rtp.remote_ssrc = ssrc;
-  config->rtp.local_ssrc = rtcp_receiver_report_ssrc_;
-
-  // TODO(pbos): This protection is against setting the same local ssrc as
-  // remote which is not permitted by the lower-level API. RTCP requires a
-  // corresponding sender SSRC. Figure out what to do when we don't have
-  // (receive-only) or know a good local SSRC.
-  if (config->rtp.remote_ssrc == config->rtp.local_ssrc) {
-    if (config->rtp.local_ssrc != kDefaultRtcpReceiverReportSsrc) {
-      config->rtp.local_ssrc = kDefaultRtcpReceiverReportSsrc;
-    } else {
-      config->rtp.local_ssrc = kDefaultRtcpReceiverReportSsrc + 1;
-    }
-  }
-
-  // Whether or not the receive stream sends reduced size RTCP is determined
-  // by the send params.
-  // TODO(deadbeef): Once we change "send_params" to "sender_params" and
-  // "recv_params" to "receiver_params", we should get this out of
-  // receiver_params_.
-  config->rtp.rtcp_mode = send_params_.rtcp.reduced_size
-                              ? webrtc::RtcpMode::kReducedSize
-                              : webrtc::RtcpMode::kCompound;
-
-  config->rtp.remb = send_codec_ ? HasRemb(send_codec_->codec) : false;
-  config->rtp.transport_cc =
-      send_codec_ ? HasTransportCc(send_codec_->codec) : false;
-
-  sp.GetFidSsrc(ssrc, &config->rtp.rtx_ssrc);
-
-  config->rtp.extensions = recv_rtp_extensions_;
-
-  // TODO(brandtr): Generalize when we add support for multistream protection.
-  if (sp.GetFecFrSsrc(ssrc, &flexfec_config->remote_ssrc)) {
-    flexfec_config->protected_media_ssrcs = {ssrc};
-    flexfec_config->local_ssrc = config->rtp.local_ssrc;
-    flexfec_config->rtcp_mode = config->rtp.rtcp_mode;
-    // TODO(brandtr): We should be spec-compliant and set |transport_cc| here
-    // based on the rtcp-fb for the FlexFEC codec, not the media codec.
-    flexfec_config->transport_cc = config->rtp.transport_cc;
-    flexfec_config->rtp_header_extensions = config->rtp.extensions;
-  }
-}
-
-bool WebRtcVideoChannel2::RemoveRecvStream(uint32_t ssrc) {
-  LOG(LS_INFO) << "RemoveRecvStream: " << ssrc;
-  if (ssrc == 0) {
-    LOG(LS_ERROR) << "RemoveRecvStream with 0 ssrc is not supported.";
-    return false;
-  }
-
-  rtc::CritScope stream_lock(&stream_crit_);
-  std::map<uint32_t, WebRtcVideoReceiveStream*>::iterator stream =
-      receive_streams_.find(ssrc);
-  if (stream == receive_streams_.end()) {
-    LOG(LS_ERROR) << "Stream not found for ssrc: " << ssrc;
-    return false;
-  }
-  DeleteReceiveStream(stream->second);
-  receive_streams_.erase(stream);
-
-  return true;
-}
-
-bool WebRtcVideoChannel2::SetSink(
-    uint32_t ssrc,
-    rtc::VideoSinkInterface<webrtc::VideoFrame>* sink) {
-  LOG(LS_INFO) << "SetSink: ssrc:" << ssrc << " "
-               << (sink ? "(ptr)" : "nullptr");
-  if (ssrc == 0) {
-    default_unsignalled_ssrc_handler_.SetDefaultSink(this, sink);
-    return true;
-  }
-
-  rtc::CritScope stream_lock(&stream_crit_);
-  std::map<uint32_t, WebRtcVideoReceiveStream*>::iterator it =
-      receive_streams_.find(ssrc);
-  if (it == receive_streams_.end()) {
-    return false;
-  }
-
-  it->second->SetSink(sink);
-  return true;
-}
-
-bool WebRtcVideoChannel2::GetStats(VideoMediaInfo* info) {
-  TRACE_EVENT0("webrtc", "WebRtcVideoChannel2::GetStats");
-
-  // Log stats periodically.
-  bool log_stats = false;
-  int64_t now_ms = rtc::TimeMillis();
-  if (last_stats_log_ms_ == -1 ||
-      now_ms - last_stats_log_ms_ > kStatsLogIntervalMs) {
-    last_stats_log_ms_ = now_ms;
-    log_stats = true;
-  }
-
-  info->Clear();
-  FillSenderStats(info, log_stats);
-  FillReceiverStats(info, log_stats);
-  FillSendAndReceiveCodecStats(info);
-  webrtc::Call::Stats stats = call_->GetStats();
-  FillBandwidthEstimationStats(stats, info);
-  if (stats.rtt_ms != -1) {
-    for (size_t i = 0; i < info->senders.size(); ++i) {
-      info->senders[i].rtt_ms = stats.rtt_ms;
-    }
-  }
-
-  if (log_stats)
-    LOG(LS_INFO) << stats.ToString(now_ms);
-
-  return true;
-}
-
-void WebRtcVideoChannel2::FillSenderStats(VideoMediaInfo* video_media_info,
-                                          bool log_stats) {
-  rtc::CritScope stream_lock(&stream_crit_);
-  for (std::map<uint32_t, WebRtcVideoSendStream*>::iterator it =
-           send_streams_.begin();
-       it != send_streams_.end(); ++it) {
-    video_media_info->senders.push_back(
-        it->second->GetVideoSenderInfo(log_stats));
-  }
-}
-
-void WebRtcVideoChannel2::FillReceiverStats(VideoMediaInfo* video_media_info,
-                                            bool log_stats) {
-  rtc::CritScope stream_lock(&stream_crit_);
-  for (std::map<uint32_t, WebRtcVideoReceiveStream*>::iterator it =
-           receive_streams_.begin();
-       it != receive_streams_.end(); ++it) {
-    video_media_info->receivers.push_back(
-        it->second->GetVideoReceiverInfo(log_stats));
-  }
-}
-
-void WebRtcVideoChannel2::FillBandwidthEstimationStats(
-    const webrtc::Call::Stats& stats,
-    VideoMediaInfo* video_media_info) {
-  BandwidthEstimationInfo bwe_info;
-  bwe_info.available_send_bandwidth = stats.send_bandwidth_bps;
-  bwe_info.available_recv_bandwidth = stats.recv_bandwidth_bps;
-  bwe_info.bucket_delay = stats.pacer_delay_ms;
-
-  // Get send stream bitrate stats.
-  rtc::CritScope stream_lock(&stream_crit_);
-  for (std::map<uint32_t, WebRtcVideoSendStream*>::iterator stream =
-           send_streams_.begin();
-       stream != send_streams_.end(); ++stream) {
-    stream->second->FillBandwidthEstimationInfo(&bwe_info);
-  }
-  video_media_info->bw_estimations.push_back(bwe_info);
-}
-
-void WebRtcVideoChannel2::FillSendAndReceiveCodecStats(
-    VideoMediaInfo* video_media_info) {
-  for (const VideoCodec& codec : send_params_.codecs) {
-    webrtc::RtpCodecParameters codec_params = codec.ToCodecParameters();
-    video_media_info->send_codecs.insert(
-        std::make_pair(codec_params.payload_type, std::move(codec_params)));
-  }
-  for (const VideoCodec& codec : recv_params_.codecs) {
-    webrtc::RtpCodecParameters codec_params = codec.ToCodecParameters();
-    video_media_info->receive_codecs.insert(
-        std::make_pair(codec_params.payload_type, std::move(codec_params)));
-  }
-}
-
-void WebRtcVideoChannel2::OnPacketReceived(
-    rtc::CopyOnWriteBuffer* packet,
-    const rtc::PacketTime& packet_time) {
-  const webrtc::PacketTime webrtc_packet_time(packet_time.timestamp,
-                                              packet_time.not_before);
-  const webrtc::PacketReceiver::DeliveryStatus delivery_result =
-      call_->Receiver()->DeliverPacket(
-          webrtc::MediaType::VIDEO,
-          packet->cdata(), packet->size(),
-          webrtc_packet_time);
-  switch (delivery_result) {
-    case webrtc::PacketReceiver::DELIVERY_OK:
-      return;
-    case webrtc::PacketReceiver::DELIVERY_PACKET_ERROR:
-      return;
-    case webrtc::PacketReceiver::DELIVERY_UNKNOWN_SSRC:
-      break;
-  }
-
-  uint32_t ssrc = 0;
-  if (!GetRtpSsrc(packet->cdata(), packet->size(), &ssrc)) {
-    return;
-  }
-
-  int payload_type = 0;
-  if (!GetRtpPayloadType(packet->cdata(), packet->size(), &payload_type)) {
-    return;
-  }
-
-  // See if this payload_type is registered as one that usually gets its own
-  // SSRC (RTX) or at least is safe to drop either way (FEC). If it is, and
-  // it wasn't handled above by DeliverPacket, that means we don't know what
-  // stream it associates with, and we shouldn't ever create an implicit channel
-  // for these.
-  for (auto& codec : recv_codecs_) {
-    if (payload_type == codec.rtx_payload_type ||
-        payload_type == codec.ulpfec.red_rtx_payload_type ||
-        payload_type == codec.ulpfec.ulpfec_payload_type ||
-        payload_type == codec.flexfec_payload_type) {
-      return;
-    }
-  }
-
-  switch (unsignalled_ssrc_handler_->OnUnsignalledSsrc(this, ssrc)) {
-    case UnsignalledSsrcHandler::kDropPacket:
-      return;
-    case UnsignalledSsrcHandler::kDeliverPacket:
-      break;
-  }
-
-  if (call_->Receiver()->DeliverPacket(
-          webrtc::MediaType::VIDEO,
-          packet->cdata(), packet->size(),
-          webrtc_packet_time) != webrtc::PacketReceiver::DELIVERY_OK) {
-    LOG(LS_WARNING) << "Failed to deliver RTP packet on re-delivery.";
-    return;
-  }
-}
-
-void WebRtcVideoChannel2::OnRtcpReceived(
-    rtc::CopyOnWriteBuffer* packet,
-    const rtc::PacketTime& packet_time) {
-  const webrtc::PacketTime webrtc_packet_time(packet_time.timestamp,
-                                              packet_time.not_before);
-  // TODO(pbos): Check webrtc::PacketReceiver::DELIVERY_OK once we deliver
-  // for both audio and video on the same path. Since BundleFilter doesn't
-  // filter RTCP anymore incoming RTCP packets could've been going to audio (so
-  // logging failures spam the log).
-  call_->Receiver()->DeliverPacket(
-      webrtc::MediaType::VIDEO,
-      packet->cdata(), packet->size(),
-      webrtc_packet_time);
-}
-
-void WebRtcVideoChannel2::OnReadyToSend(bool ready) {
-  LOG(LS_VERBOSE) << "OnReadyToSend: " << (ready ? "Ready." : "Not ready.");
-  call_->SignalChannelNetworkState(
-      webrtc::MediaType::VIDEO,
-      ready ? webrtc::kNetworkUp : webrtc::kNetworkDown);
-}
-
-void WebRtcVideoChannel2::OnNetworkRouteChanged(
-    const std::string& transport_name,
-    const rtc::NetworkRoute& network_route) {
-  call_->OnNetworkRouteChanged(transport_name, network_route);
-}
-
-void WebRtcVideoChannel2::OnTransportOverheadChanged(
-    int transport_overhead_per_packet) {
-  call_->OnTransportOverheadChanged(webrtc::MediaType::VIDEO,
-                                    transport_overhead_per_packet);
-}
-
-void WebRtcVideoChannel2::SetInterface(NetworkInterface* iface) {
-  MediaChannel::SetInterface(iface);
-  // Set the RTP recv/send buffer to a bigger size
-  MediaChannel::SetOption(NetworkInterface::ST_RTP,
-                          rtc::Socket::OPT_RCVBUF,
-                          kVideoRtpBufferSize);
-
-  // Speculative change to increase the outbound socket buffer size.
-  // In b/15152257, we are seeing a significant number of packets discarded
-  // due to lack of socket buffer space, although it's not yet clear what the
-  // ideal value should be.
-  MediaChannel::SetOption(NetworkInterface::ST_RTP,
-                          rtc::Socket::OPT_SNDBUF,
-                          kVideoRtpBufferSize);
-}
-
-bool WebRtcVideoChannel2::SendRtp(const uint8_t* data,
-                                  size_t len,
-                                  const webrtc::PacketOptions& options) {
-  rtc::CopyOnWriteBuffer packet(data, len, kMaxRtpPacketLen);
-  rtc::PacketOptions rtc_options;
-  rtc_options.packet_id = options.packet_id;
-  return MediaChannel::SendPacket(&packet, rtc_options);
-}
-
-bool WebRtcVideoChannel2::SendRtcp(const uint8_t* data, size_t len) {
-  rtc::CopyOnWriteBuffer packet(data, len, kMaxRtpPacketLen);
-  return MediaChannel::SendRtcp(&packet, rtc::PacketOptions());
-}
-
-WebRtcVideoChannel2::WebRtcVideoSendStream::VideoSendStreamParameters::
-    VideoSendStreamParameters(
-        webrtc::VideoSendStream::Config config,
-        const VideoOptions& options,
-        int max_bitrate_bps,
-        const rtc::Optional<VideoCodecSettings>& codec_settings)
-    : config(std::move(config)),
-      options(options),
-      max_bitrate_bps(max_bitrate_bps),
-      conference_mode(false),
-      codec_settings(codec_settings) {}
-
-WebRtcVideoChannel2::WebRtcVideoSendStream::AllocatedEncoder::AllocatedEncoder(
-    webrtc::VideoEncoder* encoder,
-    const cricket::VideoCodec& codec,
-    bool external)
-    : encoder(encoder),
-      external_encoder(nullptr),
-      codec(codec),
-      external(external) {
-  if (external) {
-    external_encoder = encoder;
-    this->encoder =
-        new webrtc::VideoEncoderSoftwareFallbackWrapper(codec, encoder);
-  }
-}
-
-WebRtcVideoChannel2::WebRtcVideoSendStream::WebRtcVideoSendStream(
-    webrtc::Call* call,
-    const StreamParams& sp,
-    webrtc::VideoSendStream::Config config,
-    const VideoOptions& options,
-    WebRtcVideoEncoderFactory* external_encoder_factory,
-    bool enable_cpu_overuse_detection,
-    int max_bitrate_bps,
-    const rtc::Optional<VideoCodecSettings>& codec_settings,
-    const rtc::Optional<std::vector<webrtc::RtpExtension>>& rtp_extensions,
-    // TODO(deadbeef): Don't duplicate information between send_params,
-    // rtp_extensions, options, etc.
-    const VideoSendParameters& send_params)
-    : worker_thread_(rtc::Thread::Current()),
-      ssrcs_(sp.ssrcs),
-      ssrc_groups_(sp.ssrc_groups),
-      call_(call),
-      enable_cpu_overuse_detection_(enable_cpu_overuse_detection),
-      source_(nullptr),
-      external_encoder_factory_(external_encoder_factory),
-      internal_encoder_factory_(new InternalEncoderFactory()),
-      stream_(nullptr),
-      encoder_sink_(nullptr),
-      parameters_(std::move(config), options, max_bitrate_bps, codec_settings),
-      rtp_parameters_(CreateRtpParametersWithOneEncoding()),
-      allocated_encoder_(nullptr, cricket::VideoCodec(), false),
-      sending_(false) {
-  parameters_.config.rtp.max_packet_size = kVideoMtu;
-  parameters_.conference_mode = send_params.conference_mode;
-
-  sp.GetPrimarySsrcs(&parameters_.config.rtp.ssrcs);
-
-  // ValidateStreamParams should prevent this from happening.
-  RTC_CHECK(!parameters_.config.rtp.ssrcs.empty());
-  rtp_parameters_.encodings[0].ssrc =
-      rtc::Optional<uint32_t>(parameters_.config.rtp.ssrcs[0]);
-
-  // RTX.
-  sp.GetFidSsrcs(parameters_.config.rtp.ssrcs,
-                 &parameters_.config.rtp.rtx.ssrcs);
-
-  // FlexFEC SSRCs.
-  // TODO(brandtr): This code needs to be generalized when we add support for
-  // multistream protection.
-  if (IsFlexfecFieldTrialEnabled()) {
-    uint32_t flexfec_ssrc;
-    bool flexfec_enabled = false;
-    for (uint32_t primary_ssrc : parameters_.config.rtp.ssrcs) {
-      if (sp.GetFecFrSsrc(primary_ssrc, &flexfec_ssrc)) {
-        if (flexfec_enabled) {
-          LOG(LS_INFO) << "Multiple FlexFEC streams proposed by remote, but "
-                          "our implementation only supports a single FlexFEC "
-                          "stream. Will not enable FlexFEC for proposed "
-                          "stream with SSRC: "
-                       << flexfec_ssrc << ".";
-          continue;
-        }
-
-        flexfec_enabled = true;
-        parameters_.config.rtp.flexfec.ssrc = flexfec_ssrc;
-        parameters_.config.rtp.flexfec.protected_media_ssrcs = {primary_ssrc};
-      }
-    }
-  }
-
-  parameters_.config.rtp.c_name = sp.cname;
-  if (rtp_extensions) {
-    parameters_.config.rtp.extensions = *rtp_extensions;
-  }
-  parameters_.config.rtp.rtcp_mode = send_params.rtcp.reduced_size
-                                         ? webrtc::RtcpMode::kReducedSize
-                                         : webrtc::RtcpMode::kCompound;
-  if (codec_settings) {
-    bool force_encoder_allocation = false;
-    SetCodec(*codec_settings, force_encoder_allocation);
-  }
-}
-
-WebRtcVideoChannel2::WebRtcVideoSendStream::~WebRtcVideoSendStream() {
-  if (stream_ != NULL) {
-    call_->DestroyVideoSendStream(stream_);
-  }
-  DestroyVideoEncoder(&allocated_encoder_);
-}
-
-bool WebRtcVideoChannel2::WebRtcVideoSendStream::SetVideoSend(
-    bool enable,
-    const VideoOptions* options,
-    rtc::VideoSourceInterface<webrtc::VideoFrame>* source) {
-  TRACE_EVENT0("webrtc", "WebRtcVideoSendStream::SetVideoSend");
-  RTC_DCHECK_RUN_ON(&thread_checker_);
-
-  // Ignore |options| pointer if |enable| is false.
-  bool options_present = enable && options;
-
-  if (options_present) {
-    VideoOptions old_options = parameters_.options;
-    parameters_.options.SetAll(*options);
-    if (parameters_.options.is_screencast.value_or(false) !=
-            old_options.is_screencast.value_or(false) &&
-        parameters_.codec_settings) {
-      // If screen content settings change, we may need to recreate the codec
-      // instance so that the correct type is used.
-
-      bool force_encoder_allocation = true;
-      SetCodec(*parameters_.codec_settings, force_encoder_allocation);
-      // Mark screenshare parameter as being updated, then test for any other
-      // changes that may require codec reconfiguration.
-      old_options.is_screencast = options->is_screencast;
-    }
-    if (parameters_.options != old_options) {
-      ReconfigureEncoder();
-    }
-  }
-
-  if (source_ && stream_) {
-    stream_->SetSource(
-        nullptr, webrtc::VideoSendStream::DegradationPreference::kBalanced);
-  }
-  // Switch to the new source.
-  source_ = source;
-  if (source && stream_) {
-    // Do not adapt resolution for screen content as this will likely
-    // result in blurry and unreadable text.
-    // |this| acts like a VideoSource to make sure SinkWants are handled on the
-    // correct thread.
-    stream_->SetSource(
-        this, enable_cpu_overuse_detection_ &&
-                      !parameters_.options.is_screencast.value_or(false)
-                  ? webrtc::VideoSendStream::DegradationPreference::kBalanced
-                  : webrtc::VideoSendStream::DegradationPreference::
-                        kMaintainResolution);
-  }
-  return true;
-}
-
-const std::vector<uint32_t>&
-WebRtcVideoChannel2::WebRtcVideoSendStream::GetSsrcs() const {
-  return ssrcs_;
-}
-
-WebRtcVideoChannel2::WebRtcVideoSendStream::AllocatedEncoder
-WebRtcVideoChannel2::WebRtcVideoSendStream::CreateVideoEncoder(
-    const VideoCodec& codec,
-    bool force_encoder_allocation) {
-  RTC_DCHECK_RUN_ON(&thread_checker_);
-  // Do not re-create encoders of the same type.
-  if (!force_encoder_allocation && codec == allocated_encoder_.codec &&
-      allocated_encoder_.encoder != nullptr) {
-    return allocated_encoder_;
-  }
-
-  // Try creating external encoder.
-  if (external_encoder_factory_ != nullptr &&
-      FindMatchingCodec(external_encoder_factory_->supported_codecs(), codec)) {
-    webrtc::VideoEncoder* encoder =
-        external_encoder_factory_->CreateVideoEncoder(codec);
-    if (encoder != nullptr)
-      return AllocatedEncoder(encoder, codec, true /* is_external */);
-  }
-
-  // Try creating internal encoder.
-  if (FindMatchingCodec(internal_encoder_factory_->supported_codecs(), codec)) {
-    if (parameters_.encoder_config.content_type ==
-            webrtc::VideoEncoderConfig::ContentType::kScreen &&
-        parameters_.conference_mode && UseSimulcastScreenshare()) {
-      // TODO(sprang): Remove this adapter once libvpx supports simulcast with
-      // same-resolution substreams.
-      WebRtcSimulcastEncoderFactory adapter_factory(
-          internal_encoder_factory_.get());
-      return AllocatedEncoder(adapter_factory.CreateVideoEncoder(codec), codec,
-                              false /* is_external */);
-    }
-    return AllocatedEncoder(
-        internal_encoder_factory_->CreateVideoEncoder(codec), codec,
-        false /* is_external */);
-  }
-
-  // This shouldn't happen, we should not be trying to create something we don't
-  // support.
-  RTC_NOTREACHED();
-  return AllocatedEncoder(NULL, cricket::VideoCodec(), false);
-}
-
-void WebRtcVideoChannel2::WebRtcVideoSendStream::DestroyVideoEncoder(
-    AllocatedEncoder* encoder) {
-  RTC_DCHECK_RUN_ON(&thread_checker_);
-  if (encoder->external) {
-    external_encoder_factory_->DestroyVideoEncoder(encoder->external_encoder);
-  }
-  delete encoder->encoder;
-}
-
-void WebRtcVideoChannel2::WebRtcVideoSendStream::SetCodec(
-    const VideoCodecSettings& codec_settings,
-    bool force_encoder_allocation) {
-  RTC_DCHECK_RUN_ON(&thread_checker_);
-  parameters_.encoder_config = CreateVideoEncoderConfig(codec_settings.codec);
-  RTC_DCHECK_GT(parameters_.encoder_config.number_of_streams, 0);
-
-  AllocatedEncoder new_encoder =
-      CreateVideoEncoder(codec_settings.codec, force_encoder_allocation);
-  parameters_.config.encoder_settings.encoder = new_encoder.encoder;
-  parameters_.config.encoder_settings.full_overuse_time = new_encoder.external;
-  parameters_.config.encoder_settings.payload_name = codec_settings.codec.name;
-  parameters_.config.encoder_settings.payload_type = codec_settings.codec.id;
-  if (new_encoder.external) {
-    webrtc::VideoCodecType type =
-        webrtc::PayloadNameToCodecType(codec_settings.codec.name)
-            .value_or(webrtc::kVideoCodecUnknown);
-    parameters_.config.encoder_settings.internal_source =
-        external_encoder_factory_->EncoderTypeHasInternalSource(type);
-  } else {
-    parameters_.config.encoder_settings.internal_source = false;
-  }
-  parameters_.config.rtp.ulpfec = codec_settings.ulpfec;
-  if (IsFlexfecFieldTrialEnabled()) {
-    parameters_.config.rtp.flexfec.payload_type =
-        codec_settings.flexfec_payload_type;
-  }
-
-  // Set RTX payload type if RTX is enabled.
-  if (!parameters_.config.rtp.rtx.ssrcs.empty()) {
-    if (codec_settings.rtx_payload_type == -1) {
-      LOG(LS_WARNING) << "RTX SSRCs configured but there's no configured RTX "
-                         "payload type. Ignoring.";
-      parameters_.config.rtp.rtx.ssrcs.clear();
-    } else {
-      parameters_.config.rtp.rtx.payload_type = codec_settings.rtx_payload_type;
-    }
-  }
-
-  parameters_.config.rtp.nack.rtp_history_ms =
-      HasNack(codec_settings.codec) ? kNackHistoryMs : 0;
-
-  parameters_.codec_settings =
-      rtc::Optional<WebRtcVideoChannel2::VideoCodecSettings>(codec_settings);
-
-  LOG(LS_INFO) << "RecreateWebRtcStream (send) because of SetCodec.";
-  RecreateWebRtcStream();
-  if (allocated_encoder_.encoder != new_encoder.encoder) {
-    DestroyVideoEncoder(&allocated_encoder_);
-    allocated_encoder_ = new_encoder;
-  }
-}
-
-void WebRtcVideoChannel2::WebRtcVideoSendStream::SetSendParameters(
-    const ChangedSendParameters& params) {
-  RTC_DCHECK_RUN_ON(&thread_checker_);
-  // |recreate_stream| means construction-time parameters have changed and the
-  // sending stream needs to be reset with the new config.
-  bool recreate_stream = false;
-  if (params.rtcp_mode) {
-    parameters_.config.rtp.rtcp_mode = *params.rtcp_mode;
-    recreate_stream = true;
-  }
-  if (params.rtp_header_extensions) {
-    parameters_.config.rtp.extensions = *params.rtp_header_extensions;
-    recreate_stream = true;
-  }
-  if (params.max_bandwidth_bps) {
-    parameters_.max_bitrate_bps = *params.max_bandwidth_bps;
-    ReconfigureEncoder();
-  }
-  if (params.conference_mode) {
-    parameters_.conference_mode = *params.conference_mode;
-  }
-
-  // Set codecs and options.
-  if (params.codec) {
-    bool force_encoder_allocation = false;
-    SetCodec(*params.codec, force_encoder_allocation);
-    recreate_stream = false;  // SetCodec has already recreated the stream.
-  } else if (params.conference_mode && parameters_.codec_settings) {
-    bool force_encoder_allocation = false;
-    SetCodec(*parameters_.codec_settings, force_encoder_allocation);
-    recreate_stream = false;  // SetCodec has already recreated the stream.
-  }
-  if (recreate_stream) {
-    LOG(LS_INFO) << "RecreateWebRtcStream (send) because of SetSendParameters";
-    RecreateWebRtcStream();
-  }
-}
-
-bool WebRtcVideoChannel2::WebRtcVideoSendStream::SetRtpParameters(
-    const webrtc::RtpParameters& new_parameters) {
-  RTC_DCHECK_RUN_ON(&thread_checker_);
-  if (!ValidateRtpParameters(new_parameters)) {
-    return false;
-  }
-
-  bool reconfigure_encoder = new_parameters.encodings[0].max_bitrate_bps !=
-                             rtp_parameters_.encodings[0].max_bitrate_bps;
-  rtp_parameters_ = new_parameters;
-  // Codecs are currently handled at the WebRtcVideoChannel2 level.
-  rtp_parameters_.codecs.clear();
-  if (reconfigure_encoder) {
-    ReconfigureEncoder();
-  }
-  // Encoding may have been activated/deactivated.
-  UpdateSendState();
-  return true;
-}
-
-webrtc::RtpParameters
-WebRtcVideoChannel2::WebRtcVideoSendStream::GetRtpParameters() const {
-  RTC_DCHECK_RUN_ON(&thread_checker_);
-  return rtp_parameters_;
-}
-
-bool WebRtcVideoChannel2::WebRtcVideoSendStream::ValidateRtpParameters(
-    const webrtc::RtpParameters& rtp_parameters) {
-  RTC_DCHECK_RUN_ON(&thread_checker_);
-  if (rtp_parameters.encodings.size() != 1) {
-    LOG(LS_ERROR)
-        << "Attempted to set RtpParameters without exactly one encoding";
-    return false;
-  }
-  if (rtp_parameters.encodings[0].ssrc != rtp_parameters_.encodings[0].ssrc) {
-    LOG(LS_ERROR) << "Attempted to set RtpParameters with modified SSRC";
-    return false;
-  }
-  return true;
-}
-
-void WebRtcVideoChannel2::WebRtcVideoSendStream::UpdateSendState() {
-  RTC_DCHECK_RUN_ON(&thread_checker_);
-  // TODO(deadbeef): Need to handle more than one encoding in the future.
-  RTC_DCHECK(rtp_parameters_.encodings.size() == 1u);
-  if (sending_ && rtp_parameters_.encodings[0].active) {
-    RTC_DCHECK(stream_ != nullptr);
-    stream_->Start();
-  } else {
-    if (stream_ != nullptr) {
-      stream_->Stop();
-    }
-  }
-}
-
-webrtc::VideoEncoderConfig
-WebRtcVideoChannel2::WebRtcVideoSendStream::CreateVideoEncoderConfig(
-    const VideoCodec& codec) const {
-  RTC_DCHECK_RUN_ON(&thread_checker_);
-  webrtc::VideoEncoderConfig encoder_config;
-  bool is_screencast = parameters_.options.is_screencast.value_or(false);
-  if (is_screencast) {
-    encoder_config.min_transmit_bitrate_bps =
-        1000 * parameters_.options.screencast_min_bitrate_kbps.value_or(0);
-    encoder_config.content_type =
-        webrtc::VideoEncoderConfig::ContentType::kScreen;
-  } else {
-    encoder_config.min_transmit_bitrate_bps = 0;
-    encoder_config.content_type =
-        webrtc::VideoEncoderConfig::ContentType::kRealtimeVideo;
-  }
-
-  // By default, the stream count for the codec configuration should match the
-  // number of negotiated ssrcs. But if the codec is blacklisted for simulcast
-  // or a screencast (and not in simulcast screenshare experiment), only
-  // configure a single stream.
-  encoder_config.number_of_streams = parameters_.config.rtp.ssrcs.size();
-  if (IsCodecBlacklistedForSimulcast(codec.name) ||
-      (is_screencast &&
-       (!UseSimulcastScreenshare() || !parameters_.conference_mode))) {
-    encoder_config.number_of_streams = 1;
-  }
-
-  int stream_max_bitrate = parameters_.max_bitrate_bps;
-  if (rtp_parameters_.encodings[0].max_bitrate_bps) {
-    stream_max_bitrate =
-        MinPositive(*(rtp_parameters_.encodings[0].max_bitrate_bps),
-                    parameters_.max_bitrate_bps);
-  }
-
-  int codec_max_bitrate_kbps;
-  if (codec.GetParam(kCodecParamMaxBitrate, &codec_max_bitrate_kbps)) {
-    stream_max_bitrate = codec_max_bitrate_kbps * 1000;
-  }
-  encoder_config.max_bitrate_bps = stream_max_bitrate;
-
-  int max_qp = kDefaultQpMax;
-  codec.GetParam(kCodecParamMaxQuantization, &max_qp);
-  encoder_config.video_stream_factory =
-      new rtc::RefCountedObject<EncoderStreamFactory>(
-          codec.name, max_qp, kDefaultVideoMaxFramerate, is_screencast,
-          parameters_.conference_mode);
-  return encoder_config;
-}
-
-void WebRtcVideoChannel2::WebRtcVideoSendStream::ReconfigureEncoder() {
-  RTC_DCHECK_RUN_ON(&thread_checker_);
-  if (!stream_) {
-    // The webrtc::VideoSendStream |stream_| has not yet been created but other
-    // parameters has changed.
-    return;
-  }
-
-  RTC_DCHECK_GT(parameters_.encoder_config.number_of_streams, 0);
-
-  RTC_CHECK(parameters_.codec_settings);
-  VideoCodecSettings codec_settings = *parameters_.codec_settings;
-
-  webrtc::VideoEncoderConfig encoder_config =
-      CreateVideoEncoderConfig(codec_settings.codec);
-
-  encoder_config.encoder_specific_settings = ConfigureVideoEncoderSettings(
-      codec_settings.codec);
-
-  stream_->ReconfigureVideoEncoder(encoder_config.Copy());
-
-  encoder_config.encoder_specific_settings = NULL;
-
-  parameters_.encoder_config = std::move(encoder_config);
-}
-
-void WebRtcVideoChannel2::WebRtcVideoSendStream::SetSend(bool send) {
-  RTC_DCHECK_RUN_ON(&thread_checker_);
-  sending_ = send;
-  UpdateSendState();
-}
-
-void WebRtcVideoChannel2::WebRtcVideoSendStream::RemoveSink(
-    rtc::VideoSinkInterface<webrtc::VideoFrame>* sink) {
-  RTC_DCHECK_RUN_ON(&thread_checker_);
-  RTC_DCHECK(encoder_sink_ == sink);
-  encoder_sink_ = nullptr;
-  source_->RemoveSink(sink);
-}
-
-void WebRtcVideoChannel2::WebRtcVideoSendStream::AddOrUpdateSink(
-    rtc::VideoSinkInterface<webrtc::VideoFrame>* sink,
-    const rtc::VideoSinkWants& wants) {
-  if (worker_thread_ == rtc::Thread::Current()) {
-    // AddOrUpdateSink is called on |worker_thread_| if this is the first
-    // registration of |sink|.
-    RTC_DCHECK_RUN_ON(&thread_checker_);
-    encoder_sink_ = sink;
-    source_->AddOrUpdateSink(encoder_sink_, wants);
-  } else {
-    // Subsequent calls to AddOrUpdateSink will happen on the encoder task
-    // queue.
-    invoker_.AsyncInvoke<void>(
-        RTC_FROM_HERE, worker_thread_, [this, sink, wants] {
-          RTC_DCHECK_RUN_ON(&thread_checker_);
-          // |sink| may be invalidated after this task was posted since
-          // RemoveSink is called on the worker thread.
-          bool encoder_sink_valid = (sink == encoder_sink_);
-          if (source_ && encoder_sink_valid) {
-            source_->AddOrUpdateSink(encoder_sink_, wants);
-          }
-        });
-  }
-}
-
-VideoSenderInfo WebRtcVideoChannel2::WebRtcVideoSendStream::GetVideoSenderInfo(
-    bool log_stats) {
-  VideoSenderInfo info;
-  RTC_DCHECK_RUN_ON(&thread_checker_);
-  for (uint32_t ssrc : parameters_.config.rtp.ssrcs)
-    info.add_ssrc(ssrc);
-
-  if (parameters_.codec_settings) {
-    info.codec_name = parameters_.codec_settings->codec.name;
-    info.codec_payload_type = rtc::Optional<int>(
-        parameters_.codec_settings->codec.id);
-  }
-
-  if (stream_ == NULL)
-    return info;
-
-  webrtc::VideoSendStream::Stats stats = stream_->GetStats();
-
-  if (log_stats)
-    LOG(LS_INFO) << stats.ToString(rtc::TimeMillis());
-
-  info.adapt_changes = stats.number_of_cpu_adapt_changes;
-  info.adapt_reason =
-      stats.cpu_limited_resolution ? ADAPTREASON_CPU : ADAPTREASON_NONE;
-
-  // Get bandwidth limitation info from stream_->GetStats().
-  // Input resolution (output from video_adapter) can be further scaled down or
-  // higher video layer(s) can be dropped due to bitrate constraints.
-  // Note, adapt_changes only include changes from the video_adapter.
-  if (stats.bw_limited_resolution)
-    info.adapt_reason |= ADAPTREASON_BANDWIDTH;
-
-  info.encoder_implementation_name = stats.encoder_implementation_name;
-  info.ssrc_groups = ssrc_groups_;
-  info.framerate_input = stats.input_frame_rate;
-  info.framerate_sent = stats.encode_frame_rate;
-  info.avg_encode_ms = stats.avg_encode_time_ms;
-  info.encode_usage_percent = stats.encode_usage_percent;
-  info.frames_encoded = stats.frames_encoded;
-  info.qp_sum = stats.qp_sum;
-
-  info.nominal_bitrate = stats.media_bitrate_bps;
-  info.preferred_bitrate = stats.preferred_media_bitrate_bps;
-
-  info.send_frame_width = 0;
-  info.send_frame_height = 0;
-  for (std::map<uint32_t, webrtc::VideoSendStream::StreamStats>::iterator it =
-           stats.substreams.begin();
-       it != stats.substreams.end(); ++it) {
-    // TODO(pbos): Wire up additional stats, such as padding bytes.
-    webrtc::VideoSendStream::StreamStats stream_stats = it->second;
-    info.bytes_sent += stream_stats.rtp_stats.transmitted.payload_bytes +
-                       stream_stats.rtp_stats.transmitted.header_bytes +
-                       stream_stats.rtp_stats.transmitted.padding_bytes;
-    info.packets_sent += stream_stats.rtp_stats.transmitted.packets;
-    info.packets_lost += stream_stats.rtcp_stats.cumulative_lost;
-    if (stream_stats.width > info.send_frame_width)
-      info.send_frame_width = stream_stats.width;
-    if (stream_stats.height > info.send_frame_height)
-      info.send_frame_height = stream_stats.height;
-    info.firs_rcvd += stream_stats.rtcp_packet_type_counts.fir_packets;
-    info.nacks_rcvd += stream_stats.rtcp_packet_type_counts.nack_packets;
-    info.plis_rcvd += stream_stats.rtcp_packet_type_counts.pli_packets;
-  }
-
-  if (!stats.substreams.empty()) {
-    // TODO(pbos): Report fraction lost per SSRC.
-    webrtc::VideoSendStream::StreamStats first_stream_stats =
-        stats.substreams.begin()->second;
-    info.fraction_lost =
-        static_cast<float>(first_stream_stats.rtcp_stats.fraction_lost) /
-        (1 << 8);
-  }
-
-  return info;
-}
-
-void WebRtcVideoChannel2::WebRtcVideoSendStream::FillBandwidthEstimationInfo(
-    BandwidthEstimationInfo* bwe_info) {
-  RTC_DCHECK_RUN_ON(&thread_checker_);
-  if (stream_ == NULL) {
-    return;
-  }
-  webrtc::VideoSendStream::Stats stats = stream_->GetStats();
-  for (std::map<uint32_t, webrtc::VideoSendStream::StreamStats>::iterator it =
-           stats.substreams.begin();
-       it != stats.substreams.end(); ++it) {
-    bwe_info->transmit_bitrate += it->second.total_bitrate_bps;
-    bwe_info->retransmit_bitrate += it->second.retransmit_bitrate_bps;
-  }
-  bwe_info->target_enc_bitrate += stats.target_media_bitrate_bps;
-  bwe_info->actual_enc_bitrate += stats.media_bitrate_bps;
-}
-
-void WebRtcVideoChannel2::WebRtcVideoSendStream::RecreateWebRtcStream() {
-  RTC_DCHECK_RUN_ON(&thread_checker_);
-  if (stream_ != NULL) {
-    call_->DestroyVideoSendStream(stream_);
-  }
-
-  RTC_CHECK(parameters_.codec_settings);
-  RTC_DCHECK_EQ((parameters_.encoder_config.content_type ==
-                 webrtc::VideoEncoderConfig::ContentType::kScreen),
-                parameters_.options.is_screencast.value_or(false))
-      << "encoder content type inconsistent with screencast option";
-  parameters_.encoder_config.encoder_specific_settings =
-      ConfigureVideoEncoderSettings(parameters_.codec_settings->codec);
-
-  webrtc::VideoSendStream::Config config = parameters_.config.Copy();
-  if (!config.rtp.rtx.ssrcs.empty() && config.rtp.rtx.payload_type == -1) {
-    LOG(LS_WARNING) << "RTX SSRCs configured but there's no configured RTX "
-                       "payload type the set codec. Ignoring RTX.";
-    config.rtp.rtx.ssrcs.clear();
-  }
-  stream_ = call_->CreateVideoSendStream(std::move(config),
-                                         parameters_.encoder_config.Copy());
-
-  parameters_.encoder_config.encoder_specific_settings = NULL;
-
-  if (source_) {
-    // Do not adapt resolution for screen content as this will likely result in
-    // blurry and unreadable text.
-    // |this| acts like a VideoSource to make sure SinkWants are handled on the
-    // correct thread.
-    stream_->SetSource(
-        this, enable_cpu_overuse_detection_ &&
-                      !parameters_.options.is_screencast.value_or(false)
-                  ? webrtc::VideoSendStream::DegradationPreference::kBalanced
-                  : webrtc::VideoSendStream::DegradationPreference::
-                        kMaintainResolution);
-  }
-
-  // Call stream_->Start() if necessary conditions are met.
-  UpdateSendState();
-}
-
-WebRtcVideoChannel2::WebRtcVideoReceiveStream::WebRtcVideoReceiveStream(
-    webrtc::Call* call,
-    const StreamParams& sp,
-    webrtc::VideoReceiveStream::Config config,
-    WebRtcVideoDecoderFactory* external_decoder_factory,
-    bool default_stream,
-    const std::vector<VideoCodecSettings>& recv_codecs,
-    const webrtc::FlexfecReceiveStream::Config& flexfec_config)
-    : call_(call),
-      stream_params_(sp),
-      stream_(NULL),
-      default_stream_(default_stream),
-      config_(std::move(config)),
-      flexfec_config_(flexfec_config),
-      flexfec_stream_(nullptr),
-      external_decoder_factory_(external_decoder_factory),
-      sink_(NULL),
-      first_frame_timestamp_(-1),
-      estimated_remote_start_ntp_time_ms_(0) {
-  config_.renderer = this;
-  std::vector<AllocatedDecoder> old_decoders;
-  ConfigureCodecs(recv_codecs, &old_decoders);
-  RecreateWebRtcStream();
-  RTC_DCHECK(old_decoders.empty());
-}
-
-WebRtcVideoChannel2::WebRtcVideoReceiveStream::AllocatedDecoder::
-    AllocatedDecoder(webrtc::VideoDecoder* decoder,
-                     webrtc::VideoCodecType type,
-                     bool external)
-    : decoder(decoder),
-      external_decoder(nullptr),
-      type(type),
-      external(external) {
-  if (external) {
-    external_decoder = decoder;
-    this->decoder =
-        new webrtc::VideoDecoderSoftwareFallbackWrapper(type, external_decoder);
-  }
-}
-
-WebRtcVideoChannel2::WebRtcVideoReceiveStream::~WebRtcVideoReceiveStream() {
-  if (flexfec_stream_) {
-    call_->DestroyFlexfecReceiveStream(flexfec_stream_);
-  }
-  call_->DestroyVideoReceiveStream(stream_);
-  ClearDecoders(&allocated_decoders_);
-}
-
-const std::vector<uint32_t>&
-WebRtcVideoChannel2::WebRtcVideoReceiveStream::GetSsrcs() const {
-  return stream_params_.ssrcs;
-}
-
-rtc::Optional<uint32_t>
-WebRtcVideoChannel2::WebRtcVideoReceiveStream::GetFirstPrimarySsrc() const {
-  std::vector<uint32_t> primary_ssrcs;
-  stream_params_.GetPrimarySsrcs(&primary_ssrcs);
-
-  if (primary_ssrcs.empty()) {
-    LOG(LS_WARNING) << "Empty primary ssrcs vector, returning empty optional";
-    return rtc::Optional<uint32_t>();
-  } else {
-    return rtc::Optional<uint32_t>(primary_ssrcs[0]);
-  }
-}
-
-WebRtcVideoChannel2::WebRtcVideoReceiveStream::AllocatedDecoder
-WebRtcVideoChannel2::WebRtcVideoReceiveStream::CreateOrReuseVideoDecoder(
-    std::vector<AllocatedDecoder>* old_decoders,
-    const VideoCodec& codec) {
-  webrtc::VideoCodecType type = webrtc::PayloadNameToCodecType(codec.name)
-                                    .value_or(webrtc::kVideoCodecUnknown);
-
-  for (size_t i = 0; i < old_decoders->size(); ++i) {
-    if ((*old_decoders)[i].type == type) {
-      AllocatedDecoder decoder = (*old_decoders)[i];
-      (*old_decoders)[i] = old_decoders->back();
-      old_decoders->pop_back();
-      return decoder;
-    }
-  }
-
-  if (external_decoder_factory_ != NULL) {
-    webrtc::VideoDecoder* decoder =
-        external_decoder_factory_->CreateVideoDecoderWithParams(
-            type, {stream_params_.id});
-    if (decoder != NULL) {
-      return AllocatedDecoder(decoder, type, true /* is_external */);
-    }
-  }
-
-  InternalDecoderFactory internal_decoder_factory;
-  return AllocatedDecoder(internal_decoder_factory.CreateVideoDecoderWithParams(
-                              type, {stream_params_.id}),
-                          type, false /* is_external */);
-}
-
-void WebRtcVideoChannel2::WebRtcVideoReceiveStream::ConfigureCodecs(
-    const std::vector<VideoCodecSettings>& recv_codecs,
-    std::vector<AllocatedDecoder>* old_decoders) {
-  *old_decoders = allocated_decoders_;
-  allocated_decoders_.clear();
-  config_.decoders.clear();
-  for (size_t i = 0; i < recv_codecs.size(); ++i) {
-    AllocatedDecoder allocated_decoder =
-        CreateOrReuseVideoDecoder(old_decoders, recv_codecs[i].codec);
-    allocated_decoders_.push_back(allocated_decoder);
-
-    webrtc::VideoReceiveStream::Decoder decoder;
-    decoder.decoder = allocated_decoder.decoder;
-    decoder.payload_type = recv_codecs[i].codec.id;
-    decoder.payload_name = recv_codecs[i].codec.name;
-    decoder.codec_params = recv_codecs[i].codec.params;
-    config_.decoders.push_back(decoder);
-  }
-
-  config_.rtp.rtx_payload_types.clear();
-  for (const VideoCodecSettings& recv_codec : recv_codecs) {
-    config_.rtp.rtx_payload_types[recv_codec.codec.id] =
-        recv_codec.rtx_payload_type;
-  }
-
-  config_.rtp.ulpfec = recv_codecs.front().ulpfec;
-  flexfec_config_.payload_type = recv_codecs.front().flexfec_payload_type;
-
-  config_.rtp.nack.rtp_history_ms =
-      HasNack(recv_codecs.begin()->codec) ? kNackHistoryMs : 0;
-}
-
-void WebRtcVideoChannel2::WebRtcVideoReceiveStream::SetLocalSsrc(
-    uint32_t local_ssrc) {
-  // TODO(pbos): Consider turning this sanity check into a RTC_DCHECK. You
-  // should not be able to create a sender with the same SSRC as a receiver, but
-  // right now this can't be done due to unittests depending on receiving what
-  // they are sending from the same MediaChannel.
-  if (local_ssrc == config_.rtp.remote_ssrc) {
-    LOG(LS_INFO) << "Ignoring call to SetLocalSsrc because parameters are "
-                    "unchanged; local_ssrc=" << local_ssrc;
-    return;
-  }
-
-  config_.rtp.local_ssrc = local_ssrc;
-  flexfec_config_.local_ssrc = local_ssrc;
-  LOG(LS_INFO)
-      << "RecreateWebRtcStream (recv) because of SetLocalSsrc; local_ssrc="
-      << local_ssrc;
-  RecreateWebRtcStream();
-}
-
-void WebRtcVideoChannel2::WebRtcVideoReceiveStream::SetFeedbackParameters(
-    bool nack_enabled,
-    bool remb_enabled,
-    bool transport_cc_enabled,
-    webrtc::RtcpMode rtcp_mode) {
-  int nack_history_ms = nack_enabled ? kNackHistoryMs : 0;
-  if (config_.rtp.nack.rtp_history_ms == nack_history_ms &&
-      config_.rtp.remb == remb_enabled &&
-      config_.rtp.transport_cc == transport_cc_enabled &&
-      config_.rtp.rtcp_mode == rtcp_mode) {
-    LOG(LS_INFO)
-        << "Ignoring call to SetFeedbackParameters because parameters are "
-           "unchanged; nack="
-        << nack_enabled << ", remb=" << remb_enabled
-        << ", transport_cc=" << transport_cc_enabled;
-    return;
-  }
-  config_.rtp.remb = remb_enabled;
-  config_.rtp.nack.rtp_history_ms = nack_history_ms;
-  config_.rtp.transport_cc = transport_cc_enabled;
-  config_.rtp.rtcp_mode = rtcp_mode;
-  // TODO(brandtr): We should be spec-compliant and set |transport_cc| here
-  // based on the rtcp-fb for the FlexFEC codec, not the media codec.
-  flexfec_config_.transport_cc = config_.rtp.transport_cc;
-  flexfec_config_.rtcp_mode = config_.rtp.rtcp_mode;
-  LOG(LS_INFO)
-      << "RecreateWebRtcStream (recv) because of SetFeedbackParameters; nack="
-      << nack_enabled << ", remb=" << remb_enabled
-      << ", transport_cc=" << transport_cc_enabled;
-  RecreateWebRtcStream();
-}
-
-void WebRtcVideoChannel2::WebRtcVideoReceiveStream::SetRecvParameters(
-    const ChangedRecvParameters& params) {
-  bool needs_recreation = false;
-  std::vector<AllocatedDecoder> old_decoders;
-  if (params.codec_settings) {
-    ConfigureCodecs(*params.codec_settings, &old_decoders);
-    needs_recreation = true;
-  }
-  if (params.rtp_header_extensions) {
-    config_.rtp.extensions = *params.rtp_header_extensions;
-    flexfec_config_.rtp_header_extensions = *params.rtp_header_extensions;
-    needs_recreation = true;
-  }
-  if (needs_recreation) {
-    LOG(LS_INFO) << "RecreateWebRtcStream (recv) because of SetRecvParameters";
-    RecreateWebRtcStream();
-    ClearDecoders(&old_decoders);
-  }
-}
-
-void WebRtcVideoChannel2::WebRtcVideoReceiveStream::RecreateWebRtcStream() {
-  if (stream_) {
-    call_->DestroyVideoReceiveStream(stream_);
-    stream_ = nullptr;
-  }
-  if (flexfec_stream_) {
-    call_->DestroyFlexfecReceiveStream(flexfec_stream_);
-    flexfec_stream_ = nullptr;
-  }
-  if (flexfec_config_.IsCompleteAndEnabled()) {
-    flexfec_stream_ = call_->CreateFlexfecReceiveStream(flexfec_config_);
-    flexfec_stream_->Start();
-  }
-  stream_ = call_->CreateVideoReceiveStream(config_.Copy());
-  stream_->Start();
-}
-
-void WebRtcVideoChannel2::WebRtcVideoReceiveStream::ClearDecoders(
-    std::vector<AllocatedDecoder>* allocated_decoders) {
-  for (size_t i = 0; i < allocated_decoders->size(); ++i) {
-    if ((*allocated_decoders)[i].external) {
-      external_decoder_factory_->DestroyVideoDecoder(
-          (*allocated_decoders)[i].external_decoder);
-    }
-    delete (*allocated_decoders)[i].decoder;
-  }
-  allocated_decoders->clear();
-}
-
-void WebRtcVideoChannel2::WebRtcVideoReceiveStream::OnFrame(
-    const webrtc::VideoFrame& frame) {
-  rtc::CritScope crit(&sink_lock_);
-
-  if (first_frame_timestamp_ < 0)
-    first_frame_timestamp_ = frame.timestamp();
-  int64_t rtp_time_elapsed_since_first_frame =
-      (timestamp_wraparound_handler_.Unwrap(frame.timestamp()) -
-       first_frame_timestamp_);
-  int64_t elapsed_time_ms = rtp_time_elapsed_since_first_frame /
-                            (cricket::kVideoCodecClockrate / 1000);
-  if (frame.ntp_time_ms() > 0)
-    estimated_remote_start_ntp_time_ms_ = frame.ntp_time_ms() - elapsed_time_ms;
-
-  if (sink_ == NULL) {
-    LOG(LS_WARNING) << "VideoReceiveStream not connected to a VideoSink.";
-    return;
-  }
-
-  sink_->OnFrame(frame);
-}
-
-bool WebRtcVideoChannel2::WebRtcVideoReceiveStream::IsDefaultStream() const {
-  return default_stream_;
-}
-
-void WebRtcVideoChannel2::WebRtcVideoReceiveStream::SetSink(
-    rtc::VideoSinkInterface<webrtc::VideoFrame>* sink) {
-  rtc::CritScope crit(&sink_lock_);
-  sink_ = sink;
-}
-
-std::string
-WebRtcVideoChannel2::WebRtcVideoReceiveStream::GetCodecNameFromPayloadType(
-    int payload_type) {
-  for (const webrtc::VideoReceiveStream::Decoder& decoder : config_.decoders) {
-    if (decoder.payload_type == payload_type) {
-      return decoder.payload_name;
-    }
-  }
-  return "";
-}
-
-VideoReceiverInfo
-WebRtcVideoChannel2::WebRtcVideoReceiveStream::GetVideoReceiverInfo(
-    bool log_stats) {
-  VideoReceiverInfo info;
-  info.ssrc_groups = stream_params_.ssrc_groups;
-  info.add_ssrc(config_.rtp.remote_ssrc);
-  webrtc::VideoReceiveStream::Stats stats = stream_->GetStats();
-  info.decoder_implementation_name = stats.decoder_implementation_name;
-  if (stats.current_payload_type != -1) {
-    info.codec_payload_type = rtc::Optional<int>(
-        stats.current_payload_type);
-  }
-  info.bytes_rcvd = stats.rtp_stats.transmitted.payload_bytes +
-                    stats.rtp_stats.transmitted.header_bytes +
-                    stats.rtp_stats.transmitted.padding_bytes;
-  info.packets_rcvd = stats.rtp_stats.transmitted.packets;
-  info.packets_lost = stats.rtcp_stats.cumulative_lost;
-  info.fraction_lost =
-      static_cast<float>(stats.rtcp_stats.fraction_lost) / (1 << 8);
-
-  info.framerate_rcvd = stats.network_frame_rate;
-  info.framerate_decoded = stats.decode_frame_rate;
-  info.framerate_output = stats.render_frame_rate;
-  info.frame_width = stats.width;
-  info.frame_height = stats.height;
-
-  {
-    rtc::CritScope frame_cs(&sink_lock_);
-    info.capture_start_ntp_time_ms = estimated_remote_start_ntp_time_ms_;
-  }
-
-  info.decode_ms = stats.decode_ms;
-  info.max_decode_ms = stats.max_decode_ms;
-  info.current_delay_ms = stats.current_delay_ms;
-  info.target_delay_ms = stats.target_delay_ms;
-  info.jitter_buffer_ms = stats.jitter_buffer_ms;
-  info.min_playout_delay_ms = stats.min_playout_delay_ms;
-  info.render_delay_ms = stats.render_delay_ms;
-  info.frames_received = stats.frame_counts.key_frames +
-                         stats.frame_counts.delta_frames;
-  info.frames_decoded = stats.frames_decoded;
-  info.frames_rendered = stats.frames_rendered;
-  info.qp_sum = stats.qp_sum;
-
-  info.codec_name = GetCodecNameFromPayloadType(stats.current_payload_type);
-
-  info.firs_sent = stats.rtcp_packet_type_counts.fir_packets;
-  info.plis_sent = stats.rtcp_packet_type_counts.pli_packets;
-  info.nacks_sent = stats.rtcp_packet_type_counts.nack_packets;
-
-  if (log_stats)
-    LOG(LS_INFO) << stats.ToString(rtc::TimeMillis());
-
-  return info;
-}
-
-WebRtcVideoChannel2::VideoCodecSettings::VideoCodecSettings()
-    : flexfec_payload_type(-1), rtx_payload_type(-1) {}
-
-bool WebRtcVideoChannel2::VideoCodecSettings::operator==(
-    const WebRtcVideoChannel2::VideoCodecSettings& other) const {
-  return codec == other.codec && ulpfec == other.ulpfec &&
-         flexfec_payload_type == other.flexfec_payload_type &&
-         rtx_payload_type == other.rtx_payload_type;
-}
-
-bool WebRtcVideoChannel2::VideoCodecSettings::operator!=(
-    const WebRtcVideoChannel2::VideoCodecSettings& other) const {
-  return !(*this == other);
-}
-
-std::vector<WebRtcVideoChannel2::VideoCodecSettings>
-WebRtcVideoChannel2::MapCodecs(const std::vector<VideoCodec>& codecs) {
-  RTC_DCHECK(!codecs.empty());
-
-  std::vector<VideoCodecSettings> video_codecs;
-  std::map<int, bool> payload_used;
-  std::map<int, VideoCodec::CodecType> payload_codec_type;
-  // |rtx_mapping| maps video payload type to rtx payload type.
-  std::map<int, int> rtx_mapping;
-
-  webrtc::UlpfecConfig ulpfec_config;
-  int flexfec_payload_type = -1;
-
-  for (size_t i = 0; i < codecs.size(); ++i) {
-    const VideoCodec& in_codec = codecs[i];
-    int payload_type = in_codec.id;
-
-    if (payload_used[payload_type]) {
-      LOG(LS_ERROR) << "Payload type already registered: "
-                    << in_codec.ToString();
-      return std::vector<VideoCodecSettings>();
-    }
-    payload_used[payload_type] = true;
-    payload_codec_type[payload_type] = in_codec.GetCodecType();
-
-    switch (in_codec.GetCodecType()) {
-      case VideoCodec::CODEC_RED: {
-        // RED payload type, should not have duplicates.
-        RTC_DCHECK_EQ(-1, ulpfec_config.red_payload_type);
-        ulpfec_config.red_payload_type = in_codec.id;
-        continue;
-      }
-
-      case VideoCodec::CODEC_ULPFEC: {
-        // ULPFEC payload type, should not have duplicates.
-        RTC_DCHECK_EQ(-1, ulpfec_config.ulpfec_payload_type);
-        ulpfec_config.ulpfec_payload_type = in_codec.id;
-        continue;
-      }
-
-      case VideoCodec::CODEC_FLEXFEC: {
-        // FlexFEC payload type, should not have duplicates.
-        RTC_DCHECK_EQ(-1, flexfec_payload_type);
-        flexfec_payload_type = in_codec.id;
-        continue;
-      }
-
-      case VideoCodec::CODEC_RTX: {
-        int associated_payload_type;
-        if (!in_codec.GetParam(kCodecParamAssociatedPayloadType,
-                               &associated_payload_type) ||
-            !IsValidRtpPayloadType(associated_payload_type)) {
-          LOG(LS_ERROR)
-              << "RTX codec with invalid or no associated payload type: "
-              << in_codec.ToString();
-          return std::vector<VideoCodecSettings>();
-        }
-        rtx_mapping[associated_payload_type] = in_codec.id;
-        continue;
-      }
-
-      case VideoCodec::CODEC_VIDEO:
-        break;
-    }
-
-    video_codecs.push_back(VideoCodecSettings());
-    video_codecs.back().codec = in_codec;
-  }
-
-  // One of these codecs should have been a video codec. Only having FEC
-  // parameters into this code is a logic error.
-  RTC_DCHECK(!video_codecs.empty());
-
-  for (std::map<int, int>::const_iterator it = rtx_mapping.begin();
-       it != rtx_mapping.end();
-       ++it) {
-    if (!payload_used[it->first]) {
-      LOG(LS_ERROR) << "RTX mapped to payload not in codec list.";
-      return std::vector<VideoCodecSettings>();
-    }
-    if (payload_codec_type[it->first] != VideoCodec::CODEC_VIDEO &&
-        payload_codec_type[it->first] != VideoCodec::CODEC_RED) {
-      LOG(LS_ERROR) << "RTX not mapped to regular video codec or RED codec.";
-      return std::vector<VideoCodecSettings>();
-    }
-
-    if (it->first == ulpfec_config.red_payload_type) {
-      ulpfec_config.red_rtx_payload_type = it->second;
-    }
-  }
-
-  for (size_t i = 0; i < video_codecs.size(); ++i) {
-    video_codecs[i].ulpfec = ulpfec_config;
-    video_codecs[i].flexfec_payload_type = flexfec_payload_type;
-    if (rtx_mapping[video_codecs[i].codec.id] != 0 &&
-        rtx_mapping[video_codecs[i].codec.id] !=
-            ulpfec_config.red_payload_type) {
-      video_codecs[i].rtx_payload_type = rtx_mapping[video_codecs[i].codec.id];
-    }
-  }
-
-  return video_codecs;
-}
-
-}  // namespace cricket
+/*
+ *  Copyright (c) 2014 The WebRTC project authors. All Rights Reserved.
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ */
+
+#include "webrtc/media/engine/webrtcvideoengine2.h"
+
+#include <stdio.h>
+#include <algorithm>
+#include <set>
+#include <string>
+#include <utility>
+
+#include "webrtc/api/video/i420_buffer.h"
+#include "webrtc/base/copyonwritebuffer.h"
+#include "webrtc/base/logging.h"
+#include "webrtc/base/stringutils.h"
+#include "webrtc/base/timeutils.h"
+#include "webrtc/base/trace_event.h"
+#include "webrtc/call/call.h"
+#include "webrtc/common_video/h264/profile_level_id.h"
+#include "webrtc/media/engine/constants.h"
+#include "webrtc/media/engine/internalencoderfactory.h"
+#include "webrtc/media/engine/internaldecoderfactory.h"
+#include "webrtc/media/engine/simulcast.h"
+#include "webrtc/media/engine/videoencodersoftwarefallbackwrapper.h"
+#include "webrtc/media/engine/videodecodersoftwarefallbackwrapper.h"
+#include "webrtc/media/engine/webrtcmediaengine.h"
+#include "webrtc/media/engine/webrtcvideoencoderfactory.h"
+#include "webrtc/media/engine/webrtcvoiceengine.h"
+#include "webrtc/modules/video_coding/codecs/vp8/simulcast_encoder_adapter.h"
+#include "webrtc/system_wrappers/include/field_trial.h"
+#include "webrtc/video_decoder.h"
+#include "webrtc/video_encoder.h"
+
+namespace cricket {
+namespace {
+
+// If this field trial is enabled, we will enable sending FlexFEC and disable
+// sending ULPFEC whenever the former has been negotiated. Receiving FlexFEC
+// is enabled whenever FlexFEC has been negotiated.
+bool IsFlexfecFieldTrialEnabled() {
+  return webrtc::field_trial::FindFullName("WebRTC-FlexFEC-03") == "Enabled";
+}
+
+// Wrap cricket::WebRtcVideoEncoderFactory as a webrtc::VideoEncoderFactory.
+class EncoderFactoryAdapter : public webrtc::VideoEncoderFactory {
+ public:
+  // EncoderFactoryAdapter doesn't take ownership of |factory|, which is owned
+  // by e.g. PeerConnectionFactory.
+  explicit EncoderFactoryAdapter(cricket::WebRtcVideoEncoderFactory* factory)
+      : factory_(factory) {}
+  virtual ~EncoderFactoryAdapter() {}
+
+  // Implement webrtc::VideoEncoderFactory.
+  webrtc::VideoEncoder* Create() override {
+    return factory_->CreateVideoEncoder(VideoCodec(kVp8CodecName));
+  }
+
+  void Destroy(webrtc::VideoEncoder* encoder) override {
+    return factory_->DestroyVideoEncoder(encoder);
+  }
+
+ private:
+  cricket::WebRtcVideoEncoderFactory* const factory_;
+};
+
+// An encoder factory that wraps Create requests for simulcastable codec types
+// with a webrtc::SimulcastEncoderAdapter. Non simulcastable codec type
+// requests are just passed through to the contained encoder factory.
+class WebRtcSimulcastEncoderFactory
+    : public cricket::WebRtcVideoEncoderFactory {
+ public:
+  // WebRtcSimulcastEncoderFactory doesn't take ownership of |factory|, which is
+  // owned by e.g. PeerConnectionFactory.
+  explicit WebRtcSimulcastEncoderFactory(
+      cricket::WebRtcVideoEncoderFactory* factory)
+      : factory_(factory) {}
+
+  static bool UseSimulcastEncoderFactory(
+      const std::vector<cricket::VideoCodec>& codecs) {
+    // If any codec is VP8, use the simulcast factory. If asked to create a
+    // non-VP8 codec, we'll just return a contained factory encoder directly.
+    for (const auto& codec : codecs) {
+      if (CodecNamesEq(codec.name.c_str(), kVp8CodecName)) {
+        return true;
+      }
+    }
+    return false;
+  }
+
+  webrtc::VideoEncoder* CreateVideoEncoder(
+      const cricket::VideoCodec& codec) override {
+    RTC_DCHECK(factory_ != NULL);
+    // If it's a codec type we can simulcast, create a wrapped encoder.
+    if (CodecNamesEq(codec.name.c_str(), kVp8CodecName)) {
+      return new webrtc::SimulcastEncoderAdapter(
+          new EncoderFactoryAdapter(factory_));
+    }
+    webrtc::VideoEncoder* encoder = factory_->CreateVideoEncoder(codec);
+    if (encoder) {
+      non_simulcast_encoders_.push_back(encoder);
+    }
+    return encoder;
+  }
+
+  const std::vector<cricket::VideoCodec>& supported_codecs() const override {
+    return factory_->supported_codecs();
+  }
+
+  bool EncoderTypeHasInternalSource(
+      webrtc::VideoCodecType type) const override {
+    return factory_->EncoderTypeHasInternalSource(type);
+  }
+
+  void DestroyVideoEncoder(webrtc::VideoEncoder* encoder) override {
+    // Check first to see if the encoder wasn't wrapped in a
+    // SimulcastEncoderAdapter. In that case, ask the factory to destroy it.
+    if (std::remove(non_simulcast_encoders_.begin(),
+                    non_simulcast_encoders_.end(),
+                    encoder) != non_simulcast_encoders_.end()) {
+      factory_->DestroyVideoEncoder(encoder);
+      return;
+    }
+
+    // Otherwise, SimulcastEncoderAdapter can be deleted directly, and will call
+    // DestroyVideoEncoder on the factory for individual encoder instances.
+    delete encoder;
+  }
+
+ private:
+  // Disable overloaded virtual function warning. TODO(magjed): Remove once
+  // http://crbug/webrtc/6402 is fixed.
+  using cricket::WebRtcVideoEncoderFactory::CreateVideoEncoder;
+
+  cricket::WebRtcVideoEncoderFactory* factory_;
+  // A list of encoders that were created without being wrapped in a
+  // SimulcastEncoderAdapter.
+  std::vector<webrtc::VideoEncoder*> non_simulcast_encoders_;
+};
+
+void AddDefaultFeedbackParams(VideoCodec* codec) {
+  codec->AddFeedbackParam(FeedbackParam(kRtcpFbParamCcm, kRtcpFbCcmParamFir));
+  codec->AddFeedbackParam(FeedbackParam(kRtcpFbParamNack, kParamValueEmpty));
+  codec->AddFeedbackParam(FeedbackParam(kRtcpFbParamNack, kRtcpFbNackParamPli));
+  codec->AddFeedbackParam(FeedbackParam(kRtcpFbParamRemb, kParamValueEmpty));
+  codec->AddFeedbackParam(
+      FeedbackParam(kRtcpFbParamTransportCc, kParamValueEmpty));
+}
+
+static std::string CodecVectorToString(const std::vector<VideoCodec>& codecs) {
+  std::stringstream out;
+  out << '{';
+  for (size_t i = 0; i < codecs.size(); ++i) {
+    out << codecs[i].ToString();
+    if (i != codecs.size() - 1) {
+      out << ", ";
+    }
+  }
+  out << '}';
+  return out.str();
+}
+
+static bool ValidateCodecFormats(const std::vector<VideoCodec>& codecs) {
+  bool has_video = false;
+  for (size_t i = 0; i < codecs.size(); ++i) {
+    if (!codecs[i].ValidateCodecFormat()) {
+      return false;
+    }
+    if (codecs[i].GetCodecType() == VideoCodec::CODEC_VIDEO) {
+      has_video = true;
+    }
+  }
+  if (!has_video) {
+    LOG(LS_ERROR) << "Setting codecs without a video codec is invalid: "
+                  << CodecVectorToString(codecs);
+    return false;
+  }
+  return true;
+}
+
+static bool ValidateStreamParams(const StreamParams& sp) {
+  if (sp.ssrcs.empty()) {
+    LOG(LS_ERROR) << "No SSRCs in stream parameters: " << sp.ToString();
+    return false;
+  }
+
+  std::vector<uint32_t> primary_ssrcs;
+  sp.GetPrimarySsrcs(&primary_ssrcs);
+  std::vector<uint32_t> rtx_ssrcs;
+  sp.GetFidSsrcs(primary_ssrcs, &rtx_ssrcs);
+  for (uint32_t rtx_ssrc : rtx_ssrcs) {
+    bool rtx_ssrc_present = false;
+    for (uint32_t sp_ssrc : sp.ssrcs) {
+      if (sp_ssrc == rtx_ssrc) {
+        rtx_ssrc_present = true;
+        break;
+      }
+    }
+    if (!rtx_ssrc_present) {
+      LOG(LS_ERROR) << "RTX SSRC '" << rtx_ssrc
+                    << "' missing from StreamParams ssrcs: " << sp.ToString();
+      return false;
+    }
+  }
+  if (!rtx_ssrcs.empty() && primary_ssrcs.size() != rtx_ssrcs.size()) {
+    LOG(LS_ERROR)
+        << "RTX SSRCs exist, but don't cover all SSRCs (unsupported): "
+        << sp.ToString();
+    return false;
+  }
+
+  return true;
+}
+
+// Returns true if the given codec is disallowed from doing simulcast.
+bool IsCodecBlacklistedForSimulcast(const std::string& codec_name) {
+  return CodecNamesEq(codec_name, kH264CodecName) ||
+         CodecNamesEq(codec_name, kVp9CodecName);
+}
+
+// The selected thresholds for QVGA and VGA corresponded to a QP around 10.
+// The change in QP declined above the selected bitrates.
+static int GetMaxDefaultVideoBitrateKbps(int width, int height) {
+  if (width * height <= 320 * 240) {
+    return 600;
+  } else if (width * height <= 640 * 480) {
+    return 1700;
+  } else if (width * height <= 960 * 540) {
+    return 2000;
+  } else {
+    return 2500;
+  }
+}
+
+bool GetVp9LayersFromFieldTrialGroup(int* num_spatial_layers,
+                                     int* num_temporal_layers) {
+  std::string group = webrtc::field_trial::FindFullName("WebRTC-SupportVP9SVC");
+  if (group.empty())
+    return false;
+
+  if (sscanf(group.c_str(), "EnabledByFlag_%dSL%dTL", num_spatial_layers,
+             num_temporal_layers) != 2) {
+    return false;
+  }
+  const int kMaxSpatialLayers = 2;
+  if (*num_spatial_layers > kMaxSpatialLayers || *num_spatial_layers < 1)
+    return false;
+
+  const int kMaxTemporalLayers = 3;
+  if (*num_temporal_layers > kMaxTemporalLayers || *num_temporal_layers < 1)
+    return false;
+
+  return true;
+}
+
+int GetDefaultVp9SpatialLayers() {
+  int num_sl;
+  int num_tl;
+  if (GetVp9LayersFromFieldTrialGroup(&num_sl, &num_tl)) {
+    return num_sl;
+  }
+  return 1;
+}
+
+int GetDefaultVp9TemporalLayers() {
+  int num_sl;
+  int num_tl;
+  if (GetVp9LayersFromFieldTrialGroup(&num_sl, &num_tl)) {
+    return num_tl;
+  }
+  return 1;
+}
+
+class EncoderStreamFactory
+    : public webrtc::VideoEncoderConfig::VideoStreamFactoryInterface {
+ public:
+  EncoderStreamFactory(std::string codec_name,
+                       int max_qp,
+                       int max_framerate,
+                       bool is_screencast,
+                       bool conference_mode)
+      : codec_name_(codec_name),
+        max_qp_(max_qp),
+        max_framerate_(max_framerate),
+        is_screencast_(is_screencast),
+        conference_mode_(conference_mode) {}
+
+ private:
+  std::vector<webrtc::VideoStream> CreateEncoderStreams(
+      int width,
+      int height,
+      const webrtc::VideoEncoderConfig& encoder_config) override {
+    if (is_screencast_ &&
+        (!conference_mode_ || !cricket::UseSimulcastScreenshare())) {
+      RTC_DCHECK_EQ(1, encoder_config.number_of_streams);
+    }
+    if (encoder_config.number_of_streams > 1 ||
+        (CodecNamesEq(codec_name_, kVp8CodecName) && is_screencast_ &&
+         conference_mode_)) {
+      return GetSimulcastConfig(encoder_config.number_of_streams, width, height,
+                                encoder_config.max_bitrate_bps, max_qp_,
+                                max_framerate_, is_screencast_);
+    }
+
+    // For unset max bitrates set default bitrate for non-simulcast.
+    int max_bitrate_bps =
+        (encoder_config.max_bitrate_bps > 0)
+            ? encoder_config.max_bitrate_bps
+            : GetMaxDefaultVideoBitrateKbps(width, height) * 1000;
+
+    webrtc::VideoStream stream;
+    stream.width = width;
+    stream.height = height;
+    stream.max_framerate = max_framerate_;
+    stream.min_bitrate_bps = kMinVideoBitrateKbps * 1000;
+    stream.target_bitrate_bps = stream.max_bitrate_bps = max_bitrate_bps;
+    stream.max_qp = max_qp_;
+
+    if (CodecNamesEq(codec_name_, kVp9CodecName) && !is_screencast_) {
+      stream.temporal_layer_thresholds_bps.resize(
+          GetDefaultVp9TemporalLayers() - 1);
+    }
+
+    std::vector<webrtc::VideoStream> streams;
+    streams.push_back(stream);
+    return streams;
+  }
+
+  const std::string codec_name_;
+  const int max_qp_;
+  const int max_framerate_;
+  const bool is_screencast_;
+  const bool conference_mode_;
+};
+
+}  // namespace
+
+// Constants defined in webrtc/media/engine/constants.h
+// TODO(pbos): Move these to a separate constants.cc file.
+const int kMinVideoBitrateKbps = 30;
+
+const int kVideoMtu = 1200;
+const int kVideoRtpBufferSize = 65536;
+
+// This constant is really an on/off, lower-level configurable NACK history
+// duration hasn't been implemented.
+static const int kNackHistoryMs = 1000;
+
+static const int kDefaultQpMax = 56;
+
+static const int kDefaultRtcpReceiverReportSsrc = 1;
+
+// Minimum time interval for logging stats.
+static const int64_t kStatsLogIntervalMs = 10000;
+
+static std::vector<VideoCodec> GetSupportedCodecs(
+    const WebRtcVideoEncoderFactory* external_encoder_factory);
+
+rtc::scoped_refptr<webrtc::VideoEncoderConfig::EncoderSpecificSettings>
+WebRtcVideoChannel2::WebRtcVideoSendStream::ConfigureVideoEncoderSettings(
+    const VideoCodec& codec) {
+  RTC_DCHECK_RUN_ON(&thread_checker_);
+  bool is_screencast = parameters_.options.is_screencast.value_or(false);
+  // No automatic resizing when using simulcast or screencast.
+  bool automatic_resize =
+      !is_screencast && parameters_.config.rtp.ssrcs.size() == 1;
+  bool frame_dropping = !is_screencast;
+  bool denoising;
+  bool codec_default_denoising = false;
+  if (is_screencast) {
+    denoising = false;
+  } else {
+    // Use codec default if video_noise_reduction is unset.
+    codec_default_denoising = !parameters_.options.video_noise_reduction;
+    denoising = parameters_.options.video_noise_reduction.value_or(false);
+  }
+
+  if (CodecNamesEq(codec.name, kH264CodecName)) {
+    webrtc::VideoCodecH264 h264_settings =
+        webrtc::VideoEncoder::GetDefaultH264Settings();
+    h264_settings.frameDroppingOn = frame_dropping;
+    return new rtc::RefCountedObject<
+        webrtc::VideoEncoderConfig::H264EncoderSpecificSettings>(h264_settings);
+  }
+  if (CodecNamesEq(codec.name, kVp8CodecName)) {
+    webrtc::VideoCodecVP8 vp8_settings =
+        webrtc::VideoEncoder::GetDefaultVp8Settings();
+    vp8_settings.automaticResizeOn = automatic_resize;
+    // VP8 denoising is enabled by default.
+    vp8_settings.denoisingOn = codec_default_denoising ? true : denoising;
+    vp8_settings.frameDroppingOn = frame_dropping;
+    return new rtc::RefCountedObject<
+        webrtc::VideoEncoderConfig::Vp8EncoderSpecificSettings>(vp8_settings);
+  }
+  if (CodecNamesEq(codec.name, kVp9CodecName)) {
+    webrtc::VideoCodecVP9 vp9_settings =
+        webrtc::VideoEncoder::GetDefaultVp9Settings();
+    if (is_screencast) {
+      // TODO(asapersson): Set to 2 for now since there is a DCHECK in
+      // VideoSendStream::ReconfigureVideoEncoder.
+      vp9_settings.numberOfSpatialLayers = 2;
+    } else {
+      vp9_settings.numberOfSpatialLayers = GetDefaultVp9SpatialLayers();
+    }
+    // VP9 denoising is disabled by default.
+    vp9_settings.denoisingOn = codec_default_denoising ? false : denoising;
+    vp9_settings.frameDroppingOn = frame_dropping;
+    return new rtc::RefCountedObject<
+        webrtc::VideoEncoderConfig::Vp9EncoderSpecificSettings>(vp9_settings);
+  }
+  return nullptr;
+}
+
+DefaultUnsignalledSsrcHandler::DefaultUnsignalledSsrcHandler()
+    : default_recv_ssrc_(0), default_sink_(NULL) {}
+
+UnsignalledSsrcHandler::Action DefaultUnsignalledSsrcHandler::OnUnsignalledSsrc(
+    WebRtcVideoChannel2* channel,
+    uint32_t ssrc) {
+  if (default_recv_ssrc_ != 0) {  // Already one default stream, so replace it.
+    channel->RemoveRecvStream(default_recv_ssrc_);
+    default_recv_ssrc_ = 0;
+  }
+
+  StreamParams sp;
+  sp.ssrcs.push_back(ssrc);
+  LOG(LS_INFO) << "Creating default receive stream for SSRC=" << ssrc << ".";
+  if (!channel->AddRecvStream(sp, true)) {
+    LOG(LS_WARNING) << "Could not create default receive stream.";
+  }
+
+  channel->SetSink(ssrc, default_sink_);
+  default_recv_ssrc_ = ssrc;
+  return kDeliverPacket;
+}
+
+rtc::VideoSinkInterface<webrtc::VideoFrame>*
+DefaultUnsignalledSsrcHandler::GetDefaultSink() const {
+  return default_sink_;
+}
+
+void DefaultUnsignalledSsrcHandler::SetDefaultSink(
+    VideoMediaChannel* channel,
+    rtc::VideoSinkInterface<webrtc::VideoFrame>* sink) {
+  default_sink_ = sink;
+  if (default_recv_ssrc_ != 0) {
+    channel->SetSink(default_recv_ssrc_, default_sink_);
+  }
+}
+
+WebRtcVideoEngine2::WebRtcVideoEngine2()
+    : initialized_(false),
+      external_decoder_factory_(NULL),
+      external_encoder_factory_(NULL) {
+  LOG(LS_INFO) << "WebRtcVideoEngine2::WebRtcVideoEngine2()";
+}
+
+WebRtcVideoEngine2::~WebRtcVideoEngine2() {
+  LOG(LS_INFO) << "WebRtcVideoEngine2::~WebRtcVideoEngine2";
+}
+
+void WebRtcVideoEngine2::Init() {
+  LOG(LS_INFO) << "WebRtcVideoEngine2::Init";
+  initialized_ = true;
+}
+
+WebRtcVideoChannel2* WebRtcVideoEngine2::CreateChannel(
+    webrtc::Call* call,
+    const MediaConfig& config,
+    const VideoOptions& options) {
+  RTC_DCHECK(initialized_);
+  LOG(LS_INFO) << "CreateChannel. Options: " << options.ToString();
+  return new WebRtcVideoChannel2(call, config, options,
+                                 external_encoder_factory_,
+                                 external_decoder_factory_);
+}
+
+std::vector<VideoCodec> WebRtcVideoEngine2::codecs() const {
+  return GetSupportedCodecs(external_encoder_factory_);
+}
+
+RtpCapabilities WebRtcVideoEngine2::GetCapabilities() const {
+  RtpCapabilities capabilities;
+  capabilities.header_extensions.push_back(
+      webrtc::RtpExtension(webrtc::RtpExtension::kTimestampOffsetUri,
+                           webrtc::RtpExtension::kTimestampOffsetDefaultId));
+  capabilities.header_extensions.push_back(
+      webrtc::RtpExtension(webrtc::RtpExtension::kAbsSendTimeUri,
+                           webrtc::RtpExtension::kAbsSendTimeDefaultId));
+  capabilities.header_extensions.push_back(
+      webrtc::RtpExtension(webrtc::RtpExtension::kVideoRotationUri,
+                           webrtc::RtpExtension::kVideoRotationDefaultId));
+  capabilities.header_extensions.push_back(webrtc::RtpExtension(
+      webrtc::RtpExtension::kTransportSequenceNumberUri,
+      webrtc::RtpExtension::kTransportSequenceNumberDefaultId));
+  capabilities.header_extensions.push_back(
+      webrtc::RtpExtension(webrtc::RtpExtension::kPlayoutDelayUri,
+                           webrtc::RtpExtension::kPlayoutDelayDefaultId));
+  return capabilities;
+}
+
+void WebRtcVideoEngine2::SetExternalDecoderFactory(
+    WebRtcVideoDecoderFactory* decoder_factory) {
+  RTC_DCHECK(!initialized_);
+  external_decoder_factory_ = decoder_factory;
+}
+
+void WebRtcVideoEngine2::SetExternalEncoderFactory(
+    WebRtcVideoEncoderFactory* encoder_factory) {
+  RTC_DCHECK(!initialized_);
+  if (external_encoder_factory_ == encoder_factory)
+    return;
+
+  // No matter what happens we shouldn't hold on to a stale
+  // WebRtcSimulcastEncoderFactory.
+  simulcast_encoder_factory_.reset();
+
+  if (encoder_factory &&
+      WebRtcSimulcastEncoderFactory::UseSimulcastEncoderFactory(
+          encoder_factory->supported_codecs())) {
+    simulcast_encoder_factory_.reset(
+        new WebRtcSimulcastEncoderFactory(encoder_factory));
+    encoder_factory = simulcast_encoder_factory_.get();
+  }
+  external_encoder_factory_ = encoder_factory;
+}
+
+// This is a helper function for AppendVideoCodecs below. It will return the
+// first unused dynamic payload type (in the range [96, 127]), or nothing if no
+// payload type is unused.
+static rtc::Optional<int> NextFreePayloadType(
+    const std::vector<VideoCodec>& codecs) {
+  static const int kFirstDynamicPayloadType = 96;
+  static const int kLastDynamicPayloadType = 127;
+  bool is_payload_used[1 + kLastDynamicPayloadType - kFirstDynamicPayloadType] =
+      {false};
+  for (const VideoCodec& codec : codecs) {
+    if (kFirstDynamicPayloadType <= codec.id &&
+        codec.id <= kLastDynamicPayloadType) {
+      is_payload_used[codec.id - kFirstDynamicPayloadType] = true;
+    }
+  }
+  for (int i = kFirstDynamicPayloadType; i <= kLastDynamicPayloadType; ++i) {
+    if (!is_payload_used[i - kFirstDynamicPayloadType])
+      return rtc::Optional<int>(i);
+  }
+  // No free payload type.
+  return rtc::Optional<int>();
+}
+
+// This is a helper function for GetSupportedCodecs below. It will append new
+// unique codecs from |input_codecs| to |unified_codecs|. It will add default
+// feedback params to the codecs and will also add an associated RTX codec for
+// recognized codecs (VP8, VP9, H264, and RED).
+static void AppendVideoCodecs(const std::vector<VideoCodec>& input_codecs,
+                              std::vector<VideoCodec>* unified_codecs) {
+  for (VideoCodec codec : input_codecs) {
+    const rtc::Optional<int> payload_type =
+        NextFreePayloadType(*unified_codecs);
+    if (!payload_type)
+      return;
+    codec.id = *payload_type;
+    // TODO(magjed): Move the responsibility of setting these parameters to the
+    // encoder factories instead.
+    if (codec.name != kRedCodecName && codec.name != kUlpfecCodecName &&
+        codec.name != kFlexfecCodecName)
+      AddDefaultFeedbackParams(&codec);
+    // Don't add same codec twice.
+    if (FindMatchingCodec(*unified_codecs, codec))
+      continue;
+
+	if (CodecNamesEq(codec.name, kH264CodecName))
+	{
+		auto it = unified_codecs->begin();
+		unified_codecs->insert(it, codec);
+	}
+	else
+	{
+		unified_codecs->push_back(codec);
+	}
+
+
+    // Add associated RTX codec for recognized codecs.
+    // TODO(deadbeef): Should we add RTX codecs for external codecs whose names
+    // we don't recognize?
+    if (CodecNamesEq(codec.name, kVp8CodecName) ||
+        CodecNamesEq(codec.name, kVp9CodecName) ||
+        CodecNamesEq(codec.name, kH264CodecName) ||
+        CodecNamesEq(codec.name, kRedCodecName)) {
+      const rtc::Optional<int> rtx_payload_type =
+          NextFreePayloadType(*unified_codecs);
+      if (!rtx_payload_type)
+        return;
+      unified_codecs->push_back(
+          VideoCodec::CreateRtxCodec(*rtx_payload_type, codec.id));
+    }
+  }
+}
+
+static std::vector<VideoCodec> GetSupportedCodecs(
+    const WebRtcVideoEncoderFactory* external_encoder_factory) {
+  const std::vector<VideoCodec> internal_codecs =
+      InternalEncoderFactory().supported_codecs();
+  LOG(LS_INFO) << "Internally supported codecs: "
+               << CodecVectorToString(internal_codecs);
+
+  std::vector<VideoCodec> unified_codecs;
+  AppendVideoCodecs(internal_codecs, &unified_codecs);
+
+  if (external_encoder_factory != nullptr) {
+    const std::vector<VideoCodec>& external_codecs =
+        external_encoder_factory->supported_codecs();
+    AppendVideoCodecs(external_codecs, &unified_codecs);
+    LOG(LS_INFO) << "Codecs supported by the external encoder factory: "
+                 << CodecVectorToString(external_codecs);
+  }
+
+  return unified_codecs;
+}
+
+WebRtcVideoChannel2::WebRtcVideoChannel2(
+    webrtc::Call* call,
+    const MediaConfig& config,
+    const VideoOptions& options,
+    WebRtcVideoEncoderFactory* external_encoder_factory,
+    WebRtcVideoDecoderFactory* external_decoder_factory)
+    : VideoMediaChannel(config),
+      call_(call),
+      unsignalled_ssrc_handler_(&default_unsignalled_ssrc_handler_),
+      video_config_(config.video),
+      external_encoder_factory_(external_encoder_factory),
+      external_decoder_factory_(external_decoder_factory),
+      default_send_options_(options),
+      last_stats_log_ms_(-1) {
+  RTC_DCHECK(thread_checker_.CalledOnValidThread());
+
+  rtcp_receiver_report_ssrc_ = kDefaultRtcpReceiverReportSsrc;
+  sending_ = false;
+  recv_codecs_ = MapCodecs(GetSupportedCodecs(external_encoder_factory));
+}
+
+WebRtcVideoChannel2::~WebRtcVideoChannel2() {
+  for (auto& kv : send_streams_)
+    delete kv.second;
+  for (auto& kv : receive_streams_)
+    delete kv.second;
+}
+
+rtc::Optional<WebRtcVideoChannel2::VideoCodecSettings>
+WebRtcVideoChannel2::SelectSendVideoCodec(
+    const std::vector<VideoCodecSettings>& remote_mapped_codecs) const {
+  const std::vector<VideoCodec> local_supported_codecs =
+      GetSupportedCodecs(external_encoder_factory_);
+  // Select the first remote codec that is supported locally.
+  for (const VideoCodecSettings& remote_mapped_codec : remote_mapped_codecs) {
+    // For H264, we will limit the encode level to the remote offered level
+    // regardless if level asymmetry is allowed or not. This is strictly not
+    // following the spec in https://tools.ietf.org/html/rfc6184#section-8.2.2
+    // since we should limit the encode level to the lower of local and remote
+    // level when level asymmetry is not allowed.
+    if (FindMatchingCodec(local_supported_codecs, remote_mapped_codec.codec))
+      return rtc::Optional<VideoCodecSettings>(remote_mapped_codec);
+  }
+  // No remote codec was supported.
+  return rtc::Optional<VideoCodecSettings>();
+}
+
+bool WebRtcVideoChannel2::ReceiveCodecsHaveChanged(
+    std::vector<VideoCodecSettings> before,
+    std::vector<VideoCodecSettings> after) {
+  if (before.size() != after.size()) {
+    return true;
+  }
+  // The receive codec order doesn't matter, so we sort the codecs before
+  // comparing. This is necessary because currently the
+  // only way to change the send codec is to munge SDP, which causes
+  // the receive codec list to change order, which causes the streams
+  // to be recreates which causes a "blink" of black video.  In order
+  // to support munging the SDP in this way without recreating receive
+  // streams, we ignore the order of the received codecs so that
+  // changing the order doesn't cause this "blink".
+  auto comparison =
+      [](const VideoCodecSettings& codec1, const VideoCodecSettings& codec2) {
+        return codec1.codec.id > codec2.codec.id;
+      };
+  std::sort(before.begin(), before.end(), comparison);
+  std::sort(after.begin(), after.end(), comparison);
+  return before != after;
+}
+
+bool WebRtcVideoChannel2::GetChangedSendParameters(
+    const VideoSendParameters& params,
+    ChangedSendParameters* changed_params) const {
+  if (!ValidateCodecFormats(params.codecs) ||
+      !ValidateRtpExtensions(params.extensions)) {
+    return false;
+  }
+
+  // Select one of the remote codecs that will be used as send codec.
+  const rtc::Optional<VideoCodecSettings> selected_send_codec =
+      SelectSendVideoCodec(MapCodecs(params.codecs));
+
+  if (!selected_send_codec) {
+    LOG(LS_ERROR) << "No video codecs supported.";
+    return false;
+  }
+
+  if (!send_codec_ || *selected_send_codec != *send_codec_)
+    changed_params->codec = selected_send_codec;
+
+  // Handle RTP header extensions.
+  std::vector<webrtc::RtpExtension> filtered_extensions = FilterRtpExtensions(
+      params.extensions, webrtc::RtpExtension::IsSupportedForVideo, true);
+  if (!send_rtp_extensions_ || (*send_rtp_extensions_ != filtered_extensions)) {
+    changed_params->rtp_header_extensions =
+        rtc::Optional<std::vector<webrtc::RtpExtension>>(filtered_extensions);
+  }
+
+  // Handle max bitrate.
+  if (params.max_bandwidth_bps != send_params_.max_bandwidth_bps &&
+      params.max_bandwidth_bps >= 0) {
+    // 0 uncaps max bitrate (-1).
+    changed_params->max_bandwidth_bps = rtc::Optional<int>(
+        params.max_bandwidth_bps == 0 ? -1 : params.max_bandwidth_bps);
+  }
+
+  // Handle conference mode.
+  if (params.conference_mode != send_params_.conference_mode) {
+    changed_params->conference_mode =
+        rtc::Optional<bool>(params.conference_mode);
+  }
+
+  // Handle RTCP mode.
+  if (params.rtcp.reduced_size != send_params_.rtcp.reduced_size) {
+    changed_params->rtcp_mode = rtc::Optional<webrtc::RtcpMode>(
+        params.rtcp.reduced_size ? webrtc::RtcpMode::kReducedSize
+                                 : webrtc::RtcpMode::kCompound);
+  }
+
+  return true;
+}
+
+rtc::DiffServCodePoint WebRtcVideoChannel2::PreferredDscp() const {
+  return rtc::DSCP_AF41;
+}
+
+bool WebRtcVideoChannel2::SetSendParameters(const VideoSendParameters& params) {
+  TRACE_EVENT0("webrtc", "WebRtcVideoChannel2::SetSendParameters");
+  LOG(LS_INFO) << "SetSendParameters: " << params.ToString();
+  ChangedSendParameters changed_params;
+  if (!GetChangedSendParameters(params, &changed_params)) {
+    return false;
+  }
+
+  if (changed_params.codec) {
+    const VideoCodecSettings& codec_settings = *changed_params.codec;
+    send_codec_ = rtc::Optional<VideoCodecSettings>(codec_settings);
+    LOG(LS_INFO) << "Using codec: " << codec_settings.codec.ToString();
+  }
+
+  if (changed_params.rtp_header_extensions) {
+    send_rtp_extensions_ = changed_params.rtp_header_extensions;
+  }
+
+  if (changed_params.codec || changed_params.max_bandwidth_bps) {
+    if (send_codec_) {
+      // TODO(holmer): Changing the codec parameters shouldn't necessarily mean
+      // that we change the min/max of bandwidth estimation. Reevaluate this.
+      bitrate_config_ = GetBitrateConfigForCodec(send_codec_->codec);
+      if (!changed_params.codec) {
+        // If the codec isn't changing, set the start bitrate to -1 which means
+        // "unchanged" so that BWE isn't affected.
+        bitrate_config_.start_bitrate_bps = -1;
+      }
+    }
+    if (params.max_bandwidth_bps >= 0) {
+      // Note that max_bandwidth_bps intentionally takes priority over the
+      // bitrate config for the codec. This allows FEC to be applied above the
+      // codec target bitrate.
+      // TODO(pbos): Figure out whether b=AS means max bitrate for this
+      // WebRtcVideoChannel2 (in which case we're good), or per sender (SSRC),
+      // in which case this should not set a Call::BitrateConfig but rather
+      // reconfigure all senders.
+      bitrate_config_.max_bitrate_bps =
+          params.max_bandwidth_bps == 0 ? -1 : params.max_bandwidth_bps;
+    }
+    call_->SetBitrateConfig(bitrate_config_);
+  }
+
+  {
+    rtc::CritScope stream_lock(&stream_crit_);
+    for (auto& kv : send_streams_) {
+      kv.second->SetSendParameters(changed_params);
+    }
+    if (changed_params.codec || changed_params.rtcp_mode) {
+      // Update receive feedback parameters from new codec or RTCP mode.
+      LOG(LS_INFO)
+          << "SetFeedbackOptions on all the receive streams because the send "
+             "codec or RTCP mode has changed.";
+      for (auto& kv : receive_streams_) {
+        RTC_DCHECK(kv.second != nullptr);
+        kv.second->SetFeedbackParameters(
+            HasNack(send_codec_->codec), HasRemb(send_codec_->codec),
+            HasTransportCc(send_codec_->codec),
+            params.rtcp.reduced_size ? webrtc::RtcpMode::kReducedSize
+                                     : webrtc::RtcpMode::kCompound);
+      }
+    }
+  }
+  send_params_ = params;
+  return true;
+}
+
+webrtc::RtpParameters WebRtcVideoChannel2::GetRtpSendParameters(
+    uint32_t ssrc) const {
+  rtc::CritScope stream_lock(&stream_crit_);
+  auto it = send_streams_.find(ssrc);
+  if (it == send_streams_.end()) {
+    LOG(LS_WARNING) << "Attempting to get RTP send parameters for stream "
+                    << "with ssrc " << ssrc << " which doesn't exist.";
+    return webrtc::RtpParameters();
+  }
+
+  webrtc::RtpParameters rtp_params = it->second->GetRtpParameters();
+  // Need to add the common list of codecs to the send stream-specific
+  // RTP parameters.
+  for (const VideoCodec& codec : send_params_.codecs) {
+    rtp_params.codecs.push_back(codec.ToCodecParameters());
+  }
+  return rtp_params;
+}
+
+bool WebRtcVideoChannel2::SetRtpSendParameters(
+    uint32_t ssrc,
+    const webrtc::RtpParameters& parameters) {
+  TRACE_EVENT0("webrtc", "WebRtcVideoChannel2::SetRtpSendParameters");
+  rtc::CritScope stream_lock(&stream_crit_);
+  auto it = send_streams_.find(ssrc);
+  if (it == send_streams_.end()) {
+    LOG(LS_ERROR) << "Attempting to set RTP send parameters for stream "
+                  << "with ssrc " << ssrc << " which doesn't exist.";
+    return false;
+  }
+
+  // TODO(deadbeef): Handle setting parameters with a list of codecs in a
+  // different order (which should change the send codec).
+  webrtc::RtpParameters current_parameters = GetRtpSendParameters(ssrc);
+  if (current_parameters.codecs != parameters.codecs) {
+    LOG(LS_ERROR) << "Using SetParameters to change the set of codecs "
+                  << "is not currently supported.";
+    return false;
+  }
+
+  return it->second->SetRtpParameters(parameters);
+}
+
+webrtc::RtpParameters WebRtcVideoChannel2::GetRtpReceiveParameters(
+    uint32_t ssrc) const {
+  rtc::CritScope stream_lock(&stream_crit_);
+  auto it = receive_streams_.find(ssrc);
+  if (it == receive_streams_.end()) {
+    LOG(LS_WARNING) << "Attempting to get RTP receive parameters for stream "
+                    << "with ssrc " << ssrc << " which doesn't exist.";
+    return webrtc::RtpParameters();
+  }
+
+  // TODO(deadbeef): Return stream-specific parameters.
+  webrtc::RtpParameters rtp_params = CreateRtpParametersWithOneEncoding();
+  for (const VideoCodec& codec : recv_params_.codecs) {
+    rtp_params.codecs.push_back(codec.ToCodecParameters());
+  }
+  rtp_params.encodings[0].ssrc = it->second->GetFirstPrimarySsrc();
+  return rtp_params;
+}
+
+bool WebRtcVideoChannel2::SetRtpReceiveParameters(
+    uint32_t ssrc,
+    const webrtc::RtpParameters& parameters) {
+  TRACE_EVENT0("webrtc", "WebRtcVideoChannel2::SetRtpReceiveParameters");
+  rtc::CritScope stream_lock(&stream_crit_);
+  auto it = receive_streams_.find(ssrc);
+  if (it == receive_streams_.end()) {
+    LOG(LS_ERROR) << "Attempting to set RTP receive parameters for stream "
+                  << "with ssrc " << ssrc << " which doesn't exist.";
+    return false;
+  }
+
+  webrtc::RtpParameters current_parameters = GetRtpReceiveParameters(ssrc);
+  if (current_parameters != parameters) {
+    LOG(LS_ERROR) << "Changing the RTP receive parameters is currently "
+                  << "unsupported.";
+    return false;
+  }
+  return true;
+}
+
+bool WebRtcVideoChannel2::GetChangedRecvParameters(
+    const VideoRecvParameters& params,
+    ChangedRecvParameters* changed_params) const {
+  if (!ValidateCodecFormats(params.codecs) ||
+      !ValidateRtpExtensions(params.extensions)) {
+    return false;
+  }
+
+  // Handle receive codecs.
+  const std::vector<VideoCodecSettings> mapped_codecs =
+      MapCodecs(params.codecs);
+  if (mapped_codecs.empty()) {
+    LOG(LS_ERROR) << "SetRecvParameters called without any video codecs.";
+    return false;
+  }
+
+  // Verify that every mapped codec is supported locally.
+  const std::vector<VideoCodec> local_supported_codecs =
+      GetSupportedCodecs(external_encoder_factory_);
+  for (const VideoCodecSettings& mapped_codec : mapped_codecs) {
+    if (!FindMatchingCodec(local_supported_codecs, mapped_codec.codec)) {
+      LOG(LS_ERROR) << "SetRecvParameters called with unsupported video codec: "
+                    << mapped_codec.codec.ToString();
+      return false;
+    }
+  }
+
+  if (ReceiveCodecsHaveChanged(recv_codecs_, mapped_codecs)) {
+    changed_params->codec_settings =
+        rtc::Optional<std::vector<VideoCodecSettings>>(mapped_codecs);
+  }
+
+  // Handle RTP header extensions.
+  std::vector<webrtc::RtpExtension> filtered_extensions = FilterRtpExtensions(
+      params.extensions, webrtc::RtpExtension::IsSupportedForVideo, false);
+  if (filtered_extensions != recv_rtp_extensions_) {
+    changed_params->rtp_header_extensions =
+        rtc::Optional<std::vector<webrtc::RtpExtension>>(filtered_extensions);
+  }
+
+  return true;
+}
+
+bool WebRtcVideoChannel2::SetRecvParameters(const VideoRecvParameters& params) {
+  TRACE_EVENT0("webrtc", "WebRtcVideoChannel2::SetRecvParameters");
+  LOG(LS_INFO) << "SetRecvParameters: " << params.ToString();
+  ChangedRecvParameters changed_params;
+  if (!GetChangedRecvParameters(params, &changed_params)) {
+    return false;
+  }
+  if (changed_params.rtp_header_extensions) {
+    recv_rtp_extensions_ = *changed_params.rtp_header_extensions;
+  }
+  if (changed_params.codec_settings) {
+    LOG(LS_INFO) << "Changing recv codecs from "
+                 << CodecSettingsVectorToString(recv_codecs_) << " to "
+                 << CodecSettingsVectorToString(*changed_params.codec_settings);
+    recv_codecs_ = *changed_params.codec_settings;
+  }
+
+  {
+    rtc::CritScope stream_lock(&stream_crit_);
+    for (auto& kv : receive_streams_) {
+      kv.second->SetRecvParameters(changed_params);
+    }
+  }
+  recv_params_ = params;
+  return true;
+}
+
+std::string WebRtcVideoChannel2::CodecSettingsVectorToString(
+    const std::vector<VideoCodecSettings>& codecs) {
+  std::stringstream out;
+  out << '{';
+  for (size_t i = 0; i < codecs.size(); ++i) {
+    out << codecs[i].codec.ToString();
+    if (i != codecs.size() - 1) {
+      out << ", ";
+    }
+  }
+  out << '}';
+  return out.str();
+}
+
+bool WebRtcVideoChannel2::GetSendCodec(VideoCodec* codec) {
+  if (!send_codec_) {
+    LOG(LS_VERBOSE) << "GetSendCodec: No send codec set.";
+    return false;
+  }
+  *codec = send_codec_->codec;
+  return true;
+}
+
+bool WebRtcVideoChannel2::SetSend(bool send) {
+  TRACE_EVENT0("webrtc", "WebRtcVideoChannel2::SetSend");
+  LOG(LS_VERBOSE) << "SetSend: " << (send ? "true" : "false");
+  if (send && !send_codec_) {
+    LOG(LS_ERROR) << "SetSend(true) called before setting codec.";
+    return false;
+  }
+  {
+    rtc::CritScope stream_lock(&stream_crit_);
+    for (const auto& kv : send_streams_) {
+      kv.second->SetSend(send);
+    }
+  }
+  sending_ = send;
+  return true;
+}
+
+// TODO(nisse): The enable argument was used for mute logic which has
+// been moved to VideoBroadcaster. So remove the argument from this
+// method.
+bool WebRtcVideoChannel2::SetVideoSend(
+    uint32_t ssrc,
+    bool enable,
+    const VideoOptions* options,
+    rtc::VideoSourceInterface<webrtc::VideoFrame>* source) {
+  TRACE_EVENT0("webrtc", "SetVideoSend");
+  RTC_DCHECK(ssrc != 0);
+  LOG(LS_INFO) << "SetVideoSend (ssrc= " << ssrc << ", enable = " << enable
+               << ", options: " << (options ? options->ToString() : "nullptr")
+               << ", source = " << (source ? "(source)" : "nullptr") << ")";
+
+  rtc::CritScope stream_lock(&stream_crit_);
+  const auto& kv = send_streams_.find(ssrc);
+  if (kv == send_streams_.end()) {
+    // Allow unknown ssrc only if source is null.
+    RTC_CHECK(source == nullptr);
+    LOG(LS_ERROR) << "No sending stream on ssrc " << ssrc;
+    return false;
+  }
+
+  return kv->second->SetVideoSend(enable, options, source);
+}
+
+bool WebRtcVideoChannel2::ValidateSendSsrcAvailability(
+    const StreamParams& sp) const {
+  for (uint32_t ssrc : sp.ssrcs) {
+    if (send_ssrcs_.find(ssrc) != send_ssrcs_.end()) {
+      LOG(LS_ERROR) << "Send stream with SSRC '" << ssrc << "' already exists.";
+      return false;
+    }
+  }
+  return true;
+}
+
+bool WebRtcVideoChannel2::ValidateReceiveSsrcAvailability(
+    const StreamParams& sp) const {
+  for (uint32_t ssrc : sp.ssrcs) {
+    if (receive_ssrcs_.find(ssrc) != receive_ssrcs_.end()) {
+      LOG(LS_ERROR) << "Receive stream with SSRC '" << ssrc
+                    << "' already exists.";
+      return false;
+    }
+  }
+  return true;
+}
+
+bool WebRtcVideoChannel2::AddSendStream(const StreamParams& sp) {
+  LOG(LS_INFO) << "AddSendStream: " << sp.ToString();
+  if (!ValidateStreamParams(sp))
+    return false;
+
+  rtc::CritScope stream_lock(&stream_crit_);
+
+  if (!ValidateSendSsrcAvailability(sp))
+    return false;
+
+  for (uint32_t used_ssrc : sp.ssrcs)
+    send_ssrcs_.insert(used_ssrc);
+
+  webrtc::VideoSendStream::Config config(this);
+  config.suspend_below_min_bitrate = video_config_.suspend_below_min_bitrate;
+  config.periodic_alr_bandwidth_probing =
+      video_config_.periodic_alr_bandwidth_probing;
+  WebRtcVideoSendStream* stream = new WebRtcVideoSendStream(
+      call_, sp, std::move(config), default_send_options_,
+      external_encoder_factory_, video_config_.enable_cpu_overuse_detection,
+      bitrate_config_.max_bitrate_bps, send_codec_, send_rtp_extensions_,
+      send_params_);
+
+  uint32_t ssrc = sp.first_ssrc();
+  RTC_DCHECK(ssrc != 0);
+  send_streams_[ssrc] = stream;
+
+  if (rtcp_receiver_report_ssrc_ == kDefaultRtcpReceiverReportSsrc) {
+    rtcp_receiver_report_ssrc_ = ssrc;
+    LOG(LS_INFO) << "SetLocalSsrc on all the receive streams because we added "
+                    "a send stream.";
+    for (auto& kv : receive_streams_)
+      kv.second->SetLocalSsrc(ssrc);
+  }
+  if (sending_) {
+    stream->SetSend(true);
+  }
+
+  return true;
+}
+
+bool WebRtcVideoChannel2::RemoveSendStream(uint32_t ssrc) {
+  LOG(LS_INFO) << "RemoveSendStream: " << ssrc;
+
+  WebRtcVideoSendStream* removed_stream;
+  {
+    rtc::CritScope stream_lock(&stream_crit_);
+    std::map<uint32_t, WebRtcVideoSendStream*>::iterator it =
+        send_streams_.find(ssrc);
+    if (it == send_streams_.end()) {
+      return false;
+    }
+
+    for (uint32_t old_ssrc : it->second->GetSsrcs())
+      send_ssrcs_.erase(old_ssrc);
+
+    removed_stream = it->second;
+    send_streams_.erase(it);
+
+    // Switch receiver report SSRCs, the one in use is no longer valid.
+    if (rtcp_receiver_report_ssrc_ == ssrc) {
+      rtcp_receiver_report_ssrc_ = send_streams_.empty()
+                                       ? kDefaultRtcpReceiverReportSsrc
+                                       : send_streams_.begin()->first;
+      LOG(LS_INFO) << "SetLocalSsrc on all the receive streams because the "
+                      "previous local SSRC was removed.";
+
+      for (auto& kv : receive_streams_) {
+        kv.second->SetLocalSsrc(rtcp_receiver_report_ssrc_);
+      }
+    }
+  }
+
+  delete removed_stream;
+
+  return true;
+}
+
+void WebRtcVideoChannel2::DeleteReceiveStream(
+    WebRtcVideoChannel2::WebRtcVideoReceiveStream* stream) {
+  for (uint32_t old_ssrc : stream->GetSsrcs())
+    receive_ssrcs_.erase(old_ssrc);
+  delete stream;
+}
+
+bool WebRtcVideoChannel2::AddRecvStream(const StreamParams& sp) {
+  return AddRecvStream(sp, false);
+}
+
+bool WebRtcVideoChannel2::AddRecvStream(const StreamParams& sp,
+                                        bool default_stream) {
+  RTC_DCHECK(thread_checker_.CalledOnValidThread());
+
+  LOG(LS_INFO) << "AddRecvStream" << (default_stream ? " (default stream)" : "")
+               << ": " << sp.ToString();
+  if (!ValidateStreamParams(sp))
+    return false;
+
+  uint32_t ssrc = sp.first_ssrc();
+  RTC_DCHECK(ssrc != 0);  // TODO(pbos): Is this ever valid?
+
+  rtc::CritScope stream_lock(&stream_crit_);
+  // Remove running stream if this was a default stream.
+  const auto& prev_stream = receive_streams_.find(ssrc);
+  if (prev_stream != receive_streams_.end()) {
+    if (default_stream || !prev_stream->second->IsDefaultStream()) {
+      LOG(LS_ERROR) << "Receive stream for SSRC '" << ssrc
+                    << "' already exists.";
+      return false;
+    }
+    DeleteReceiveStream(prev_stream->second);
+    receive_streams_.erase(prev_stream);
+  }
+
+  if (!ValidateReceiveSsrcAvailability(sp))
+    return false;
+
+  for (uint32_t used_ssrc : sp.ssrcs)
+    receive_ssrcs_.insert(used_ssrc);
+
+  webrtc::VideoReceiveStream::Config config(this);
+  webrtc::FlexfecReceiveStream::Config flexfec_config(this);
+  ConfigureReceiverRtp(&config, &flexfec_config, sp);
+
+  config.disable_prerenderer_smoothing =
+      video_config_.disable_prerenderer_smoothing;
+  config.sync_group = sp.sync_label;
+
+  receive_streams_[ssrc] = new WebRtcVideoReceiveStream(
+      call_, sp, std::move(config), external_decoder_factory_, default_stream,
+      recv_codecs_, flexfec_config);
+
+  return true;
+}
+
+void WebRtcVideoChannel2::ConfigureReceiverRtp(
+    webrtc::VideoReceiveStream::Config* config,
+    webrtc::FlexfecReceiveStream::Config* flexfec_config,
+    const StreamParams& sp) const {
+  uint32_t ssrc = sp.first_ssrc();
+
+  config->rtp.remote_ssrc = ssrc;
+  config->rtp.local_ssrc = rtcp_receiver_report_ssrc_;
+
+  // TODO(pbos): This protection is against setting the same local ssrc as
+  // remote which is not permitted by the lower-level API. RTCP requires a
+  // corresponding sender SSRC. Figure out what to do when we don't have
+  // (receive-only) or know a good local SSRC.
+  if (config->rtp.remote_ssrc == config->rtp.local_ssrc) {
+    if (config->rtp.local_ssrc != kDefaultRtcpReceiverReportSsrc) {
+      config->rtp.local_ssrc = kDefaultRtcpReceiverReportSsrc;
+    } else {
+      config->rtp.local_ssrc = kDefaultRtcpReceiverReportSsrc + 1;
+    }
+  }
+
+  // Whether or not the receive stream sends reduced size RTCP is determined
+  // by the send params.
+  // TODO(deadbeef): Once we change "send_params" to "sender_params" and
+  // "recv_params" to "receiver_params", we should get this out of
+  // receiver_params_.
+  config->rtp.rtcp_mode = send_params_.rtcp.reduced_size
+                              ? webrtc::RtcpMode::kReducedSize
+                              : webrtc::RtcpMode::kCompound;
+
+  config->rtp.remb = send_codec_ ? HasRemb(send_codec_->codec) : false;
+  config->rtp.transport_cc =
+      send_codec_ ? HasTransportCc(send_codec_->codec) : false;
+
+  sp.GetFidSsrc(ssrc, &config->rtp.rtx_ssrc);
+
+  config->rtp.extensions = recv_rtp_extensions_;
+
+  // TODO(brandtr): Generalize when we add support for multistream protection.
+  if (sp.GetFecFrSsrc(ssrc, &flexfec_config->remote_ssrc)) {
+    flexfec_config->protected_media_ssrcs = {ssrc};
+    flexfec_config->local_ssrc = config->rtp.local_ssrc;
+    flexfec_config->rtcp_mode = config->rtp.rtcp_mode;
+    // TODO(brandtr): We should be spec-compliant and set |transport_cc| here
+    // based on the rtcp-fb for the FlexFEC codec, not the media codec.
+    flexfec_config->transport_cc = config->rtp.transport_cc;
+    flexfec_config->rtp_header_extensions = config->rtp.extensions;
+  }
+}
+
+bool WebRtcVideoChannel2::RemoveRecvStream(uint32_t ssrc) {
+  LOG(LS_INFO) << "RemoveRecvStream: " << ssrc;
+  if (ssrc == 0) {
+    LOG(LS_ERROR) << "RemoveRecvStream with 0 ssrc is not supported.";
+    return false;
+  }
+
+  rtc::CritScope stream_lock(&stream_crit_);
+  std::map<uint32_t, WebRtcVideoReceiveStream*>::iterator stream =
+      receive_streams_.find(ssrc);
+  if (stream == receive_streams_.end()) {
+    LOG(LS_ERROR) << "Stream not found for ssrc: " << ssrc;
+    return false;
+  }
+  DeleteReceiveStream(stream->second);
+  receive_streams_.erase(stream);
+
+  return true;
+}
+
+bool WebRtcVideoChannel2::SetSink(
+    uint32_t ssrc,
+    rtc::VideoSinkInterface<webrtc::VideoFrame>* sink) {
+  LOG(LS_INFO) << "SetSink: ssrc:" << ssrc << " "
+               << (sink ? "(ptr)" : "nullptr");
+  if (ssrc == 0) {
+    default_unsignalled_ssrc_handler_.SetDefaultSink(this, sink);
+    return true;
+  }
+
+  rtc::CritScope stream_lock(&stream_crit_);
+  std::map<uint32_t, WebRtcVideoReceiveStream*>::iterator it =
+      receive_streams_.find(ssrc);
+  if (it == receive_streams_.end()) {
+    return false;
+  }
+
+  it->second->SetSink(sink);
+  return true;
+}
+
+bool WebRtcVideoChannel2::GetStats(VideoMediaInfo* info) {
+  TRACE_EVENT0("webrtc", "WebRtcVideoChannel2::GetStats");
+
+  // Log stats periodically.
+  bool log_stats = false;
+  int64_t now_ms = rtc::TimeMillis();
+  if (last_stats_log_ms_ == -1 ||
+      now_ms - last_stats_log_ms_ > kStatsLogIntervalMs) {
+    last_stats_log_ms_ = now_ms;
+    log_stats = true;
+  }
+
+  info->Clear();
+  FillSenderStats(info, log_stats);
+  FillReceiverStats(info, log_stats);
+  FillSendAndReceiveCodecStats(info);
+  webrtc::Call::Stats stats = call_->GetStats();
+  FillBandwidthEstimationStats(stats, info);
+  if (stats.rtt_ms != -1) {
+    for (size_t i = 0; i < info->senders.size(); ++i) {
+      info->senders[i].rtt_ms = stats.rtt_ms;
+    }
+  }
+
+  if (log_stats)
+    LOG(LS_INFO) << stats.ToString(now_ms);
+
+  return true;
+}
+
+void WebRtcVideoChannel2::FillSenderStats(VideoMediaInfo* video_media_info,
+                                          bool log_stats) {
+  rtc::CritScope stream_lock(&stream_crit_);
+  for (std::map<uint32_t, WebRtcVideoSendStream*>::iterator it =
+           send_streams_.begin();
+       it != send_streams_.end(); ++it) {
+    video_media_info->senders.push_back(
+        it->second->GetVideoSenderInfo(log_stats));
+  }
+}
+
+void WebRtcVideoChannel2::FillReceiverStats(VideoMediaInfo* video_media_info,
+                                            bool log_stats) {
+  rtc::CritScope stream_lock(&stream_crit_);
+  for (std::map<uint32_t, WebRtcVideoReceiveStream*>::iterator it =
+           receive_streams_.begin();
+       it != receive_streams_.end(); ++it) {
+    video_media_info->receivers.push_back(
+        it->second->GetVideoReceiverInfo(log_stats));
+  }
+}
+
+void WebRtcVideoChannel2::FillBandwidthEstimationStats(
+    const webrtc::Call::Stats& stats,
+    VideoMediaInfo* video_media_info) {
+  BandwidthEstimationInfo bwe_info;
+  bwe_info.available_send_bandwidth = stats.send_bandwidth_bps;
+  bwe_info.available_recv_bandwidth = stats.recv_bandwidth_bps;
+  bwe_info.bucket_delay = stats.pacer_delay_ms;
+
+  // Get send stream bitrate stats.
+  rtc::CritScope stream_lock(&stream_crit_);
+  for (std::map<uint32_t, WebRtcVideoSendStream*>::iterator stream =
+           send_streams_.begin();
+       stream != send_streams_.end(); ++stream) {
+    stream->second->FillBandwidthEstimationInfo(&bwe_info);
+  }
+  video_media_info->bw_estimations.push_back(bwe_info);
+}
+
+void WebRtcVideoChannel2::FillSendAndReceiveCodecStats(
+    VideoMediaInfo* video_media_info) {
+  for (const VideoCodec& codec : send_params_.codecs) {
+    webrtc::RtpCodecParameters codec_params = codec.ToCodecParameters();
+    video_media_info->send_codecs.insert(
+        std::make_pair(codec_params.payload_type, std::move(codec_params)));
+  }
+  for (const VideoCodec& codec : recv_params_.codecs) {
+    webrtc::RtpCodecParameters codec_params = codec.ToCodecParameters();
+    video_media_info->receive_codecs.insert(
+        std::make_pair(codec_params.payload_type, std::move(codec_params)));
+  }
+}
+
+void WebRtcVideoChannel2::OnPacketReceived(
+    rtc::CopyOnWriteBuffer* packet,
+    const rtc::PacketTime& packet_time) {
+  const webrtc::PacketTime webrtc_packet_time(packet_time.timestamp,
+                                              packet_time.not_before);
+  const webrtc::PacketReceiver::DeliveryStatus delivery_result =
+      call_->Receiver()->DeliverPacket(
+          webrtc::MediaType::VIDEO,
+          packet->cdata(), packet->size(),
+          webrtc_packet_time);
+  switch (delivery_result) {
+    case webrtc::PacketReceiver::DELIVERY_OK:
+      return;
+    case webrtc::PacketReceiver::DELIVERY_PACKET_ERROR:
+      return;
+    case webrtc::PacketReceiver::DELIVERY_UNKNOWN_SSRC:
+      break;
+  }
+
+  uint32_t ssrc = 0;
+  if (!GetRtpSsrc(packet->cdata(), packet->size(), &ssrc)) {
+    return;
+  }
+
+  int payload_type = 0;
+  if (!GetRtpPayloadType(packet->cdata(), packet->size(), &payload_type)) {
+    return;
+  }
+
+  // See if this payload_type is registered as one that usually gets its own
+  // SSRC (RTX) or at least is safe to drop either way (FEC). If it is, and
+  // it wasn't handled above by DeliverPacket, that means we don't know what
+  // stream it associates with, and we shouldn't ever create an implicit channel
+  // for these.
+  for (auto& codec : recv_codecs_) {
+    if (payload_type == codec.rtx_payload_type ||
+        payload_type == codec.ulpfec.red_rtx_payload_type ||
+        payload_type == codec.ulpfec.ulpfec_payload_type ||
+        payload_type == codec.flexfec_payload_type) {
+      return;
+    }
+  }
+
+  switch (unsignalled_ssrc_handler_->OnUnsignalledSsrc(this, ssrc)) {
+    case UnsignalledSsrcHandler::kDropPacket:
+      return;
+    case UnsignalledSsrcHandler::kDeliverPacket:
+      break;
+  }
+
+  if (call_->Receiver()->DeliverPacket(
+          webrtc::MediaType::VIDEO,
+          packet->cdata(), packet->size(),
+          webrtc_packet_time) != webrtc::PacketReceiver::DELIVERY_OK) {
+    LOG(LS_WARNING) << "Failed to deliver RTP packet on re-delivery.";
+    return;
+  }
+}
+
+void WebRtcVideoChannel2::OnRtcpReceived(
+    rtc::CopyOnWriteBuffer* packet,
+    const rtc::PacketTime& packet_time) {
+  const webrtc::PacketTime webrtc_packet_time(packet_time.timestamp,
+                                              packet_time.not_before);
+  // TODO(pbos): Check webrtc::PacketReceiver::DELIVERY_OK once we deliver
+  // for both audio and video on the same path. Since BundleFilter doesn't
+  // filter RTCP anymore incoming RTCP packets could've been going to audio (so
+  // logging failures spam the log).
+  call_->Receiver()->DeliverPacket(
+      webrtc::MediaType::VIDEO,
+      packet->cdata(), packet->size(),
+      webrtc_packet_time);
+}
+
+void WebRtcVideoChannel2::OnReadyToSend(bool ready) {
+  LOG(LS_VERBOSE) << "OnReadyToSend: " << (ready ? "Ready." : "Not ready.");
+  call_->SignalChannelNetworkState(
+      webrtc::MediaType::VIDEO,
+      ready ? webrtc::kNetworkUp : webrtc::kNetworkDown);
+}
+
+void WebRtcVideoChannel2::OnNetworkRouteChanged(
+    const std::string& transport_name,
+    const rtc::NetworkRoute& network_route) {
+  call_->OnNetworkRouteChanged(transport_name, network_route);
+}
+
+void WebRtcVideoChannel2::OnTransportOverheadChanged(
+    int transport_overhead_per_packet) {
+  call_->OnTransportOverheadChanged(webrtc::MediaType::VIDEO,
+                                    transport_overhead_per_packet);
+}
+
+void WebRtcVideoChannel2::SetInterface(NetworkInterface* iface) {
+  MediaChannel::SetInterface(iface);
+  // Set the RTP recv/send buffer to a bigger size
+  MediaChannel::SetOption(NetworkInterface::ST_RTP,
+                          rtc::Socket::OPT_RCVBUF,
+                          kVideoRtpBufferSize);
+
+  // Speculative change to increase the outbound socket buffer size.
+  // In b/15152257, we are seeing a significant number of packets discarded
+  // due to lack of socket buffer space, although it's not yet clear what the
+  // ideal value should be.
+  MediaChannel::SetOption(NetworkInterface::ST_RTP,
+                          rtc::Socket::OPT_SNDBUF,
+                          kVideoRtpBufferSize);
+}
+
+bool WebRtcVideoChannel2::SendRtp(const uint8_t* data,
+                                  size_t len,
+                                  const webrtc::PacketOptions& options) {
+  rtc::CopyOnWriteBuffer packet(data, len, kMaxRtpPacketLen);
+  rtc::PacketOptions rtc_options;
+  rtc_options.packet_id = options.packet_id;
+  return MediaChannel::SendPacket(&packet, rtc_options);
+}
+
+bool WebRtcVideoChannel2::SendRtcp(const uint8_t* data, size_t len) {
+  rtc::CopyOnWriteBuffer packet(data, len, kMaxRtpPacketLen);
+  return MediaChannel::SendRtcp(&packet, rtc::PacketOptions());
+}
+
+WebRtcVideoChannel2::WebRtcVideoSendStream::VideoSendStreamParameters::
+    VideoSendStreamParameters(
+        webrtc::VideoSendStream::Config config,
+        const VideoOptions& options,
+        int max_bitrate_bps,
+        const rtc::Optional<VideoCodecSettings>& codec_settings)
+    : config(std::move(config)),
+      options(options),
+      max_bitrate_bps(max_bitrate_bps),
+      conference_mode(false),
+      codec_settings(codec_settings) {}
+
+WebRtcVideoChannel2::WebRtcVideoSendStream::AllocatedEncoder::AllocatedEncoder(
+    webrtc::VideoEncoder* encoder,
+    const cricket::VideoCodec& codec,
+    bool external)
+    : encoder(encoder),
+      external_encoder(nullptr),
+      codec(codec),
+      external(external) {
+  if (external) {
+    external_encoder = encoder;
+    this->encoder =
+        new webrtc::VideoEncoderSoftwareFallbackWrapper(codec, encoder);
+  }
+}
+
+WebRtcVideoChannel2::WebRtcVideoSendStream::WebRtcVideoSendStream(
+    webrtc::Call* call,
+    const StreamParams& sp,
+    webrtc::VideoSendStream::Config config,
+    const VideoOptions& options,
+    WebRtcVideoEncoderFactory* external_encoder_factory,
+    bool enable_cpu_overuse_detection,
+    int max_bitrate_bps,
+    const rtc::Optional<VideoCodecSettings>& codec_settings,
+    const rtc::Optional<std::vector<webrtc::RtpExtension>>& rtp_extensions,
+    // TODO(deadbeef): Don't duplicate information between send_params,
+    // rtp_extensions, options, etc.
+    const VideoSendParameters& send_params)
+    : worker_thread_(rtc::Thread::Current()),
+      ssrcs_(sp.ssrcs),
+      ssrc_groups_(sp.ssrc_groups),
+      call_(call),
+      enable_cpu_overuse_detection_(enable_cpu_overuse_detection),
+      source_(nullptr),
+      external_encoder_factory_(external_encoder_factory),
+      internal_encoder_factory_(new InternalEncoderFactory()),
+      stream_(nullptr),
+      encoder_sink_(nullptr),
+      parameters_(std::move(config), options, max_bitrate_bps, codec_settings),
+      rtp_parameters_(CreateRtpParametersWithOneEncoding()),
+      allocated_encoder_(nullptr, cricket::VideoCodec(), false),
+      sending_(false) {
+  parameters_.config.rtp.max_packet_size = kVideoMtu;
+  parameters_.conference_mode = send_params.conference_mode;
+
+  sp.GetPrimarySsrcs(&parameters_.config.rtp.ssrcs);
+
+  // ValidateStreamParams should prevent this from happening.
+  RTC_CHECK(!parameters_.config.rtp.ssrcs.empty());
+  rtp_parameters_.encodings[0].ssrc =
+      rtc::Optional<uint32_t>(parameters_.config.rtp.ssrcs[0]);
+
+  // RTX.
+  sp.GetFidSsrcs(parameters_.config.rtp.ssrcs,
+                 &parameters_.config.rtp.rtx.ssrcs);
+
+  // FlexFEC SSRCs.
+  // TODO(brandtr): This code needs to be generalized when we add support for
+  // multistream protection.
+  if (IsFlexfecFieldTrialEnabled()) {
+    uint32_t flexfec_ssrc;
+    bool flexfec_enabled = false;
+    for (uint32_t primary_ssrc : parameters_.config.rtp.ssrcs) {
+      if (sp.GetFecFrSsrc(primary_ssrc, &flexfec_ssrc)) {
+        if (flexfec_enabled) {
+          LOG(LS_INFO) << "Multiple FlexFEC streams proposed by remote, but "
+                          "our implementation only supports a single FlexFEC "
+                          "stream. Will not enable FlexFEC for proposed "
+                          "stream with SSRC: "
+                       << flexfec_ssrc << ".";
+          continue;
+        }
+
+        flexfec_enabled = true;
+        parameters_.config.rtp.flexfec.ssrc = flexfec_ssrc;
+        parameters_.config.rtp.flexfec.protected_media_ssrcs = {primary_ssrc};
+      }
+    }
+  }
+
+  parameters_.config.rtp.c_name = sp.cname;
+  if (rtp_extensions) {
+    parameters_.config.rtp.extensions = *rtp_extensions;
+  }
+  parameters_.config.rtp.rtcp_mode = send_params.rtcp.reduced_size
+                                         ? webrtc::RtcpMode::kReducedSize
+                                         : webrtc::RtcpMode::kCompound;
+  if (codec_settings) {
+    bool force_encoder_allocation = false;
+    SetCodec(*codec_settings, force_encoder_allocation);
+  }
+}
+
+WebRtcVideoChannel2::WebRtcVideoSendStream::~WebRtcVideoSendStream() {
+  if (stream_ != NULL) {
+    call_->DestroyVideoSendStream(stream_);
+  }
+  DestroyVideoEncoder(&allocated_encoder_);
+}
+
+bool WebRtcVideoChannel2::WebRtcVideoSendStream::SetVideoSend(
+    bool enable,
+    const VideoOptions* options,
+    rtc::VideoSourceInterface<webrtc::VideoFrame>* source) {
+  TRACE_EVENT0("webrtc", "WebRtcVideoSendStream::SetVideoSend");
+  RTC_DCHECK_RUN_ON(&thread_checker_);
+
+  // Ignore |options| pointer if |enable| is false.
+  bool options_present = enable && options;
+
+  if (options_present) {
+    VideoOptions old_options = parameters_.options;
+    parameters_.options.SetAll(*options);
+    if (parameters_.options.is_screencast.value_or(false) !=
+            old_options.is_screencast.value_or(false) &&
+        parameters_.codec_settings) {
+      // If screen content settings change, we may need to recreate the codec
+      // instance so that the correct type is used.
+
+      bool force_encoder_allocation = true;
+      SetCodec(*parameters_.codec_settings, force_encoder_allocation);
+      // Mark screenshare parameter as being updated, then test for any other
+      // changes that may require codec reconfiguration.
+      old_options.is_screencast = options->is_screencast;
+    }
+    if (parameters_.options != old_options) {
+      ReconfigureEncoder();
+    }
+  }
+
+  if (source_ && stream_) {
+    stream_->SetSource(
+        nullptr, webrtc::VideoSendStream::DegradationPreference::kBalanced);
+  }
+  // Switch to the new source.
+  source_ = source;
+  if (source && stream_) {
+    // Do not adapt resolution for screen content as this will likely
+    // result in blurry and unreadable text.
+    // |this| acts like a VideoSource to make sure SinkWants are handled on the
+    // correct thread.
+    stream_->SetSource(
+        this, enable_cpu_overuse_detection_ &&
+                      !parameters_.options.is_screencast.value_or(false)
+                  ? webrtc::VideoSendStream::DegradationPreference::kBalanced
+                  : webrtc::VideoSendStream::DegradationPreference::
+                        kMaintainResolution);
+  }
+  return true;
+}
+
+const std::vector<uint32_t>&
+WebRtcVideoChannel2::WebRtcVideoSendStream::GetSsrcs() const {
+  return ssrcs_;
+}
+
+WebRtcVideoChannel2::WebRtcVideoSendStream::AllocatedEncoder
+WebRtcVideoChannel2::WebRtcVideoSendStream::CreateVideoEncoder(
+    const VideoCodec& codec,
+    bool force_encoder_allocation) {
+  RTC_DCHECK_RUN_ON(&thread_checker_);
+  // Do not re-create encoders of the same type.
+  if (!force_encoder_allocation && codec == allocated_encoder_.codec &&
+      allocated_encoder_.encoder != nullptr) {
+    return allocated_encoder_;
+  }
+
+  // Try creating external encoder.
+  if (external_encoder_factory_ != nullptr &&
+      FindMatchingCodec(external_encoder_factory_->supported_codecs(), codec)) {
+    webrtc::VideoEncoder* encoder =
+        external_encoder_factory_->CreateVideoEncoder(codec);
+    if (encoder != nullptr)
+      return AllocatedEncoder(encoder, codec, true /* is_external */);
+  }
+
+  // Try creating internal encoder.
+  if (FindMatchingCodec(internal_encoder_factory_->supported_codecs(), codec)) {
+    if (parameters_.encoder_config.content_type ==
+            webrtc::VideoEncoderConfig::ContentType::kScreen &&
+        parameters_.conference_mode && UseSimulcastScreenshare()) {
+      // TODO(sprang): Remove this adapter once libvpx supports simulcast with
+      // same-resolution substreams.
+      WebRtcSimulcastEncoderFactory adapter_factory(
+          internal_encoder_factory_.get());
+      return AllocatedEncoder(adapter_factory.CreateVideoEncoder(codec), codec,
+                              false /* is_external */);
+    }
+    return AllocatedEncoder(
+        internal_encoder_factory_->CreateVideoEncoder(codec), codec,
+        false /* is_external */);
+  }
+
+  // This shouldn't happen, we should not be trying to create something we don't
+  // support.
+  RTC_NOTREACHED();
+  return AllocatedEncoder(NULL, cricket::VideoCodec(), false);
+}
+
+void WebRtcVideoChannel2::WebRtcVideoSendStream::DestroyVideoEncoder(
+    AllocatedEncoder* encoder) {
+  RTC_DCHECK_RUN_ON(&thread_checker_);
+  if (encoder->external) {
+    external_encoder_factory_->DestroyVideoEncoder(encoder->external_encoder);
+  }
+  delete encoder->encoder;
+}
+
+void WebRtcVideoChannel2::WebRtcVideoSendStream::SetCodec(
+    const VideoCodecSettings& codec_settings,
+    bool force_encoder_allocation) {
+  RTC_DCHECK_RUN_ON(&thread_checker_);
+  parameters_.encoder_config = CreateVideoEncoderConfig(codec_settings.codec);
+  RTC_DCHECK_GT(parameters_.encoder_config.number_of_streams, 0);
+
+  AllocatedEncoder new_encoder =
+      CreateVideoEncoder(codec_settings.codec, force_encoder_allocation);
+  parameters_.config.encoder_settings.encoder = new_encoder.encoder;
+  parameters_.config.encoder_settings.full_overuse_time = new_encoder.external;
+  parameters_.config.encoder_settings.payload_name = codec_settings.codec.name;
+  parameters_.config.encoder_settings.payload_type = codec_settings.codec.id;
+  if (new_encoder.external) {
+    webrtc::VideoCodecType type =
+        webrtc::PayloadNameToCodecType(codec_settings.codec.name)
+            .value_or(webrtc::kVideoCodecUnknown);
+    parameters_.config.encoder_settings.internal_source =
+        external_encoder_factory_->EncoderTypeHasInternalSource(type);
+  } else {
+    parameters_.config.encoder_settings.internal_source = false;
+  }
+  parameters_.config.rtp.ulpfec = codec_settings.ulpfec;
+  if (IsFlexfecFieldTrialEnabled()) {
+    parameters_.config.rtp.flexfec.payload_type =
+        codec_settings.flexfec_payload_type;
+  }
+
+  // Set RTX payload type if RTX is enabled.
+  if (!parameters_.config.rtp.rtx.ssrcs.empty()) {
+    if (codec_settings.rtx_payload_type == -1) {
+      LOG(LS_WARNING) << "RTX SSRCs configured but there's no configured RTX "
+                         "payload type. Ignoring.";
+      parameters_.config.rtp.rtx.ssrcs.clear();
+    } else {
+      parameters_.config.rtp.rtx.payload_type = codec_settings.rtx_payload_type;
+    }
+  }
+
+  parameters_.config.rtp.nack.rtp_history_ms =
+      HasNack(codec_settings.codec) ? kNackHistoryMs : 0;
+
+  parameters_.codec_settings =
+      rtc::Optional<WebRtcVideoChannel2::VideoCodecSettings>(codec_settings);
+
+  LOG(LS_INFO) << "RecreateWebRtcStream (send) because of SetCodec.";
+  RecreateWebRtcStream();
+  if (allocated_encoder_.encoder != new_encoder.encoder) {
+    DestroyVideoEncoder(&allocated_encoder_);
+    allocated_encoder_ = new_encoder;
+  }
+}
+
+void WebRtcVideoChannel2::WebRtcVideoSendStream::SetSendParameters(
+    const ChangedSendParameters& params) {
+  RTC_DCHECK_RUN_ON(&thread_checker_);
+  // |recreate_stream| means construction-time parameters have changed and the
+  // sending stream needs to be reset with the new config.
+  bool recreate_stream = false;
+  if (params.rtcp_mode) {
+    parameters_.config.rtp.rtcp_mode = *params.rtcp_mode;
+    recreate_stream = true;
+  }
+  if (params.rtp_header_extensions) {
+    parameters_.config.rtp.extensions = *params.rtp_header_extensions;
+    recreate_stream = true;
+  }
+  if (params.max_bandwidth_bps) {
+    parameters_.max_bitrate_bps = *params.max_bandwidth_bps;
+    ReconfigureEncoder();
+  }
+  if (params.conference_mode) {
+    parameters_.conference_mode = *params.conference_mode;
+  }
+
+  // Set codecs and options.
+  if (params.codec) {
+    bool force_encoder_allocation = false;
+    SetCodec(*params.codec, force_encoder_allocation);
+    recreate_stream = false;  // SetCodec has already recreated the stream.
+  } else if (params.conference_mode && parameters_.codec_settings) {
+    bool force_encoder_allocation = false;
+    SetCodec(*parameters_.codec_settings, force_encoder_allocation);
+    recreate_stream = false;  // SetCodec has already recreated the stream.
+  }
+  if (recreate_stream) {
+    LOG(LS_INFO) << "RecreateWebRtcStream (send) because of SetSendParameters";
+    RecreateWebRtcStream();
+  }
+}
+
+bool WebRtcVideoChannel2::WebRtcVideoSendStream::SetRtpParameters(
+    const webrtc::RtpParameters& new_parameters) {
+  RTC_DCHECK_RUN_ON(&thread_checker_);
+  if (!ValidateRtpParameters(new_parameters)) {
+    return false;
+  }
+
+  bool reconfigure_encoder = new_parameters.encodings[0].max_bitrate_bps !=
+                             rtp_parameters_.encodings[0].max_bitrate_bps;
+  rtp_parameters_ = new_parameters;
+  // Codecs are currently handled at the WebRtcVideoChannel2 level.
+  rtp_parameters_.codecs.clear();
+  if (reconfigure_encoder) {
+    ReconfigureEncoder();
+  }
+  // Encoding may have been activated/deactivated.
+  UpdateSendState();
+  return true;
+}
+
+webrtc::RtpParameters
+WebRtcVideoChannel2::WebRtcVideoSendStream::GetRtpParameters() const {
+  RTC_DCHECK_RUN_ON(&thread_checker_);
+  return rtp_parameters_;
+}
+
+bool WebRtcVideoChannel2::WebRtcVideoSendStream::ValidateRtpParameters(
+    const webrtc::RtpParameters& rtp_parameters) {
+  RTC_DCHECK_RUN_ON(&thread_checker_);
+  if (rtp_parameters.encodings.size() != 1) {
+    LOG(LS_ERROR)
+        << "Attempted to set RtpParameters without exactly one encoding";
+    return false;
+  }
+  if (rtp_parameters.encodings[0].ssrc != rtp_parameters_.encodings[0].ssrc) {
+    LOG(LS_ERROR) << "Attempted to set RtpParameters with modified SSRC";
+    return false;
+  }
+  return true;
+}
+
+void WebRtcVideoChannel2::WebRtcVideoSendStream::UpdateSendState() {
+  RTC_DCHECK_RUN_ON(&thread_checker_);
+  // TODO(deadbeef): Need to handle more than one encoding in the future.
+  RTC_DCHECK(rtp_parameters_.encodings.size() == 1u);
+  if (sending_ && rtp_parameters_.encodings[0].active) {
+    RTC_DCHECK(stream_ != nullptr);
+    stream_->Start();
+  } else {
+    if (stream_ != nullptr) {
+      stream_->Stop();
+    }
+  }
+}
+
+webrtc::VideoEncoderConfig
+WebRtcVideoChannel2::WebRtcVideoSendStream::CreateVideoEncoderConfig(
+    const VideoCodec& codec) const {
+  RTC_DCHECK_RUN_ON(&thread_checker_);
+  webrtc::VideoEncoderConfig encoder_config;
+  bool is_screencast = parameters_.options.is_screencast.value_or(false);
+  if (is_screencast) {
+    encoder_config.min_transmit_bitrate_bps =
+        1000 * parameters_.options.screencast_min_bitrate_kbps.value_or(0);
+    encoder_config.content_type =
+        webrtc::VideoEncoderConfig::ContentType::kScreen;
+  } else {
+    encoder_config.min_transmit_bitrate_bps = 0;
+    encoder_config.content_type =
+        webrtc::VideoEncoderConfig::ContentType::kRealtimeVideo;
+  }
+
+  // By default, the stream count for the codec configuration should match the
+  // number of negotiated ssrcs. But if the codec is blacklisted for simulcast
+  // or a screencast (and not in simulcast screenshare experiment), only
+  // configure a single stream.
+  encoder_config.number_of_streams = parameters_.config.rtp.ssrcs.size();
+  if (IsCodecBlacklistedForSimulcast(codec.name) ||
+      (is_screencast &&
+       (!UseSimulcastScreenshare() || !parameters_.conference_mode))) {
+    encoder_config.number_of_streams = 1;
+  }
+
+  int stream_max_bitrate = parameters_.max_bitrate_bps;
+  if (rtp_parameters_.encodings[0].max_bitrate_bps) {
+    stream_max_bitrate =
+        MinPositive(*(rtp_parameters_.encodings[0].max_bitrate_bps),
+                    parameters_.max_bitrate_bps);
+  }
+
+  int codec_max_bitrate_kbps;
+  if (codec.GetParam(kCodecParamMaxBitrate, &codec_max_bitrate_kbps)) {
+    stream_max_bitrate = codec_max_bitrate_kbps * 1000;
+  }
+  encoder_config.max_bitrate_bps = stream_max_bitrate;
+
+  int max_qp = kDefaultQpMax;
+  codec.GetParam(kCodecParamMaxQuantization, &max_qp);
+  encoder_config.video_stream_factory =
+      new rtc::RefCountedObject<EncoderStreamFactory>(
+          codec.name, max_qp, kDefaultVideoMaxFramerate, is_screencast,
+          parameters_.conference_mode);
+  return encoder_config;
+}
+
+void WebRtcVideoChannel2::WebRtcVideoSendStream::ReconfigureEncoder() {
+  RTC_DCHECK_RUN_ON(&thread_checker_);
+  if (!stream_) {
+    // The webrtc::VideoSendStream |stream_| has not yet been created but other
+    // parameters has changed.
+    return;
+  }
+
+  RTC_DCHECK_GT(parameters_.encoder_config.number_of_streams, 0);
+
+  RTC_CHECK(parameters_.codec_settings);
+  VideoCodecSettings codec_settings = *parameters_.codec_settings;
+
+  webrtc::VideoEncoderConfig encoder_config =
+      CreateVideoEncoderConfig(codec_settings.codec);
+
+  encoder_config.encoder_specific_settings = ConfigureVideoEncoderSettings(
+      codec_settings.codec);
+
+  stream_->ReconfigureVideoEncoder(encoder_config.Copy());
+
+  encoder_config.encoder_specific_settings = NULL;
+
+  parameters_.encoder_config = std::move(encoder_config);
+}
+
+void WebRtcVideoChannel2::WebRtcVideoSendStream::SetSend(bool send) {
+  RTC_DCHECK_RUN_ON(&thread_checker_);
+  sending_ = send;
+  UpdateSendState();
+}
+
+void WebRtcVideoChannel2::WebRtcVideoSendStream::RemoveSink(
+    rtc::VideoSinkInterface<webrtc::VideoFrame>* sink) {
+  RTC_DCHECK_RUN_ON(&thread_checker_);
+  RTC_DCHECK(encoder_sink_ == sink);
+  encoder_sink_ = nullptr;
+  source_->RemoveSink(sink);
+}
+
+void WebRtcVideoChannel2::WebRtcVideoSendStream::AddOrUpdateSink(
+    rtc::VideoSinkInterface<webrtc::VideoFrame>* sink,
+    const rtc::VideoSinkWants& wants) {
+  if (worker_thread_ == rtc::Thread::Current()) {
+    // AddOrUpdateSink is called on |worker_thread_| if this is the first
+    // registration of |sink|.
+    RTC_DCHECK_RUN_ON(&thread_checker_);
+    encoder_sink_ = sink;
+    source_->AddOrUpdateSink(encoder_sink_, wants);
+  } else {
+    // Subsequent calls to AddOrUpdateSink will happen on the encoder task
+    // queue.
+    invoker_.AsyncInvoke<void>(
+        RTC_FROM_HERE, worker_thread_, [this, sink, wants] {
+          RTC_DCHECK_RUN_ON(&thread_checker_);
+          // |sink| may be invalidated after this task was posted since
+          // RemoveSink is called on the worker thread.
+          bool encoder_sink_valid = (sink == encoder_sink_);
+          if (source_ && encoder_sink_valid) {
+            source_->AddOrUpdateSink(encoder_sink_, wants);
+          }
+        });
+  }
+}
+
+VideoSenderInfo WebRtcVideoChannel2::WebRtcVideoSendStream::GetVideoSenderInfo(
+    bool log_stats) {
+  VideoSenderInfo info;
+  RTC_DCHECK_RUN_ON(&thread_checker_);
+  for (uint32_t ssrc : parameters_.config.rtp.ssrcs)
+    info.add_ssrc(ssrc);
+
+  if (parameters_.codec_settings) {
+    info.codec_name = parameters_.codec_settings->codec.name;
+    info.codec_payload_type = rtc::Optional<int>(
+        parameters_.codec_settings->codec.id);
+  }
+
+  if (stream_ == NULL)
+    return info;
+
+  webrtc::VideoSendStream::Stats stats = stream_->GetStats();
+
+  if (log_stats)
+    LOG(LS_INFO) << stats.ToString(rtc::TimeMillis());
+
+  info.adapt_changes = stats.number_of_cpu_adapt_changes;
+  info.adapt_reason =
+      stats.cpu_limited_resolution ? ADAPTREASON_CPU : ADAPTREASON_NONE;
+
+  // Get bandwidth limitation info from stream_->GetStats().
+  // Input resolution (output from video_adapter) can be further scaled down or
+  // higher video layer(s) can be dropped due to bitrate constraints.
+  // Note, adapt_changes only include changes from the video_adapter.
+  if (stats.bw_limited_resolution)
+    info.adapt_reason |= ADAPTREASON_BANDWIDTH;
+
+  info.encoder_implementation_name = stats.encoder_implementation_name;
+  info.ssrc_groups = ssrc_groups_;
+  info.framerate_input = stats.input_frame_rate;
+  info.framerate_sent = stats.encode_frame_rate;
+  info.avg_encode_ms = stats.avg_encode_time_ms;
+  info.encode_usage_percent = stats.encode_usage_percent;
+  info.frames_encoded = stats.frames_encoded;
+  info.qp_sum = stats.qp_sum;
+
+  info.nominal_bitrate = stats.media_bitrate_bps;
+  info.preferred_bitrate = stats.preferred_media_bitrate_bps;
+
+  info.send_frame_width = 0;
+  info.send_frame_height = 0;
+  for (std::map<uint32_t, webrtc::VideoSendStream::StreamStats>::iterator it =
+           stats.substreams.begin();
+       it != stats.substreams.end(); ++it) {
+    // TODO(pbos): Wire up additional stats, such as padding bytes.
+    webrtc::VideoSendStream::StreamStats stream_stats = it->second;
+    info.bytes_sent += stream_stats.rtp_stats.transmitted.payload_bytes +
+                       stream_stats.rtp_stats.transmitted.header_bytes +
+                       stream_stats.rtp_stats.transmitted.padding_bytes;
+    info.packets_sent += stream_stats.rtp_stats.transmitted.packets;
+    info.packets_lost += stream_stats.rtcp_stats.cumulative_lost;
+    if (stream_stats.width > info.send_frame_width)
+      info.send_frame_width = stream_stats.width;
+    if (stream_stats.height > info.send_frame_height)
+      info.send_frame_height = stream_stats.height;
+    info.firs_rcvd += stream_stats.rtcp_packet_type_counts.fir_packets;
+    info.nacks_rcvd += stream_stats.rtcp_packet_type_counts.nack_packets;
+    info.plis_rcvd += stream_stats.rtcp_packet_type_counts.pli_packets;
+  }
+
+  if (!stats.substreams.empty()) {
+    // TODO(pbos): Report fraction lost per SSRC.
+    webrtc::VideoSendStream::StreamStats first_stream_stats =
+        stats.substreams.begin()->second;
+    info.fraction_lost =
+        static_cast<float>(first_stream_stats.rtcp_stats.fraction_lost) /
+        (1 << 8);
+  }
+
+  return info;
+}
+
+void WebRtcVideoChannel2::WebRtcVideoSendStream::FillBandwidthEstimationInfo(
+    BandwidthEstimationInfo* bwe_info) {
+  RTC_DCHECK_RUN_ON(&thread_checker_);
+  if (stream_ == NULL) {
+    return;
+  }
+  webrtc::VideoSendStream::Stats stats = stream_->GetStats();
+  for (std::map<uint32_t, webrtc::VideoSendStream::StreamStats>::iterator it =
+           stats.substreams.begin();
+       it != stats.substreams.end(); ++it) {
+    bwe_info->transmit_bitrate += it->second.total_bitrate_bps;
+    bwe_info->retransmit_bitrate += it->second.retransmit_bitrate_bps;
+  }
+  bwe_info->target_enc_bitrate += stats.target_media_bitrate_bps;
+  bwe_info->actual_enc_bitrate += stats.media_bitrate_bps;
+}
+
+void WebRtcVideoChannel2::WebRtcVideoSendStream::RecreateWebRtcStream() {
+  RTC_DCHECK_RUN_ON(&thread_checker_);
+  if (stream_ != NULL) {
+    call_->DestroyVideoSendStream(stream_);
+  }
+
+  RTC_CHECK(parameters_.codec_settings);
+  RTC_DCHECK_EQ((parameters_.encoder_config.content_type ==
+                 webrtc::VideoEncoderConfig::ContentType::kScreen),
+                parameters_.options.is_screencast.value_or(false))
+      << "encoder content type inconsistent with screencast option";
+  parameters_.encoder_config.encoder_specific_settings =
+      ConfigureVideoEncoderSettings(parameters_.codec_settings->codec);
+
+  webrtc::VideoSendStream::Config config = parameters_.config.Copy();
+  if (!config.rtp.rtx.ssrcs.empty() && config.rtp.rtx.payload_type == -1) {
+    LOG(LS_WARNING) << "RTX SSRCs configured but there's no configured RTX "
+                       "payload type the set codec. Ignoring RTX.";
+    config.rtp.rtx.ssrcs.clear();
+  }
+  stream_ = call_->CreateVideoSendStream(std::move(config),
+                                         parameters_.encoder_config.Copy());
+
+  parameters_.encoder_config.encoder_specific_settings = NULL;
+
+  if (source_) {
+    // Do not adapt resolution for screen content as this will likely result in
+    // blurry and unreadable text.
+    // |this| acts like a VideoSource to make sure SinkWants are handled on the
+    // correct thread.
+    stream_->SetSource(
+        this, enable_cpu_overuse_detection_ &&
+                      !parameters_.options.is_screencast.value_or(false)
+                  ? webrtc::VideoSendStream::DegradationPreference::kBalanced
+                  : webrtc::VideoSendStream::DegradationPreference::
+                        kMaintainResolution);
+  }
+
+  // Call stream_->Start() if necessary conditions are met.
+  UpdateSendState();
+}
+
+WebRtcVideoChannel2::WebRtcVideoReceiveStream::WebRtcVideoReceiveStream(
+    webrtc::Call* call,
+    const StreamParams& sp,
+    webrtc::VideoReceiveStream::Config config,
+    WebRtcVideoDecoderFactory* external_decoder_factory,
+    bool default_stream,
+    const std::vector<VideoCodecSettings>& recv_codecs,
+    const webrtc::FlexfecReceiveStream::Config& flexfec_config)
+    : call_(call),
+      stream_params_(sp),
+      stream_(NULL),
+      default_stream_(default_stream),
+      config_(std::move(config)),
+      flexfec_config_(flexfec_config),
+      flexfec_stream_(nullptr),
+      external_decoder_factory_(external_decoder_factory),
+      sink_(NULL),
+      first_frame_timestamp_(-1),
+      estimated_remote_start_ntp_time_ms_(0) {
+  config_.renderer = this;
+  std::vector<AllocatedDecoder> old_decoders;
+  ConfigureCodecs(recv_codecs, &old_decoders);
+  RecreateWebRtcStream();
+  RTC_DCHECK(old_decoders.empty());
+}
+
+WebRtcVideoChannel2::WebRtcVideoReceiveStream::AllocatedDecoder::
+    AllocatedDecoder(webrtc::VideoDecoder* decoder,
+                     webrtc::VideoCodecType type,
+                     bool external)
+    : decoder(decoder),
+      external_decoder(nullptr),
+      type(type),
+      external(external) {
+  if (external) {
+    external_decoder = decoder;
+    this->decoder =
+        new webrtc::VideoDecoderSoftwareFallbackWrapper(type, external_decoder);
+  }
+}
+
+WebRtcVideoChannel2::WebRtcVideoReceiveStream::~WebRtcVideoReceiveStream() {
+  if (flexfec_stream_) {
+    call_->DestroyFlexfecReceiveStream(flexfec_stream_);
+  }
+  call_->DestroyVideoReceiveStream(stream_);
+  ClearDecoders(&allocated_decoders_);
+}
+
+const std::vector<uint32_t>&
+WebRtcVideoChannel2::WebRtcVideoReceiveStream::GetSsrcs() const {
+  return stream_params_.ssrcs;
+}
+
+rtc::Optional<uint32_t>
+WebRtcVideoChannel2::WebRtcVideoReceiveStream::GetFirstPrimarySsrc() const {
+  std::vector<uint32_t> primary_ssrcs;
+  stream_params_.GetPrimarySsrcs(&primary_ssrcs);
+
+  if (primary_ssrcs.empty()) {
+    LOG(LS_WARNING) << "Empty primary ssrcs vector, returning empty optional";
+    return rtc::Optional<uint32_t>();
+  } else {
+    return rtc::Optional<uint32_t>(primary_ssrcs[0]);
+  }
+}
+
+WebRtcVideoChannel2::WebRtcVideoReceiveStream::AllocatedDecoder
+WebRtcVideoChannel2::WebRtcVideoReceiveStream::CreateOrReuseVideoDecoder(
+    std::vector<AllocatedDecoder>* old_decoders,
+    const VideoCodec& codec) {
+  webrtc::VideoCodecType type = webrtc::PayloadNameToCodecType(codec.name)
+                                    .value_or(webrtc::kVideoCodecUnknown);
+
+  for (size_t i = 0; i < old_decoders->size(); ++i) {
+    if ((*old_decoders)[i].type == type) {
+      AllocatedDecoder decoder = (*old_decoders)[i];
+      (*old_decoders)[i] = old_decoders->back();
+      old_decoders->pop_back();
+      return decoder;
+    }
+  }
+
+  if (external_decoder_factory_ != NULL) {
+    webrtc::VideoDecoder* decoder =
+        external_decoder_factory_->CreateVideoDecoderWithParams(
+            type, {stream_params_.id});
+    if (decoder != NULL) {
+      return AllocatedDecoder(decoder, type, true /* is_external */);
+    }
+  }
+
+  InternalDecoderFactory internal_decoder_factory;
+  return AllocatedDecoder(internal_decoder_factory.CreateVideoDecoderWithParams(
+                              type, {stream_params_.id}),
+                          type, false /* is_external */);
+}
+
+void WebRtcVideoChannel2::WebRtcVideoReceiveStream::ConfigureCodecs(
+    const std::vector<VideoCodecSettings>& recv_codecs,
+    std::vector<AllocatedDecoder>* old_decoders) {
+  *old_decoders = allocated_decoders_;
+  allocated_decoders_.clear();
+  config_.decoders.clear();
+  for (size_t i = 0; i < recv_codecs.size(); ++i) {
+    AllocatedDecoder allocated_decoder =
+        CreateOrReuseVideoDecoder(old_decoders, recv_codecs[i].codec);
+    allocated_decoders_.push_back(allocated_decoder);
+
+    webrtc::VideoReceiveStream::Decoder decoder;
+    decoder.decoder = allocated_decoder.decoder;
+    decoder.payload_type = recv_codecs[i].codec.id;
+    decoder.payload_name = recv_codecs[i].codec.name;
+    decoder.codec_params = recv_codecs[i].codec.params;
+    config_.decoders.push_back(decoder);
+  }
+
+  config_.rtp.rtx_payload_types.clear();
+  for (const VideoCodecSettings& recv_codec : recv_codecs) {
+    config_.rtp.rtx_payload_types[recv_codec.codec.id] =
+        recv_codec.rtx_payload_type;
+  }
+
+  config_.rtp.ulpfec = recv_codecs.front().ulpfec;
+  flexfec_config_.payload_type = recv_codecs.front().flexfec_payload_type;
+
+  config_.rtp.nack.rtp_history_ms =
+      HasNack(recv_codecs.begin()->codec) ? kNackHistoryMs : 0;
+}
+
+void WebRtcVideoChannel2::WebRtcVideoReceiveStream::SetLocalSsrc(
+    uint32_t local_ssrc) {
+  // TODO(pbos): Consider turning this sanity check into a RTC_DCHECK. You
+  // should not be able to create a sender with the same SSRC as a receiver, but
+  // right now this can't be done due to unittests depending on receiving what
+  // they are sending from the same MediaChannel.
+  if (local_ssrc == config_.rtp.remote_ssrc) {
+    LOG(LS_INFO) << "Ignoring call to SetLocalSsrc because parameters are "
+                    "unchanged; local_ssrc=" << local_ssrc;
+    return;
+  }
+
+  config_.rtp.local_ssrc = local_ssrc;
+  flexfec_config_.local_ssrc = local_ssrc;
+  LOG(LS_INFO)
+      << "RecreateWebRtcStream (recv) because of SetLocalSsrc; local_ssrc="
+      << local_ssrc;
+  RecreateWebRtcStream();
+}
+
+void WebRtcVideoChannel2::WebRtcVideoReceiveStream::SetFeedbackParameters(
+    bool nack_enabled,
+    bool remb_enabled,
+    bool transport_cc_enabled,
+    webrtc::RtcpMode rtcp_mode) {
+  int nack_history_ms = nack_enabled ? kNackHistoryMs : 0;
+  if (config_.rtp.nack.rtp_history_ms == nack_history_ms &&
+      config_.rtp.remb == remb_enabled &&
+      config_.rtp.transport_cc == transport_cc_enabled &&
+      config_.rtp.rtcp_mode == rtcp_mode) {
+    LOG(LS_INFO)
+        << "Ignoring call to SetFeedbackParameters because parameters are "
+           "unchanged; nack="
+        << nack_enabled << ", remb=" << remb_enabled
+        << ", transport_cc=" << transport_cc_enabled;
+    return;
+  }
+  config_.rtp.remb = remb_enabled;
+  config_.rtp.nack.rtp_history_ms = nack_history_ms;
+  config_.rtp.transport_cc = transport_cc_enabled;
+  config_.rtp.rtcp_mode = rtcp_mode;
+  // TODO(brandtr): We should be spec-compliant and set |transport_cc| here
+  // based on the rtcp-fb for the FlexFEC codec, not the media codec.
+  flexfec_config_.transport_cc = config_.rtp.transport_cc;
+  flexfec_config_.rtcp_mode = config_.rtp.rtcp_mode;
+  LOG(LS_INFO)
+      << "RecreateWebRtcStream (recv) because of SetFeedbackParameters; nack="
+      << nack_enabled << ", remb=" << remb_enabled
+      << ", transport_cc=" << transport_cc_enabled;
+  RecreateWebRtcStream();
+}
+
+void WebRtcVideoChannel2::WebRtcVideoReceiveStream::SetRecvParameters(
+    const ChangedRecvParameters& params) {
+  bool needs_recreation = false;
+  std::vector<AllocatedDecoder> old_decoders;
+  if (params.codec_settings) {
+    ConfigureCodecs(*params.codec_settings, &old_decoders);
+    needs_recreation = true;
+  }
+  if (params.rtp_header_extensions) {
+    config_.rtp.extensions = *params.rtp_header_extensions;
+    flexfec_config_.rtp_header_extensions = *params.rtp_header_extensions;
+    needs_recreation = true;
+  }
+  if (needs_recreation) {
+    LOG(LS_INFO) << "RecreateWebRtcStream (recv) because of SetRecvParameters";
+    RecreateWebRtcStream();
+    ClearDecoders(&old_decoders);
+  }
+}
+
+void WebRtcVideoChannel2::WebRtcVideoReceiveStream::RecreateWebRtcStream() {
+  if (stream_) {
+    call_->DestroyVideoReceiveStream(stream_);
+    stream_ = nullptr;
+  }
+  if (flexfec_stream_) {
+    call_->DestroyFlexfecReceiveStream(flexfec_stream_);
+    flexfec_stream_ = nullptr;
+  }
+  if (flexfec_config_.IsCompleteAndEnabled()) {
+    flexfec_stream_ = call_->CreateFlexfecReceiveStream(flexfec_config_);
+    flexfec_stream_->Start();
+  }
+  stream_ = call_->CreateVideoReceiveStream(config_.Copy());
+  stream_->Start();
+}
+
+void WebRtcVideoChannel2::WebRtcVideoReceiveStream::ClearDecoders(
+    std::vector<AllocatedDecoder>* allocated_decoders) {
+  for (size_t i = 0; i < allocated_decoders->size(); ++i) {
+    if ((*allocated_decoders)[i].external) {
+      external_decoder_factory_->DestroyVideoDecoder(
+          (*allocated_decoders)[i].external_decoder);
+    }
+    delete (*allocated_decoders)[i].decoder;
+  }
+  allocated_decoders->clear();
+}
+
+void WebRtcVideoChannel2::WebRtcVideoReceiveStream::OnFrame(
+    const webrtc::VideoFrame& frame) {
+  rtc::CritScope crit(&sink_lock_);
+
+  if (first_frame_timestamp_ < 0)
+    first_frame_timestamp_ = frame.timestamp();
+  int64_t rtp_time_elapsed_since_first_frame =
+      (timestamp_wraparound_handler_.Unwrap(frame.timestamp()) -
+       first_frame_timestamp_);
+  int64_t elapsed_time_ms = rtp_time_elapsed_since_first_frame /
+                            (cricket::kVideoCodecClockrate / 1000);
+  if (frame.ntp_time_ms() > 0)
+    estimated_remote_start_ntp_time_ms_ = frame.ntp_time_ms() - elapsed_time_ms;
+
+  if (sink_ == NULL) {
+    LOG(LS_WARNING) << "VideoReceiveStream not connected to a VideoSink.";
+    return;
+  }
+
+  sink_->OnFrame(frame);
+}
+
+bool WebRtcVideoChannel2::WebRtcVideoReceiveStream::IsDefaultStream() const {
+  return default_stream_;
+}
+
+void WebRtcVideoChannel2::WebRtcVideoReceiveStream::SetSink(
+    rtc::VideoSinkInterface<webrtc::VideoFrame>* sink) {
+  rtc::CritScope crit(&sink_lock_);
+  sink_ = sink;
+}
+
+std::string
+WebRtcVideoChannel2::WebRtcVideoReceiveStream::GetCodecNameFromPayloadType(
+    int payload_type) {
+  for (const webrtc::VideoReceiveStream::Decoder& decoder : config_.decoders) {
+    if (decoder.payload_type == payload_type) {
+      return decoder.payload_name;
+    }
+  }
+  return "";
+}
+
+VideoReceiverInfo
+WebRtcVideoChannel2::WebRtcVideoReceiveStream::GetVideoReceiverInfo(
+    bool log_stats) {
+  VideoReceiverInfo info;
+  info.ssrc_groups = stream_params_.ssrc_groups;
+  info.add_ssrc(config_.rtp.remote_ssrc);
+  webrtc::VideoReceiveStream::Stats stats = stream_->GetStats();
+  info.decoder_implementation_name = stats.decoder_implementation_name;
+  if (stats.current_payload_type != -1) {
+    info.codec_payload_type = rtc::Optional<int>(
+        stats.current_payload_type);
+  }
+  info.bytes_rcvd = stats.rtp_stats.transmitted.payload_bytes +
+                    stats.rtp_stats.transmitted.header_bytes +
+                    stats.rtp_stats.transmitted.padding_bytes;
+  info.packets_rcvd = stats.rtp_stats.transmitted.packets;
+  info.packets_lost = stats.rtcp_stats.cumulative_lost;
+  info.fraction_lost =
+      static_cast<float>(stats.rtcp_stats.fraction_lost) / (1 << 8);
+
+  info.framerate_rcvd = stats.network_frame_rate;
+  info.framerate_decoded = stats.decode_frame_rate;
+  info.framerate_output = stats.render_frame_rate;
+  info.frame_width = stats.width;
+  info.frame_height = stats.height;
+
+  {
+    rtc::CritScope frame_cs(&sink_lock_);
+    info.capture_start_ntp_time_ms = estimated_remote_start_ntp_time_ms_;
+  }
+
+  info.decode_ms = stats.decode_ms;
+  info.max_decode_ms = stats.max_decode_ms;
+  info.current_delay_ms = stats.current_delay_ms;
+  info.target_delay_ms = stats.target_delay_ms;
+  info.jitter_buffer_ms = stats.jitter_buffer_ms;
+  info.min_playout_delay_ms = stats.min_playout_delay_ms;
+  info.render_delay_ms = stats.render_delay_ms;
+  info.frames_received = stats.frame_counts.key_frames +
+                         stats.frame_counts.delta_frames;
+  info.frames_decoded = stats.frames_decoded;
+  info.frames_rendered = stats.frames_rendered;
+  info.qp_sum = stats.qp_sum;
+
+  info.codec_name = GetCodecNameFromPayloadType(stats.current_payload_type);
+
+  info.firs_sent = stats.rtcp_packet_type_counts.fir_packets;
+  info.plis_sent = stats.rtcp_packet_type_counts.pli_packets;
+  info.nacks_sent = stats.rtcp_packet_type_counts.nack_packets;
+
+  if (log_stats)
+    LOG(LS_INFO) << stats.ToString(rtc::TimeMillis());
+
+  return info;
+}
+
+WebRtcVideoChannel2::VideoCodecSettings::VideoCodecSettings()
+    : flexfec_payload_type(-1), rtx_payload_type(-1) {}
+
+bool WebRtcVideoChannel2::VideoCodecSettings::operator==(
+    const WebRtcVideoChannel2::VideoCodecSettings& other) const {
+  return codec == other.codec && ulpfec == other.ulpfec &&
+         flexfec_payload_type == other.flexfec_payload_type &&
+         rtx_payload_type == other.rtx_payload_type;
+}
+
+bool WebRtcVideoChannel2::VideoCodecSettings::operator!=(
+    const WebRtcVideoChannel2::VideoCodecSettings& other) const {
+  return !(*this == other);
+}
+
+std::vector<WebRtcVideoChannel2::VideoCodecSettings>
+WebRtcVideoChannel2::MapCodecs(const std::vector<VideoCodec>& codecs) {
+  RTC_DCHECK(!codecs.empty());
+
+  std::vector<VideoCodecSettings> video_codecs;
+  std::map<int, bool> payload_used;
+  std::map<int, VideoCodec::CodecType> payload_codec_type;
+  // |rtx_mapping| maps video payload type to rtx payload type.
+  std::map<int, int> rtx_mapping;
+
+  webrtc::UlpfecConfig ulpfec_config;
+  int flexfec_payload_type = -1;
+
+  for (size_t i = 0; i < codecs.size(); ++i) {
+    const VideoCodec& in_codec = codecs[i];
+    int payload_type = in_codec.id;
+
+    if (payload_used[payload_type]) {
+      LOG(LS_ERROR) << "Payload type already registered: "
+                    << in_codec.ToString();
+      return std::vector<VideoCodecSettings>();
+    }
+    payload_used[payload_type] = true;
+    payload_codec_type[payload_type] = in_codec.GetCodecType();
+
+    switch (in_codec.GetCodecType()) {
+      case VideoCodec::CODEC_RED: {
+        // RED payload type, should not have duplicates.
+        RTC_DCHECK_EQ(-1, ulpfec_config.red_payload_type);
+        ulpfec_config.red_payload_type = in_codec.id;
+        continue;
+      }
+
+      case VideoCodec::CODEC_ULPFEC: {
+        // ULPFEC payload type, should not have duplicates.
+        RTC_DCHECK_EQ(-1, ulpfec_config.ulpfec_payload_type);
+        ulpfec_config.ulpfec_payload_type = in_codec.id;
+        continue;
+      }
+
+      case VideoCodec::CODEC_FLEXFEC: {
+        // FlexFEC payload type, should not have duplicates.
+        RTC_DCHECK_EQ(-1, flexfec_payload_type);
+        flexfec_payload_type = in_codec.id;
+        continue;
+      }
+
+      case VideoCodec::CODEC_RTX: {
+        int associated_payload_type;
+        if (!in_codec.GetParam(kCodecParamAssociatedPayloadType,
+                               &associated_payload_type) ||
+            !IsValidRtpPayloadType(associated_payload_type)) {
+          LOG(LS_ERROR)
+              << "RTX codec with invalid or no associated payload type: "
+              << in_codec.ToString();
+          return std::vector<VideoCodecSettings>();
+        }
+        rtx_mapping[associated_payload_type] = in_codec.id;
+        continue;
+      }
+
+      case VideoCodec::CODEC_VIDEO:
+        break;
+    }
+
+    video_codecs.push_back(VideoCodecSettings());
+    video_codecs.back().codec = in_codec;
+  }
+
+  // One of these codecs should have been a video codec. Only having FEC
+  // parameters into this code is a logic error.
+  RTC_DCHECK(!video_codecs.empty());
+
+  for (std::map<int, int>::const_iterator it = rtx_mapping.begin();
+       it != rtx_mapping.end();
+       ++it) {
+    if (!payload_used[it->first]) {
+      LOG(LS_ERROR) << "RTX mapped to payload not in codec list.";
+      return std::vector<VideoCodecSettings>();
+    }
+    if (payload_codec_type[it->first] != VideoCodec::CODEC_VIDEO &&
+        payload_codec_type[it->first] != VideoCodec::CODEC_RED) {
+      LOG(LS_ERROR) << "RTX not mapped to regular video codec or RED codec.";
+      return std::vector<VideoCodecSettings>();
+    }
+
+    if (it->first == ulpfec_config.red_payload_type) {
+      ulpfec_config.red_rtx_payload_type = it->second;
+    }
+  }
+
+  for (size_t i = 0; i < video_codecs.size(); ++i) {
+    video_codecs[i].ulpfec = ulpfec_config;
+    video_codecs[i].flexfec_payload_type = flexfec_payload_type;
+    if (rtx_mapping[video_codecs[i].codec.id] != 0 &&
+        rtx_mapping[video_codecs[i].codec.id] !=
+            ulpfec_config.red_payload_type) {
+      video_codecs[i].rtx_payload_type = rtx_mapping[video_codecs[i].codec.id];
+    }
+  }
+
+  return video_codecs;
+}
+
+}  // namespace cricket
diff --git a/webrtc/modules/video_coding/BUILD.gn b/webrtc/modules/video_coding/BUILD.gn
index 643260a..cc193d9 100644
--- a/webrtc/modules/video_coding/BUILD.gn
+++ b/webrtc/modules/video_coding/BUILD.gn
@@ -171,6 +171,9 @@ rtc_static_library("webrtc_h264") {
       "codecs/h264/h264_decoder_impl.h",
       "codecs/h264/h264_encoder_impl.cc",
       "codecs/h264/h264_encoder_impl.h",
+      "codecs/h264/include/NvHWEncoder.h",
+      "codecs/h264/include/nvEncodeAPI.h",
+      "codecs/h264/NvHWEncoder.cc"
     ]
     deps += [
       "../../common_video",
@@ -271,7 +274,7 @@ rtc_static_library("webrtc_vp9") {
   }
 }
 
-if (rtc_include_tests) {
+if (true) {
   rtc_executable("video_quality_measurement") {
     testonly = true
 
diff --git a/webrtc/modules/video_coding/codecs/h264/NvHWEncoder.cc b/webrtc/modules/video_coding/codecs/h264/NvHWEncoder.cc
new file mode 100644
index 0000000..418548a
--- /dev/null
+++ b/webrtc/modules/video_coding/codecs/h264/NvHWEncoder.cc
@@ -0,0 +1,1591 @@
+/*
+ * Copyright 1993-2015 NVIDIA Corporation.  All rights reserved.
+ *
+ * Please refer to the NVIDIA end user license agreement (EULA) associated
+ * with this source code for terms and conditions that govern your use of
+ * this software. Any use, reproduction, disclosure, or distribution of
+ * this software and related documentation outside the terms of the EULA
+ * is strictly prohibited.
+ *
+ */
+
+#include "webrtc/modules/video_coding/codecs/h264/include/NvHWEncoder.h"
+
+NVENCSTATUS CNvHWEncoder::NvEncOpenEncodeSession(void* device, uint32_t deviceType)
+{
+    NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+
+    nvStatus = m_pEncodeAPI->nvEncOpenEncodeSession(device, deviceType, &m_hEncoder);
+    if (nvStatus != NV_ENC_SUCCESS)
+    {
+        assert(0);
+    }
+
+    return nvStatus;
+}
+
+NVENCSTATUS CNvHWEncoder::NvEncGetEncodeGUIDCount(uint32_t* encodeGUIDCount)
+{
+    NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+
+    nvStatus = m_pEncodeAPI->nvEncGetEncodeGUIDCount(m_hEncoder, encodeGUIDCount);
+    if (nvStatus != NV_ENC_SUCCESS)
+    {
+        assert(0);
+    }
+
+    return nvStatus;
+}
+
+NVENCSTATUS CNvHWEncoder::NvEncGetEncodeProfileGUIDCount(GUID encodeGUID, uint32_t* encodeProfileGUIDCount)
+{
+    NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+
+    nvStatus = m_pEncodeAPI->nvEncGetEncodeProfileGUIDCount(m_hEncoder, encodeGUID, encodeProfileGUIDCount);
+    if (nvStatus != NV_ENC_SUCCESS)
+    {
+        assert(0);
+    }
+
+    return nvStatus;
+}
+
+NVENCSTATUS CNvHWEncoder::NvEncGetEncodeProfileGUIDs(GUID encodeGUID, GUID* profileGUIDs, uint32_t guidArraySize, uint32_t* GUIDCount)
+{
+    NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+
+    nvStatus = m_pEncodeAPI->nvEncGetEncodeProfileGUIDs(m_hEncoder, encodeGUID, profileGUIDs, guidArraySize, GUIDCount);
+    if (nvStatus != NV_ENC_SUCCESS)
+    {
+        assert(0);
+    }
+
+    return nvStatus;
+}
+
+NVENCSTATUS CNvHWEncoder::NvEncGetEncodeGUIDs(GUID* GUIDs, uint32_t guidArraySize, uint32_t* GUIDCount)
+{
+    NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+
+    nvStatus = m_pEncodeAPI->nvEncGetEncodeGUIDs(m_hEncoder, GUIDs, guidArraySize, GUIDCount);
+    if (nvStatus != NV_ENC_SUCCESS)
+    {
+        assert(0);
+    }
+
+    return nvStatus;
+}
+
+NVENCSTATUS CNvHWEncoder::NvEncGetInputFormatCount(GUID encodeGUID, uint32_t* inputFmtCount)
+{
+    NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+
+    nvStatus = m_pEncodeAPI->nvEncGetInputFormatCount(m_hEncoder, encodeGUID, inputFmtCount);
+    if (nvStatus != NV_ENC_SUCCESS)
+    {
+        assert(0);
+    }
+
+    return nvStatus;
+}
+
+NVENCSTATUS CNvHWEncoder::NvEncGetInputFormats(GUID encodeGUID, NV_ENC_BUFFER_FORMAT* inputFmts, uint32_t inputFmtArraySize, uint32_t* inputFmtCount)
+{
+    NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+
+    nvStatus = m_pEncodeAPI->nvEncGetInputFormats(m_hEncoder, encodeGUID, inputFmts, inputFmtArraySize, inputFmtCount);
+    if (nvStatus != NV_ENC_SUCCESS)
+    {
+        assert(0);
+    }
+
+    return nvStatus;
+}
+
+NVENCSTATUS CNvHWEncoder::NvEncGetEncodeCaps(GUID encodeGUID, NV_ENC_CAPS_PARAM* capsParam, int* capsVal)
+{
+    NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+
+    nvStatus = m_pEncodeAPI->nvEncGetEncodeCaps(m_hEncoder, encodeGUID, capsParam, capsVal);
+    if (nvStatus != NV_ENC_SUCCESS)
+    {
+        assert(0);
+    }
+
+    return nvStatus;
+}
+
+NVENCSTATUS CNvHWEncoder::NvEncGetEncodePresetCount(GUID encodeGUID, uint32_t* encodePresetGUIDCount)
+{
+    NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+
+    nvStatus = m_pEncodeAPI->nvEncGetEncodePresetCount(m_hEncoder, encodeGUID, encodePresetGUIDCount);
+    if (nvStatus != NV_ENC_SUCCESS)
+    {
+        assert(0);
+    }
+
+    return nvStatus;
+}
+
+NVENCSTATUS CNvHWEncoder::NvEncGetEncodePresetGUIDs(GUID encodeGUID, GUID* presetGUIDs, uint32_t guidArraySize, uint32_t* encodePresetGUIDCount)
+{
+    NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+
+    nvStatus = m_pEncodeAPI->nvEncGetEncodePresetGUIDs(m_hEncoder, encodeGUID, presetGUIDs, guidArraySize, encodePresetGUIDCount);
+    if (nvStatus != NV_ENC_SUCCESS)
+    {
+        assert(0);
+    }
+
+    return nvStatus;
+}
+
+NVENCSTATUS CNvHWEncoder::NvEncGetEncodePresetConfig(GUID encodeGUID, GUID  presetGUID, NV_ENC_PRESET_CONFIG* presetConfig)
+{
+    NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+
+    nvStatus = m_pEncodeAPI->nvEncGetEncodePresetConfig(m_hEncoder, encodeGUID, presetGUID, presetConfig);
+    if (nvStatus != NV_ENC_SUCCESS)
+    {
+        assert(0);
+    }
+
+    return nvStatus;
+}
+
+NVENCSTATUS CNvHWEncoder::NvEncCreateInputBuffer(uint32_t width, uint32_t height, void** inputBuffer, NV_ENC_BUFFER_FORMAT inputFormat)
+{
+    NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+    NV_ENC_CREATE_INPUT_BUFFER createInputBufferParams;
+
+    memset(&createInputBufferParams, 0, sizeof(createInputBufferParams));
+    SET_VER(createInputBufferParams, NV_ENC_CREATE_INPUT_BUFFER);
+
+    createInputBufferParams.width = width;
+    createInputBufferParams.height = height;
+    createInputBufferParams.memoryHeap = NV_ENC_MEMORY_HEAP_SYSMEM_CACHED;
+    createInputBufferParams.bufferFmt = inputFormat;
+
+    nvStatus = m_pEncodeAPI->nvEncCreateInputBuffer(m_hEncoder, &createInputBufferParams);
+    if (nvStatus != NV_ENC_SUCCESS)
+    {
+        assert(0);
+    }
+
+    *inputBuffer = createInputBufferParams.inputBuffer;
+
+    return nvStatus;
+}
+
+NVENCSTATUS CNvHWEncoder::NvEncDestroyInputBuffer(NV_ENC_INPUT_PTR inputBuffer)
+{
+    NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+
+    if (inputBuffer)
+    {
+        nvStatus = m_pEncodeAPI->nvEncDestroyInputBuffer(m_hEncoder, inputBuffer);
+        if (nvStatus != NV_ENC_SUCCESS)
+        {
+            assert(0);
+        }
+    }
+
+    return nvStatus;
+}
+
+NVENCSTATUS CNvHWEncoder::NvEncCreateMVBuffer(uint32_t size, void** bitstreamBuffer)
+{
+    NVENCSTATUS status;
+    NV_ENC_CREATE_MV_BUFFER stAllocMVBuffer;
+    memset(&stAllocMVBuffer, 0, sizeof(stAllocMVBuffer));
+    SET_VER(stAllocMVBuffer, NV_ENC_CREATE_MV_BUFFER);
+    status = m_pEncodeAPI->nvEncCreateMVBuffer(m_hEncoder, &stAllocMVBuffer);
+    if (status != NV_ENC_SUCCESS)
+    {
+        assert(0);
+    }
+    *bitstreamBuffer = stAllocMVBuffer.mvBuffer;
+    return status;
+}
+
+NVENCSTATUS CNvHWEncoder::NvEncDestroyMVBuffer(NV_ENC_OUTPUT_PTR bitstreamBuffer)
+{
+    NVENCSTATUS status;
+    NV_ENC_CREATE_MV_BUFFER stAllocMVBuffer;
+    memset(&stAllocMVBuffer, 0, sizeof(stAllocMVBuffer));
+    SET_VER(stAllocMVBuffer, NV_ENC_CREATE_MV_BUFFER);
+    status = m_pEncodeAPI->nvEncDestroyMVBuffer(m_hEncoder, bitstreamBuffer);
+    if (status != NV_ENC_SUCCESS)
+    {
+        assert(0);
+    }
+    bitstreamBuffer = NULL;
+    return status;
+}
+
+NVENCSTATUS CNvHWEncoder::NvRunMotionEstimationOnly(MotionEstimationBuffer *pMEBuffer, MEOnlyConfig *pMEOnly)
+{
+    NVENCSTATUS nvStatus;
+    NV_ENC_MEONLY_PARAMS stMEOnlyParams;
+    SET_VER(stMEOnlyParams,NV_ENC_MEONLY_PARAMS);
+    stMEOnlyParams.referenceFrame = pMEBuffer->stInputBfr[0].hInputSurface;
+    stMEOnlyParams.inputBuffer = pMEBuffer->stInputBfr[1].hInputSurface;
+    stMEOnlyParams.bufferFmt = pMEBuffer->stInputBfr[1].bufferFmt;
+    stMEOnlyParams.inputWidth = pMEBuffer->stInputBfr[1].dwWidth;
+    stMEOnlyParams.inputHeight = pMEBuffer->stInputBfr[1].dwHeight;
+    stMEOnlyParams.mvBuffer = pMEBuffer->stOutputBfr.hBitstreamBuffer;
+    stMEOnlyParams.completionEvent = pMEBuffer->stOutputBfr.hOutputEvent;
+    nvStatus = m_pEncodeAPI->nvEncRunMotionEstimationOnly(m_hEncoder, &stMEOnlyParams);
+    return nvStatus;
+}
+
+NVENCSTATUS CNvHWEncoder::NvEncCreateBitstreamBuffer(uint32_t size, void** bitstreamBuffer)
+{
+    NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+    NV_ENC_CREATE_BITSTREAM_BUFFER createBitstreamBufferParams;
+
+    memset(&createBitstreamBufferParams, 0, sizeof(createBitstreamBufferParams));
+    SET_VER(createBitstreamBufferParams, NV_ENC_CREATE_BITSTREAM_BUFFER);
+
+    createBitstreamBufferParams.size = size;
+    createBitstreamBufferParams.memoryHeap = NV_ENC_MEMORY_HEAP_SYSMEM_CACHED;
+
+    nvStatus = m_pEncodeAPI->nvEncCreateBitstreamBuffer(m_hEncoder, &createBitstreamBufferParams);
+    if (nvStatus != NV_ENC_SUCCESS)
+    {
+        assert(0);
+    }
+
+    *bitstreamBuffer = createBitstreamBufferParams.bitstreamBuffer;
+
+    return nvStatus;
+}
+
+NVENCSTATUS CNvHWEncoder::NvEncDestroyBitstreamBuffer(NV_ENC_OUTPUT_PTR bitstreamBuffer)
+{
+    NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+
+    if (bitstreamBuffer)
+    {
+        nvStatus = m_pEncodeAPI->nvEncDestroyBitstreamBuffer(m_hEncoder, bitstreamBuffer);
+        if (nvStatus != NV_ENC_SUCCESS)
+        {
+            assert(0);
+        }
+    }
+
+    return nvStatus;
+}
+
+NVENCSTATUS CNvHWEncoder::NvEncLockBitstream(NV_ENC_LOCK_BITSTREAM* lockBitstreamBufferParams)
+{
+    NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+
+    nvStatus = m_pEncodeAPI->nvEncLockBitstream(m_hEncoder, lockBitstreamBufferParams);
+    if (nvStatus != NV_ENC_SUCCESS)
+    {
+        assert(0);
+    }
+
+    return nvStatus;
+}
+
+NVENCSTATUS CNvHWEncoder::NvEncUnlockBitstream(NV_ENC_OUTPUT_PTR bitstreamBuffer)
+{
+    NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+
+    nvStatus = m_pEncodeAPI->nvEncUnlockBitstream(m_hEncoder, bitstreamBuffer);
+    if (nvStatus != NV_ENC_SUCCESS)
+    {
+        assert(0);
+    }
+
+    return nvStatus;
+}
+
+NVENCSTATUS CNvHWEncoder::NvEncLockInputBuffer(void* inputBuffer, void** bufferDataPtr, uint32_t* pitch)
+{
+    NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+    NV_ENC_LOCK_INPUT_BUFFER lockInputBufferParams;
+
+    memset(&lockInputBufferParams, 0, sizeof(lockInputBufferParams));
+    SET_VER(lockInputBufferParams, NV_ENC_LOCK_INPUT_BUFFER);
+
+    lockInputBufferParams.inputBuffer = inputBuffer;
+    nvStatus = m_pEncodeAPI->nvEncLockInputBuffer(m_hEncoder, &lockInputBufferParams);
+    if (nvStatus != NV_ENC_SUCCESS)
+    {
+        assert(0);
+    }
+
+    *bufferDataPtr = lockInputBufferParams.bufferDataPtr;
+    *pitch = lockInputBufferParams.pitch;
+
+    return nvStatus;
+}
+
+NVENCSTATUS CNvHWEncoder::NvEncUnlockInputBuffer(NV_ENC_INPUT_PTR inputBuffer)
+{
+    NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+
+    nvStatus = m_pEncodeAPI->nvEncUnlockInputBuffer(m_hEncoder, inputBuffer);
+    if (nvStatus != NV_ENC_SUCCESS)
+    {
+        assert(0);
+    }
+
+    return nvStatus;
+}
+
+NVENCSTATUS CNvHWEncoder::NvEncGetEncodeStats(NV_ENC_STAT* encodeStats)
+{
+    NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+
+    nvStatus = m_pEncodeAPI->nvEncGetEncodeStats(m_hEncoder, encodeStats);
+    if (nvStatus != NV_ENC_SUCCESS)
+    {
+        assert(0);
+    }
+
+    return nvStatus;
+}
+
+NVENCSTATUS CNvHWEncoder::NvEncGetSequenceParams(NV_ENC_SEQUENCE_PARAM_PAYLOAD* sequenceParamPayload)
+{
+    NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+
+    nvStatus = m_pEncodeAPI->nvEncGetSequenceParams(m_hEncoder, sequenceParamPayload);
+    if (nvStatus != NV_ENC_SUCCESS)
+    {
+        assert(0);
+    }
+
+    return nvStatus;
+}
+
+NVENCSTATUS CNvHWEncoder::NvEncRegisterAsyncEvent(void** completionEvent)
+{
+    NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+    NV_ENC_EVENT_PARAMS eventParams;
+
+    memset(&eventParams, 0, sizeof(eventParams));
+    SET_VER(eventParams, NV_ENC_EVENT_PARAMS);
+
+#if defined (NV_WINDOWS)
+    eventParams.completionEvent = CreateEvent(NULL, FALSE, FALSE, NULL);
+#else
+    eventParams.completionEvent = NULL;
+#endif
+    nvStatus = m_pEncodeAPI->nvEncRegisterAsyncEvent(m_hEncoder, &eventParams);
+    if (nvStatus != NV_ENC_SUCCESS)
+    {
+        assert(0);
+    }
+
+    *completionEvent = eventParams.completionEvent;
+
+    return nvStatus;
+}
+
+NVENCSTATUS CNvHWEncoder::NvEncUnregisterAsyncEvent(void* completionEvent)
+{
+    NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+    NV_ENC_EVENT_PARAMS eventParams;
+
+    if (completionEvent)
+    {
+        memset(&eventParams, 0, sizeof(eventParams));
+        SET_VER(eventParams, NV_ENC_EVENT_PARAMS);
+
+        eventParams.completionEvent = completionEvent;
+
+        nvStatus = m_pEncodeAPI->nvEncUnregisterAsyncEvent(m_hEncoder, &eventParams);
+        if (nvStatus != NV_ENC_SUCCESS)
+        {
+            assert(0);
+        }
+    }
+
+    return nvStatus;
+}
+
+NVENCSTATUS CNvHWEncoder::NvEncMapInputResource(void* registeredResource, void** mappedResource)
+{
+    NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+    NV_ENC_MAP_INPUT_RESOURCE mapInputResParams;
+
+    memset(&mapInputResParams, 0, sizeof(mapInputResParams));
+    SET_VER(mapInputResParams, NV_ENC_MAP_INPUT_RESOURCE);
+
+    mapInputResParams.registeredResource = registeredResource;
+
+    nvStatus = m_pEncodeAPI->nvEncMapInputResource(m_hEncoder, &mapInputResParams);
+    if (nvStatus != NV_ENC_SUCCESS)
+    {
+        assert(0);
+    }
+
+    *mappedResource = mapInputResParams.mappedResource;
+
+    return nvStatus;
+}
+
+NVENCSTATUS CNvHWEncoder::NvEncUnmapInputResource(NV_ENC_INPUT_PTR mappedInputBuffer)
+{
+    NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+    
+    if (mappedInputBuffer)
+    {
+        nvStatus = m_pEncodeAPI->nvEncUnmapInputResource(m_hEncoder, mappedInputBuffer);
+        if (nvStatus != NV_ENC_SUCCESS)
+        {
+            assert(0);
+        }
+    }
+
+    return nvStatus;
+}
+
+NVENCSTATUS CNvHWEncoder::NvEncDestroyEncoder()
+{
+    NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+
+    if (m_bEncoderInitialized)
+    {
+        nvStatus = m_pEncodeAPI->nvEncDestroyEncoder(m_hEncoder);
+
+        m_bEncoderInitialized = false;
+    }
+
+    return nvStatus;
+}
+
+NVENCSTATUS CNvHWEncoder::NvEncInvalidateRefFrames(const NvEncPictureCommand *pEncPicCommand)
+{
+    NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+
+    for (uint32_t i = 0; i < pEncPicCommand->numRefFramesToInvalidate; i++)
+    {
+        nvStatus = m_pEncodeAPI->nvEncInvalidateRefFrames(m_hEncoder, pEncPicCommand->refFrameNumbers[i]);
+    }
+
+    return nvStatus;
+}
+
+NVENCSTATUS CNvHWEncoder::NvEncOpenEncodeSessionEx(void* device, NV_ENC_DEVICE_TYPE deviceType)
+{
+    NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+    NV_ENC_OPEN_ENCODE_SESSION_EX_PARAMS openSessionExParams;
+
+    memset(&openSessionExParams, 0, sizeof(openSessionExParams));
+    SET_VER(openSessionExParams, NV_ENC_OPEN_ENCODE_SESSION_EX_PARAMS);
+
+    openSessionExParams.device = device;
+    openSessionExParams.deviceType = deviceType;
+    openSessionExParams.apiVersion = NVENCAPI_VERSION;
+
+    nvStatus = m_pEncodeAPI->nvEncOpenEncodeSessionEx(&openSessionExParams, &m_hEncoder);
+    if (nvStatus != NV_ENC_SUCCESS)
+    {
+        assert(0);
+    }
+
+    return nvStatus;
+}
+
+NVENCSTATUS CNvHWEncoder::NvEncRegisterResource(NV_ENC_INPUT_RESOURCE_TYPE resourceType, void* resourceToRegister, uint32_t width, uint32_t height, uint32_t pitch, void** registeredResource)
+{
+    NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+    NV_ENC_REGISTER_RESOURCE registerResParams;
+
+    memset(&registerResParams, 0, sizeof(registerResParams));
+    SET_VER(registerResParams, NV_ENC_REGISTER_RESOURCE);
+
+    registerResParams.resourceType = resourceType;
+    registerResParams.resourceToRegister = resourceToRegister;
+    registerResParams.width = width;
+    registerResParams.height = height;
+    registerResParams.pitch = pitch;
+    registerResParams.bufferFormat = NV_ENC_BUFFER_FORMAT_ABGR;
+
+    nvStatus = m_pEncodeAPI->nvEncRegisterResource(m_hEncoder, &registerResParams);
+    if (nvStatus != NV_ENC_SUCCESS)
+    {
+        assert(0);
+    }
+
+    *registeredResource = registerResParams.registeredResource;
+
+    return nvStatus;
+}
+
+NVENCSTATUS CNvHWEncoder::NvEncUnregisterResource(NV_ENC_REGISTERED_PTR registeredRes)
+{
+    NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+
+    nvStatus = m_pEncodeAPI->nvEncUnregisterResource(m_hEncoder, registeredRes);
+    if (nvStatus != NV_ENC_SUCCESS)
+    {
+        assert(0);
+    }
+
+    return nvStatus;
+}
+
+NVENCSTATUS CNvHWEncoder::NvEncReconfigureEncoder(const NvEncPictureCommand *pEncPicCommand)
+{
+    NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+
+    if (pEncPicCommand->bBitrateChangePending || pEncPicCommand->bResolutionChangePending)
+    {
+        if (pEncPicCommand->bResolutionChangePending)
+        {
+            m_uCurWidth = pEncPicCommand->newWidth;
+            m_uCurHeight = pEncPicCommand->newHeight;
+            if ((m_uCurWidth > m_uMaxWidth) || (m_uCurHeight > m_uMaxHeight))
+            {
+                return NV_ENC_ERR_INVALID_PARAM;
+            }
+            m_stCreateEncodeParams.encodeWidth = m_uCurWidth;
+            m_stCreateEncodeParams.encodeHeight = m_uCurHeight;
+            m_stCreateEncodeParams.darWidth = m_uCurWidth;
+            m_stCreateEncodeParams.darHeight = m_uCurHeight;
+        }
+
+        if (pEncPicCommand->bBitrateChangePending)
+        {
+            m_stEncodeConfig.rcParams.averageBitRate = pEncPicCommand->newBitrate;
+            m_stEncodeConfig.rcParams.maxBitRate = pEncPicCommand->newBitrate;
+			m_stEncodeConfig.rcParams.vbvBufferSize = pEncPicCommand->newVBVSize != 0 ? pEncPicCommand->newVBVSize : (pEncPicCommand->newBitrate * m_stCreateEncodeParams.frameRateDen) / (m_stCreateEncodeParams.frameRateNum > 0 ? m_stCreateEncodeParams.frameRateNum : 30);
+            m_stEncodeConfig.rcParams.vbvInitialDelay = m_stEncodeConfig.rcParams.vbvBufferSize;
+        }
+
+        NV_ENC_RECONFIGURE_PARAMS stReconfigParams;
+        memset(&stReconfigParams, 0, sizeof(stReconfigParams));
+        memcpy(&stReconfigParams.reInitEncodeParams, &m_stCreateEncodeParams, sizeof(m_stCreateEncodeParams));
+        stReconfigParams.version = NV_ENC_RECONFIGURE_PARAMS_VER;
+        stReconfigParams.forceIDR = pEncPicCommand->bResolutionChangePending ? 1 : 0;
+
+        nvStatus = m_pEncodeAPI->nvEncReconfigureEncoder(m_hEncoder, &stReconfigParams);
+        if (nvStatus != NV_ENC_SUCCESS)
+        {
+            assert(0);
+        }
+    }
+
+    return nvStatus;
+}
+
+CNvHWEncoder::CNvHWEncoder()
+{
+    m_hEncoder = NULL;
+    m_bEncoderInitialized = false;
+    m_pEncodeAPI = NULL;
+    m_hinstLib = NULL;
+    m_fOutput = NULL;
+    m_EncodeIdx = 0;
+    m_uCurWidth = 0;
+    m_uCurHeight = 0;
+    m_uMaxWidth = 0;
+    m_uMaxHeight = 0;
+
+    memset(&m_stCreateEncodeParams, 0, sizeof(m_stCreateEncodeParams));
+    SET_VER(m_stCreateEncodeParams, NV_ENC_INITIALIZE_PARAMS);
+
+    memset(&m_stEncodeConfig, 0, sizeof(m_stEncodeConfig));
+    SET_VER(m_stEncodeConfig, NV_ENC_CONFIG);
+}
+
+CNvHWEncoder::~CNvHWEncoder()
+{
+    // clean up encode API resources here
+    if (m_pEncodeAPI)
+    {
+        delete m_pEncodeAPI;
+        m_pEncodeAPI = NULL;
+    }
+
+    if (m_hinstLib)
+    {
+#if defined (NV_WINDOWS)
+        FreeLibrary(m_hinstLib);
+#else
+        dlclose(m_hinstLib);
+#endif
+
+        m_hinstLib = NULL;
+    }
+}
+
+NVENCSTATUS CNvHWEncoder::ValidateEncodeGUID (GUID inputCodecGuid)
+{
+    unsigned int i, codecFound, encodeGUIDCount, encodeGUIDArraySize;
+    NVENCSTATUS nvStatus;
+    GUID *encodeGUIDArray;
+
+    nvStatus = m_pEncodeAPI->nvEncGetEncodeGUIDCount(m_hEncoder, &encodeGUIDCount);
+    if (nvStatus != NV_ENC_SUCCESS)
+    {
+        assert(0);
+        return nvStatus;
+    }
+
+    encodeGUIDArray = new GUID[encodeGUIDCount];
+    memset(encodeGUIDArray, 0, sizeof(GUID)* encodeGUIDCount);
+
+    encodeGUIDArraySize = 0;
+    nvStatus = m_pEncodeAPI->nvEncGetEncodeGUIDs(m_hEncoder, encodeGUIDArray, encodeGUIDCount, &encodeGUIDArraySize);
+    if (nvStatus != NV_ENC_SUCCESS)
+    {
+        delete[] encodeGUIDArray;
+        assert(0);
+        return nvStatus;
+    }
+
+    assert(encodeGUIDArraySize <= encodeGUIDCount);
+
+    codecFound = 0;
+    for (i = 0; i < encodeGUIDArraySize; i++)
+    {
+        if (inputCodecGuid == encodeGUIDArray[i])
+        {
+            codecFound = 1;
+            break;
+        }
+    }
+
+    delete[] encodeGUIDArray;
+
+    if (codecFound)
+        return NV_ENC_SUCCESS;
+    else
+        return NV_ENC_ERR_INVALID_PARAM;
+}
+
+NVENCSTATUS CNvHWEncoder::ValidatePresetGUID(GUID inputPresetGuid, GUID inputCodecGuid)
+{
+    uint32_t i, presetFound, presetGUIDCount, presetGUIDArraySize;
+    NVENCSTATUS nvStatus;
+    GUID *presetGUIDArray;
+
+    nvStatus = m_pEncodeAPI->nvEncGetEncodePresetCount(m_hEncoder, inputCodecGuid, &presetGUIDCount);
+    if (nvStatus != NV_ENC_SUCCESS)
+    {
+        assert(0);
+        return nvStatus;
+    }
+
+    presetGUIDArray = new GUID[presetGUIDCount];
+    memset(presetGUIDArray, 0, sizeof(GUID)* presetGUIDCount);
+
+    presetGUIDArraySize = 0;
+    nvStatus = m_pEncodeAPI->nvEncGetEncodePresetGUIDs(m_hEncoder, inputCodecGuid, presetGUIDArray, presetGUIDCount, &presetGUIDArraySize);
+    if (nvStatus != NV_ENC_SUCCESS)
+    {
+        assert(0);
+        delete[] presetGUIDArray;
+        return nvStatus;
+    }
+
+    assert(presetGUIDArraySize <= presetGUIDCount);
+
+    presetFound = 0;
+    for (i = 0; i < presetGUIDArraySize; i++)
+    {
+        if (inputPresetGuid == presetGUIDArray[i])
+        {
+            presetFound = 1;
+            break;
+        }
+    }
+
+    delete[] presetGUIDArray;
+
+    if (presetFound)
+        return NV_ENC_SUCCESS;
+    else
+        return NV_ENC_ERR_INVALID_PARAM;
+}
+
+NVENCSTATUS CNvHWEncoder::CreateEncoder(EncodeConfig *pEncCfg)
+{
+    NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+
+    if (pEncCfg == NULL)
+    {
+        return NV_ENC_ERR_INVALID_PARAM;
+    }
+
+    m_uCurWidth = pEncCfg->width;
+    m_uCurHeight = pEncCfg->height;
+
+    m_uMaxWidth = (pEncCfg->maxWidth > 0 ? pEncCfg->maxWidth : pEncCfg->width);
+    m_uMaxHeight = (pEncCfg->maxHeight > 0 ? pEncCfg->maxHeight : pEncCfg->height);
+
+    if ((m_uCurWidth > m_uMaxWidth) || (m_uCurHeight > m_uMaxHeight)) {
+        return NV_ENC_ERR_INVALID_PARAM;
+    }
+
+    m_fOutput = pEncCfg->fOutput;
+
+    if (!pEncCfg->width || !pEncCfg->height)
+    {
+        return NV_ENC_ERR_INVALID_PARAM;
+    }
+
+    if ((pEncCfg->inputFormat == NV_ENC_BUFFER_FORMAT_YUV420_10BIT || pEncCfg->inputFormat == NV_ENC_BUFFER_FORMAT_YUV444_10BIT) && (pEncCfg->codec == NV_ENC_H264))
+    {
+        PRINTERR("10 bit is not supported with H264 \n");
+        return NV_ENC_ERR_INVALID_PARAM;
+    }
+
+    GUID inputCodecGUID = pEncCfg->codec == NV_ENC_H264 ? NV_ENC_CODEC_H264_GUID : NV_ENC_CODEC_HEVC_GUID;
+    nvStatus = ValidateEncodeGUID(inputCodecGUID);
+    if (nvStatus != NV_ENC_SUCCESS)
+    {
+        PRINTERR("codec not supported \n");
+        return nvStatus;
+    }
+
+    codecGUID = inputCodecGUID;
+
+    m_stCreateEncodeParams.encodeGUID = inputCodecGUID;
+    m_stCreateEncodeParams.presetGUID = pEncCfg->presetGUID;
+    m_stCreateEncodeParams.encodeWidth = pEncCfg->width;
+    m_stCreateEncodeParams.encodeHeight = pEncCfg->height;
+
+    m_stCreateEncodeParams.darWidth = pEncCfg->width;
+    m_stCreateEncodeParams.darHeight = pEncCfg->height;
+    m_stCreateEncodeParams.frameRateNum = pEncCfg->fps;
+    m_stCreateEncodeParams.frameRateDen = 1;
+    m_stCreateEncodeParams.enableEncodeAsync = 0;
+
+    m_stCreateEncodeParams.enablePTD = 1;
+    m_stCreateEncodeParams.reportSliceOffsets = 0;
+    m_stCreateEncodeParams.enableSubFrameWrite = 0;
+    m_stCreateEncodeParams.encodeConfig = &m_stEncodeConfig;
+    m_stCreateEncodeParams.maxEncodeWidth = m_uMaxWidth;
+    m_stCreateEncodeParams.maxEncodeHeight = m_uMaxHeight;
+
+    // apply preset
+    NV_ENC_PRESET_CONFIG stPresetCfg;
+    memset(&stPresetCfg, 0, sizeof(NV_ENC_PRESET_CONFIG));
+    SET_VER(stPresetCfg, NV_ENC_PRESET_CONFIG);
+    SET_VER(stPresetCfg.presetCfg, NV_ENC_CONFIG);
+
+    nvStatus = m_pEncodeAPI->nvEncGetEncodePresetConfig(m_hEncoder, m_stCreateEncodeParams.encodeGUID, m_stCreateEncodeParams.presetGUID, &stPresetCfg);
+    if (nvStatus != NV_ENC_SUCCESS)
+    {
+        PRINTERR("nvEncGetEncodePresetConfig returned failure");
+        return nvStatus;
+    }
+
+    memcpy(&m_stEncodeConfig, &stPresetCfg.presetCfg, sizeof(NV_ENC_CONFIG));
+
+    m_stEncodeConfig.gopLength = pEncCfg->gopLength;
+    m_stEncodeConfig.frameIntervalP = pEncCfg->numB + 1;
+    if (pEncCfg->pictureStruct == NV_ENC_PIC_STRUCT_FRAME)
+    {
+        m_stEncodeConfig.frameFieldMode = NV_ENC_PARAMS_FRAME_FIELD_MODE_FRAME;
+    }
+    else
+    {
+        m_stEncodeConfig.frameFieldMode = NV_ENC_PARAMS_FRAME_FIELD_MODE_FIELD;
+    }
+
+    m_stEncodeConfig.mvPrecision = NV_ENC_MV_PRECISION_QUARTER_PEL;
+
+    if (pEncCfg->bitrate || pEncCfg->vbvMaxBitrate)
+    {
+        m_stEncodeConfig.rcParams.rateControlMode = (NV_ENC_PARAMS_RC_MODE)pEncCfg->rcMode;
+        m_stEncodeConfig.rcParams.averageBitRate = pEncCfg->bitrate;
+        m_stEncodeConfig.rcParams.maxBitRate = pEncCfg->vbvMaxBitrate;
+        m_stEncodeConfig.rcParams.vbvBufferSize = pEncCfg->vbvSize;
+        m_stEncodeConfig.rcParams.vbvInitialDelay = pEncCfg->vbvSize * 9 / 10;
+    }
+    else
+    {
+        m_stEncodeConfig.rcParams.rateControlMode = NV_ENC_PARAMS_RC_CONSTQP;
+    }
+
+    if (pEncCfg->rcMode == 0)
+    {
+        m_stEncodeConfig.rcParams.constQP.qpInterP = pEncCfg->presetGUID == NV_ENC_PRESET_LOSSLESS_HP_GUID? 0 : pEncCfg->qp;
+        m_stEncodeConfig.rcParams.constQP.qpInterB = pEncCfg->presetGUID == NV_ENC_PRESET_LOSSLESS_HP_GUID? 0 : pEncCfg->qp;
+        m_stEncodeConfig.rcParams.constQP.qpIntra = pEncCfg->presetGUID == NV_ENC_PRESET_LOSSLESS_HP_GUID? 0 : pEncCfg->qp;
+    }
+
+    // set up initial QP value
+    if (pEncCfg->rcMode == NV_ENC_PARAMS_RC_VBR || pEncCfg->rcMode == NV_ENC_PARAMS_RC_VBR_MINQP ||
+        pEncCfg->rcMode == NV_ENC_PARAMS_RC_2_PASS_VBR) {
+        m_stEncodeConfig.rcParams.enableInitialRCQP = 1;
+        m_stEncodeConfig.rcParams.initialRCQP.qpInterP  = pEncCfg->qp;
+        if(pEncCfg->i_quant_factor != 0.0 && pEncCfg->b_quant_factor != 0.0) {               
+            m_stEncodeConfig.rcParams.initialRCQP.qpIntra = (int)(pEncCfg->qp * FABS(pEncCfg->i_quant_factor) + pEncCfg->i_quant_offset);
+            m_stEncodeConfig.rcParams.initialRCQP.qpInterB = (int)(pEncCfg->qp * FABS(pEncCfg->b_quant_factor) + pEncCfg->b_quant_offset);
+        } else {
+            m_stEncodeConfig.rcParams.initialRCQP.qpIntra = pEncCfg->qp;
+            m_stEncodeConfig.rcParams.initialRCQP.qpInterB = pEncCfg->qp;
+        }
+
+    }
+
+    if (pEncCfg->inputFormat == NV_ENC_BUFFER_FORMAT_YUV444 || pEncCfg->inputFormat == NV_ENC_BUFFER_FORMAT_YUV444_10BIT)
+    {
+        if (pEncCfg->codec == NV_ENC_HEVC) {
+            m_stEncodeConfig.encodeCodecConfig.hevcConfig.chromaFormatIDC = 3;
+        } else {
+            m_stEncodeConfig.encodeCodecConfig.h264Config.chromaFormatIDC = 3;
+        }
+    }
+    else
+    {
+        if (pEncCfg->codec == NV_ENC_HEVC) {
+            m_stEncodeConfig.encodeCodecConfig.hevcConfig.chromaFormatIDC = 1;
+        } else {
+            m_stEncodeConfig.encodeCodecConfig.h264Config.chromaFormatIDC = 1;
+        }
+    }
+
+    if (pEncCfg->inputFormat == NV_ENC_BUFFER_FORMAT_YUV420_10BIT || pEncCfg->inputFormat == NV_ENC_BUFFER_FORMAT_YUV444_10BIT)
+    {
+        if (pEncCfg->codec == NV_ENC_HEVC) {
+            m_stEncodeConfig.encodeCodecConfig.hevcConfig.pixelBitDepthMinus8 = 2;
+        }
+    }
+
+    if (pEncCfg->intraRefreshEnableFlag)
+    {
+        if (pEncCfg->codec == NV_ENC_HEVC)
+        {
+            m_stEncodeConfig.encodeCodecConfig.hevcConfig.enableIntraRefresh = 1;
+            m_stEncodeConfig.encodeCodecConfig.hevcConfig.intraRefreshPeriod = pEncCfg->intraRefreshPeriod;
+            m_stEncodeConfig.encodeCodecConfig.hevcConfig.intraRefreshCnt = pEncCfg->intraRefreshDuration;
+        }
+        else
+        {
+            m_stEncodeConfig.encodeCodecConfig.h264Config.enableIntraRefresh = 1;
+            m_stEncodeConfig.encodeCodecConfig.h264Config.intraRefreshPeriod = pEncCfg->intraRefreshPeriod;
+            m_stEncodeConfig.encodeCodecConfig.h264Config.intraRefreshCnt = pEncCfg->intraRefreshDuration;
+        }
+    }
+
+    if (pEncCfg->invalidateRefFramesEnableFlag)
+    {
+        if (pEncCfg->codec == NV_ENC_HEVC)
+        {
+            m_stEncodeConfig.encodeCodecConfig.hevcConfig.maxNumRefFramesInDPB = 16;
+        }
+        else
+        {
+            m_stEncodeConfig.encodeCodecConfig.h264Config.maxNumRefFrames = 16;
+        }
+    }
+
+    if (pEncCfg->qpDeltaMapFile)
+    {
+        m_stEncodeConfig.rcParams.enableExtQPDeltaMap = 1;
+    }
+    if (pEncCfg->codec == NV_ENC_H264)
+    {
+        m_stEncodeConfig.encodeCodecConfig.h264Config.idrPeriod = pEncCfg->gopLength;
+    }
+    else if (pEncCfg->codec == NV_ENC_HEVC)
+    {
+        m_stEncodeConfig.encodeCodecConfig.hevcConfig.idrPeriod = pEncCfg->gopLength;
+    }
+
+    NV_ENC_CAPS_PARAM stCapsParam;
+    int asyncMode = 0;
+    memset(&stCapsParam, 0, sizeof(NV_ENC_CAPS_PARAM));
+    SET_VER(stCapsParam, NV_ENC_CAPS_PARAM);
+
+    stCapsParam.capsToQuery = NV_ENC_CAPS_ASYNC_ENCODE_SUPPORT;
+    m_pEncodeAPI->nvEncGetEncodeCaps(m_hEncoder, m_stCreateEncodeParams.encodeGUID, &stCapsParam, &asyncMode);
+    m_stCreateEncodeParams.enableEncodeAsync = asyncMode;
+
+    pEncCfg->enableAsyncMode = asyncMode;
+
+    if (pEncCfg->enableMEOnly == 1 || pEncCfg->enableMEOnly == 2)
+    {
+
+        stCapsParam.capsToQuery = NV_ENC_CAPS_SUPPORT_MEONLY_MODE;
+        m_stCreateEncodeParams.enableMEOnlyMode =  true;
+        int meonlyMode = 0;
+        nvStatus = m_pEncodeAPI->nvEncGetEncodeCaps(m_hEncoder, m_stCreateEncodeParams.encodeGUID, &stCapsParam, &meonlyMode);
+        if (nvStatus != NV_ENC_SUCCESS)
+        {
+            PRINTERR("Encode Session Initialization failed");
+            return nvStatus;
+        }
+        else
+        {
+            if (meonlyMode == 1)
+            {
+                printf("NV_ENC_CAPS_SUPPORT_MEONLY_MODE  supported\n");
+            }
+            else
+            {
+                PRINTERR("NV_ENC_CAPS_SUPPORT_MEONLY_MODE not supported\n");
+                return NV_ENC_ERR_UNSUPPORTED_DEVICE;
+            }
+        } 
+    }
+
+    if (pEncCfg->enableTemporalAQ == 1)
+    {
+        NV_ENC_CAPS_PARAM stCapsParam;
+        memset(&stCapsParam, 0, sizeof(NV_ENC_CAPS_PARAM));
+        SET_VER(stCapsParam, NV_ENC_CAPS_PARAM);
+        stCapsParam.capsToQuery = NV_ENC_CAPS_SUPPORT_TEMPORAL_AQ;
+        int temporalAQSupported = 0;
+        nvStatus = m_pEncodeAPI->nvEncGetEncodeCaps(m_hEncoder, m_stCreateEncodeParams.encodeGUID, &stCapsParam, &temporalAQSupported);
+        if (nvStatus != NV_ENC_SUCCESS)
+        {
+            PRINTERR("Encode Session Initialization failed");
+            return nvStatus;
+        }
+        else
+        {
+            if (temporalAQSupported == 1)
+            {
+                m_stEncodeConfig.rcParams.enableTemporalAQ = 1;
+            }
+            else
+            {
+                PRINTERR("NV_ENC_CAPS_SUPPORT_TEMPORAL_AQ not supported\n");
+                return NV_ENC_ERR_UNSUPPORTED_DEVICE;
+            }
+        }
+    }
+
+    nvStatus = m_pEncodeAPI->nvEncInitializeEncoder(m_hEncoder, &m_stCreateEncodeParams);
+    if (nvStatus != NV_ENC_SUCCESS)
+    {
+        PRINTERR("Encode Session Initialization failed");
+        return nvStatus;
+    }
+    m_bEncoderInitialized = true;
+
+    return nvStatus;
+}
+
+GUID CNvHWEncoder::GetPresetGUID(char* encoderPreset, int codec)
+{
+    NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+    GUID presetGUID = NV_ENC_PRESET_DEFAULT_GUID;
+
+    if (encoderPreset && (stricmp(encoderPreset, "hq") == 0))
+    {
+        presetGUID = NV_ENC_PRESET_HQ_GUID;
+    }
+    else if (encoderPreset && (stricmp(encoderPreset, "lowLatencyHP") == 0))
+    {
+        presetGUID = NV_ENC_PRESET_LOW_LATENCY_HP_GUID;
+    }
+    else if (encoderPreset && (stricmp(encoderPreset, "hp") == 0))
+    {
+        presetGUID = NV_ENC_PRESET_HP_GUID;
+    }
+    else if (encoderPreset && (stricmp(encoderPreset, "lowLatencyHQ") == 0))
+    {
+        presetGUID = NV_ENC_PRESET_LOW_LATENCY_HQ_GUID;
+    }
+    else if (encoderPreset && (stricmp(encoderPreset, "losslessHP") == 0))
+    {
+        presetGUID = NV_ENC_PRESET_LOSSLESS_HP_GUID;
+    }
+	else if (encoderPreset && (stricmp(encoderPreset, "lossless") == 0)) 
+	{
+		presetGUID = NV_ENC_PRESET_LOSSLESS_DEFAULT_GUID;
+	} 
+	else if (encoderPreset && (stricmp(encoderPreset, "lowLatency") == 0))
+	{
+		presetGUID = NV_ENC_PRESET_LOW_LATENCY_DEFAULT_GUID;
+	}
+	else if (encoderPreset && (stricmp(encoderPreset, "bluray") == 0))
+	{
+		presetGUID = NV_ENC_PRESET_BD_GUID;
+	}
+	else {
+        if (encoderPreset)
+            PRINTERR("Unsupported preset guid %s\n", encoderPreset);
+        presetGUID = NV_ENC_PRESET_DEFAULT_GUID;
+    }
+
+    GUID inputCodecGUID = codec == NV_ENC_H264 ? NV_ENC_CODEC_H264_GUID : NV_ENC_CODEC_HEVC_GUID;
+    nvStatus = ValidatePresetGUID(presetGUID, inputCodecGUID);
+    if (nvStatus != NV_ENC_SUCCESS)
+    {
+        presetGUID = NV_ENC_PRESET_DEFAULT_GUID;
+        PRINTERR("Unsupported preset guid %s\n", encoderPreset);
+    }
+
+    return presetGUID;
+}
+
+NVENCSTATUS CNvHWEncoder::ProcessOutput(const EncodeBuffer *pEncodeBuffer)
+{
+    NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+
+    if (pEncodeBuffer->stOutputBfr.hBitstreamBuffer == NULL && pEncodeBuffer->stOutputBfr.bEOSFlag == FALSE)
+    {
+        return NV_ENC_ERR_INVALID_PARAM;
+    }
+
+    if (pEncodeBuffer->stOutputBfr.bWaitOnEvent == TRUE)
+    {
+        if (!pEncodeBuffer->stOutputBfr.hOutputEvent)
+        {
+            return NV_ENC_ERR_INVALID_PARAM;
+        }
+#if defined(NV_WINDOWS)
+        WaitForSingleObject(pEncodeBuffer->stOutputBfr.hOutputEvent, INFINITE);
+#endif
+    }
+
+	// TODO: Check eos issue
+    //if (pEncodeBuffer->stOutputBfr.bEOSFlag)
+    //    return NV_ENC_SUCCESS;
+
+    nvStatus = NV_ENC_SUCCESS;
+    memset(&m_lockBitstreamData, 0, sizeof(m_lockBitstreamData));
+    SET_VER(m_lockBitstreamData, NV_ENC_LOCK_BITSTREAM);
+	m_lockBitstreamData.outputBitstream = pEncodeBuffer->stOutputBfr.hBitstreamBuffer;
+	m_lockBitstreamData.doNotWait = true;
+
+    nvStatus = m_pEncodeAPI->nvEncLockBitstream(m_hEncoder, &m_lockBitstreamData);
+    if (nvStatus == NV_ENC_SUCCESS)
+    {
+		//if (m_fOutput)
+		//{
+		//	fwrite(m_lockBitstreamData.bitstreamBufferPtr, 1, m_lockBitstreamData.bitstreamSizeInBytes, m_fOutput);
+		//}
+
+        nvStatus = m_pEncodeAPI->nvEncUnlockBitstream(m_hEncoder, pEncodeBuffer->stOutputBfr.hBitstreamBuffer);
+    }
+    else
+    {
+        PRINTERR("lock bitstream function failed \n");
+    }
+
+    return nvStatus;
+}
+
+NVENCSTATUS CNvHWEncoder::ProcessMVOutput(const MotionEstimationBuffer *pMEBuffer)
+{
+    NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+
+    if (pMEBuffer->stOutputBfr.hBitstreamBuffer == NULL && pMEBuffer->stOutputBfr.bEOSFlag == FALSE)
+    {
+        return NV_ENC_ERR_INVALID_PARAM;
+    }
+
+    if (pMEBuffer->stOutputBfr.bWaitOnEvent == TRUE)
+    {
+        if (!pMEBuffer->stOutputBfr.hOutputEvent)
+        {
+            return NV_ENC_ERR_INVALID_PARAM;
+        }
+#if defined(NV_WINDOWS)
+        WaitForSingleObject(pMEBuffer->stOutputBfr.hOutputEvent, INFINITE);
+#endif
+    }
+
+    if (pMEBuffer->stOutputBfr.bEOSFlag)
+        return NV_ENC_SUCCESS;
+
+    nvStatus = NV_ENC_SUCCESS;
+    NV_ENC_LOCK_BITSTREAM lockBitstreamData;
+    memset(&lockBitstreamData, 0, sizeof(lockBitstreamData));
+    SET_VER(lockBitstreamData, NV_ENC_LOCK_BITSTREAM);
+    lockBitstreamData.outputBitstream = pMEBuffer->stOutputBfr.hBitstreamBuffer;
+    lockBitstreamData.doNotWait = false;
+
+    nvStatus = m_pEncodeAPI->nvEncLockBitstream(m_hEncoder, &lockBitstreamData);
+    if (nvStatus == NV_ENC_SUCCESS)
+    {
+        if (codecGUID == NV_ENC_CODEC_H264_GUID)
+        {
+            unsigned int numMBs = ((m_uMaxWidth + 15) >> 4) * ((m_uMaxHeight + 15) >> 4);
+            fprintf(m_fOutput, "Motion Vectors for input frame = %d, reference frame = %d\n", pMEBuffer->inputFrameIndex, pMEBuffer->referenceFrameIndex);
+            fprintf(m_fOutput, "block, mb_type, partitionType, "
+                "MV[0].x, MV[0].y, MV[1].x, MV[1].y, MV[2].x, MV[2].y, MV[3].x, MV[3].y, cost\n");
+            NV_ENC_H264_MV_DATA *outputMV = (NV_ENC_H264_MV_DATA *)lockBitstreamData.bitstreamBufferPtr;
+            for (unsigned int i = 0; i < numMBs; i++)
+            {
+                fprintf(m_fOutput, "%d, %d, %d, %d, %d, %d, %d, %d, %d, %d, %d, %d\n", \
+                    i, outputMV[i].mbType, outputMV[i].partitionType, \
+                    outputMV[i].mv[0].mvx, outputMV[i].mv[0].mvy, outputMV[i].mv[1].mvx, outputMV[i].mv[1].mvy, \
+                    outputMV[i].mv[2].mvx, outputMV[i].mv[2].mvy, outputMV[i].mv[3].mvx, outputMV[i].mv[3].mvy, outputMV[i].mbCost);
+            }
+            fprintf(m_fOutput, "\n");
+        }
+        else
+        {
+            unsigned int numCTBs = ((m_uMaxWidth + 31) >> 5) * ((m_uMaxHeight + 31) >> 5);
+            fprintf(m_fOutput, "Motion Vectors for input frame = %d, reference frame = %d\n", pMEBuffer->inputFrameIndex, pMEBuffer->referenceFrameIndex);
+            NV_ENC_HEVC_MV_DATA *outputMV = (NV_ENC_HEVC_MV_DATA *)lockBitstreamData.bitstreamBufferPtr;
+            fprintf(m_fOutput, "ctb, cuType, cuSize, partitionMode, "
+                "MV[0].x, MV[0].y, MV[1].x, MV[1].y, MV[2].x, MV[2].y, MV[3].x, MV[3].y\n");
+            bool lastCUInCTB = false;
+            for (unsigned int i = 0; i < numCTBs; i++)
+            {
+                do
+                {
+                    lastCUInCTB = outputMV->lastCUInCTB ? true : false;
+                    fprintf(m_fOutput, "%d, %d, %d, %d, %d, %d, %d, %d, %d, %d, %d, %d\n", \
+                        i, outputMV->cuType, outputMV->cuSize, outputMV->partitionMode, \
+                        outputMV->mv[0].mvx, outputMV->mv[0].mvy, outputMV->mv[1].mvx, outputMV->mv[1].mvy, \
+                        outputMV->mv[2].mvx, outputMV->mv[2].mvy, outputMV->mv[3].mvx, outputMV->mv[3].mvy);
+                    outputMV += 1;
+                } while (!lastCUInCTB);
+            }
+            fprintf(m_fOutput, "\n");
+        }
+        nvStatus = m_pEncodeAPI->nvEncUnlockBitstream(m_hEncoder, pMEBuffer->stOutputBfr.hBitstreamBuffer);
+    }
+    else
+    {
+        PRINTERR("lock bitstream function failed \n");
+    }
+
+    return nvStatus;
+}
+
+NVENCSTATUS CNvHWEncoder::Initialize(void* device, NV_ENC_DEVICE_TYPE deviceType)
+{
+    NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+    MYPROC nvEncodeAPICreateInstance; // function pointer to create instance in nvEncodeAPI
+
+#if defined(NV_WINDOWS)
+#if defined (_WIN64)
+	m_hinstLib = LoadLibrary(TEXT("nvEncodeAPI64.dll"));
+#else
+	m_hinstLib = LoadLibrary(TEXT("nvEncodeAPI.dll"));
+#endif
+#else
+	m_hinstLib = dlopen("libnvidia-encode.so.1", RTLD_LAZY);
+#endif
+
+    if (m_hinstLib == NULL)
+        return NV_ENC_ERR_OUT_OF_MEMORY;
+
+#if defined(NV_WINDOWS)
+    nvEncodeAPICreateInstance = (MYPROC)GetProcAddress(m_hinstLib, "NvEncodeAPICreateInstance");
+#else
+    nvEncodeAPICreateInstance = (MYPROC)dlsym(lib, "NvEncodeAPICreateInstance");
+#endif
+
+    if (nvEncodeAPICreateInstance == NULL)
+        return NV_ENC_ERR_OUT_OF_MEMORY;
+
+    m_pEncodeAPI = new NV_ENCODE_API_FUNCTION_LIST;
+    if (m_pEncodeAPI == NULL)
+        return NV_ENC_ERR_OUT_OF_MEMORY;
+
+    memset(m_pEncodeAPI, 0, sizeof(NV_ENCODE_API_FUNCTION_LIST));
+    m_pEncodeAPI->version = NV_ENCODE_API_FUNCTION_LIST_VER;
+    nvStatus = nvEncodeAPICreateInstance(m_pEncodeAPI);
+    if (nvStatus != NV_ENC_SUCCESS)
+        return nvStatus;
+
+    nvStatus = NvEncOpenEncodeSessionEx(device, deviceType);
+    if (nvStatus != NV_ENC_SUCCESS)
+        return nvStatus;
+
+    return NV_ENC_SUCCESS;
+}
+
+NVENCSTATUS CNvHWEncoder::NvEncEncodeFrame(EncodeBuffer *pEncodeBuffer, NvEncPictureCommand *encPicCommand,
+                                           uint32_t width, uint32_t height, NV_ENC_PIC_STRUCT ePicStruct,
+                                           int8_t *qpDeltaMapArray, uint32_t qpDeltaMapArraySize)
+{
+    NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+    NV_ENC_PIC_PARAMS encPicParams;
+
+    memset(&encPicParams, 0, sizeof(encPicParams));
+    SET_VER(encPicParams, NV_ENC_PIC_PARAMS);
+
+    encPicParams.inputBuffer = pEncodeBuffer->stInputBfr.hInputSurface;
+    encPicParams.bufferFmt = pEncodeBuffer->stInputBfr.bufferFmt;
+    encPicParams.inputWidth = width;
+    encPicParams.inputHeight = height;
+    encPicParams.outputBitstream = pEncodeBuffer->stOutputBfr.hBitstreamBuffer;
+    encPicParams.completionEvent = pEncodeBuffer->stOutputBfr.hOutputEvent;
+    encPicParams.inputTimeStamp = m_EncodeIdx;
+    encPicParams.pictureStruct = ePicStruct;
+    encPicParams.qpDeltaMap = qpDeltaMapArray;
+    encPicParams.qpDeltaMapSize = qpDeltaMapArraySize;
+
+    if (encPicCommand)
+    {
+        if (encPicCommand->bForceIDR)
+        {
+            encPicParams.encodePicFlags |= NV_ENC_PIC_FLAG_FORCEIDR;
+        }
+
+        if (encPicCommand->bForceIntraRefresh)
+        {
+            if (codecGUID == NV_ENC_CODEC_HEVC_GUID)
+            {
+                encPicParams.codecPicParams.hevcPicParams.forceIntraRefreshWithFrameCnt = encPicCommand->intraRefreshDuration;
+            }
+            else
+            {
+                encPicParams.codecPicParams.h264PicParams.forceIntraRefreshWithFrameCnt = encPicCommand->intraRefreshDuration;
+            }
+        }
+    }
+
+    nvStatus = m_pEncodeAPI->nvEncEncodePicture(m_hEncoder, &encPicParams);
+    if (nvStatus != NV_ENC_SUCCESS && nvStatus != NV_ENC_ERR_NEED_MORE_INPUT)
+    {
+        assert(0);
+        return nvStatus;
+    }
+
+    m_EncodeIdx++;
+
+    return NV_ENC_SUCCESS;
+}
+
+NVENCSTATUS CNvHWEncoder::NvEncFlushEncoderQueue(void *hEOSEvent)
+{
+    NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+    NV_ENC_PIC_PARAMS encPicParams;
+    memset(&encPicParams, 0, sizeof(encPicParams));
+    SET_VER(encPicParams, NV_ENC_PIC_PARAMS);
+    encPicParams.encodePicFlags = NV_ENC_PIC_FLAG_EOS;
+    encPicParams.completionEvent = hEOSEvent;
+
+	if (!m_bEncoderInitialized)
+		return nvStatus;
+
+    nvStatus = m_pEncodeAPI->nvEncEncodePicture(m_hEncoder, &encPicParams);
+    if (nvStatus != NV_ENC_SUCCESS)
+    {
+        assert(0);
+    }
+
+    return nvStatus;
+}
+
+NV_ENC_LOCK_BITSTREAM CNvHWEncoder::GetLockBitStream()
+{
+	return m_lockBitstreamData;
+}
+
+NVENCSTATUS CNvHWEncoder::ParseArguments(EncodeConfig *encodeConfig, int argc, char *argv[])
+{
+    for (int i = 1; i < argc; i++)
+    {
+        if (stricmp(argv[i], "-bmpfilePath") == 0)
+        {
+            if (++i >= argc)
+            {
+                PRINTERR("invalid parameter for %s\n", argv[i - 1]);
+                return NV_ENC_ERR_INVALID_PARAM;
+            }
+            encodeConfig->inputFilePath = argv[i];
+        }
+        else if (stricmp(argv[i], "-i") == 0)
+        {
+            if (++i >= argc)
+            {
+                PRINTERR("invalid parameter for %s\n", argv[i - 1]);
+                return NV_ENC_ERR_INVALID_PARAM;
+            }
+            encodeConfig->inputFileName = argv[i];
+        }
+        else if (stricmp(argv[i], "-o") == 0)
+        {
+            if (++i >= argc)
+            {
+                PRINTERR("invalid parameter for %s\n", argv[i - 1]);
+                return NV_ENC_ERR_INVALID_PARAM;
+            }
+            encodeConfig->outputFileName = argv[i];
+        }
+        else if (stricmp(argv[i], "-size") == 0)
+        {
+            if (++i >= argc || sscanf(argv[i], "%d", &encodeConfig->width) != 1)
+            {
+                PRINTERR("invalid parameter for %s\n", argv[i - 1]);
+                return NV_ENC_ERR_INVALID_PARAM;
+            }
+
+            if (++i >= argc || sscanf(argv[i], "%d", &encodeConfig->height) != 1)
+            {
+                PRINTERR("invalid parameter for %s\n", argv[i - 2]);
+                return NV_ENC_ERR_INVALID_PARAM;
+            }
+        }
+        else if (stricmp(argv[i], "-maxSize") == 0)
+        {
+            if (++i >= argc || sscanf(argv[i], "%d", &encodeConfig->maxWidth) != 1)
+            {
+                PRINTERR("invalid parameter for %s\n", argv[i - 1]);
+                return NV_ENC_ERR_INVALID_PARAM;
+            }
+
+            if (++i >= argc || sscanf(argv[i], "%d", &encodeConfig->maxHeight) != 1)
+            {
+                PRINTERR("invalid parameter for %s\n", argv[i - 2]);
+                return NV_ENC_ERR_INVALID_PARAM;
+            }
+        }
+        else if (stricmp(argv[i], "-bitrate") == 0)
+        {
+            if (++i >= argc || sscanf(argv[i], "%d", &encodeConfig->bitrate) != 1)
+            {
+                PRINTERR("invalid parameter for %s\n", argv[i - 1]);
+                return NV_ENC_ERR_INVALID_PARAM;
+            }
+        }
+        else if (stricmp(argv[i], "-vbvMaxBitrate") == 0)
+        {
+            if (++i >= argc || sscanf(argv[i], "%d", &encodeConfig->vbvMaxBitrate) != 1)
+            {
+                PRINTERR("invalid parameter for %s\n", argv[i - 1]);
+                return NV_ENC_ERR_INVALID_PARAM;
+            }
+        }
+        else if (stricmp(argv[i], "-vbvSize") == 0)
+        {
+            if (++i >= argc || sscanf(argv[i], "%d", &encodeConfig->vbvSize) != 1)
+            {
+                PRINTERR("invalid parameter for %s\n", argv[i - 1]);
+                return NV_ENC_ERR_INVALID_PARAM;
+            }
+        }
+        else if (stricmp(argv[i], "-fps") == 0)
+        {
+            if (++i >= argc || sscanf(argv[i], "%d", &encodeConfig->fps) != 1)
+            {
+                PRINTERR("invalid parameter for %s\n", argv[i - 1]);
+                return NV_ENC_ERR_INVALID_PARAM;
+            }
+        }
+        else if (stricmp(argv[i], "-startf") == 0)
+        {
+            if (++i >= argc || sscanf(argv[i], "%d", &encodeConfig->startFrameIdx) != 1)
+            {
+                PRINTERR("invalid parameter for %s\n", argv[i - 1]);
+                return NV_ENC_ERR_INVALID_PARAM;
+            }
+        }
+        else if (stricmp(argv[i], "-endf") == 0)
+        {
+            if (++i >= argc || sscanf(argv[i], "%d", &encodeConfig->endFrameIdx) != 1)
+            {
+                PRINTERR("invalid parameter for %s\n", argv[i - 1]);
+                return NV_ENC_ERR_INVALID_PARAM;
+            }
+        }
+        else if (stricmp(argv[i], "-rcmode") == 0)
+        {
+            if (++i >= argc || sscanf(argv[i], "%d", &encodeConfig->rcMode) != 1)
+            {
+                PRINTERR("invalid parameter for %s\n", argv[i - 1]);
+                return NV_ENC_ERR_INVALID_PARAM;
+            }
+        }
+        else if (stricmp(argv[i], "-goplength") == 0)
+        {
+            if (++i >= argc || sscanf(argv[i], "%d", &encodeConfig->gopLength) != 1)
+            {
+                PRINTERR("invalid parameter for %s\n", argv[i - 1]);
+                return NV_ENC_ERR_INVALID_PARAM;
+            }
+        }
+        else if (stricmp(argv[i], "-numB") == 0)
+        {
+            if (++i >= argc || sscanf(argv[i], "%d", &encodeConfig->numB) != 1)
+            {
+                PRINTERR("invalid parameter for %s\n", argv[i - 1]);
+                return NV_ENC_ERR_INVALID_PARAM;
+            }
+        }
+        else if (stricmp(argv[i], "-qp") == 0)
+        {
+            if (++i >= argc || sscanf(argv[i], "%d", &encodeConfig->qp) != 1)
+            {
+                PRINTERR("invalid parameter for %s\n", argv[i - 1]);
+                return NV_ENC_ERR_INVALID_PARAM;
+            }
+        }
+        else if (stricmp(argv[i], "-i_qfactor") == 0)
+        {
+            if (++i >= argc || sscanf(argv[i], "%f", &encodeConfig->i_quant_factor) != 1)
+            {
+                PRINTERR("invalid parameter for %s\n", argv[i - 1]);
+                return NV_ENC_ERR_INVALID_PARAM;
+            }
+        }
+        else if (stricmp(argv[i], "-b_qfactor") == 0)
+        {
+            if (++i >= argc || sscanf(argv[i], "%f", &encodeConfig->b_quant_factor) != 1)
+            {
+                PRINTERR("invalid parameter for %s\n", argv[i - 1]);
+                return NV_ENC_ERR_INVALID_PARAM;
+            }
+        }
+        else if (stricmp(argv[i], "-i_qoffset") == 0)
+        {
+            if (++i >= argc || sscanf(argv[i], "%f", &encodeConfig->i_quant_offset) != 1)
+            {
+                PRINTERR("invalid parameter for %s\n", argv[i - 1]);
+                return NV_ENC_ERR_INVALID_PARAM;
+            }
+        }
+        else if (stricmp(argv[i], "-b_qoffset") == 0)
+        {
+            if (++i >= argc || sscanf(argv[i], "%f", &encodeConfig->b_quant_offset) != 1)
+            {
+                PRINTERR("invalid parameter for %s\n", argv[i - 1]);
+                return NV_ENC_ERR_INVALID_PARAM;
+            }
+        }
+        else if (stricmp(argv[i], "-preset") == 0)
+        {
+            if (++i >= argc)
+            {
+                PRINTERR("invalid parameter for %s\n", argv[i - 1]);
+                return NV_ENC_ERR_INVALID_PARAM;
+            }
+            encodeConfig->encoderPreset = argv[i];
+        }
+        else if (stricmp(argv[i], "-devicetype") == 0)
+        {
+            if (++i >= argc || sscanf(argv[i], "%d", &encodeConfig->deviceType) != 1)
+            {
+                PRINTERR("invalid parameter for %s\n", argv[i - 1]);
+                return NV_ENC_ERR_INVALID_PARAM;
+            }
+        }
+        else if (stricmp(argv[i], "-codec") == 0)
+        {
+            if (++i >= argc || sscanf(argv[i], "%d", &encodeConfig->codec) != 1)
+            {
+                PRINTERR("invalid parameter for %s\n", argv[i - 1]);
+                return NV_ENC_ERR_INVALID_PARAM;
+            }
+        }
+        else if (stricmp(argv[i], "-encCmdFile") == 0)
+        {
+            if (++i >= argc)
+            {
+                PRINTERR("invalid parameter for %s\n", argv[i - 1]);
+                return NV_ENC_ERR_INVALID_PARAM;
+            }
+            encodeConfig->encCmdFileName = argv[i];
+        }
+        else if (stricmp(argv[i], "-intraRefresh") == 0)
+        {
+            if (++i >= argc || sscanf(argv[i], "%d", &encodeConfig->intraRefreshEnableFlag) != 1)
+            {
+                PRINTERR("invalid parameter for %s\n", argv[i - 1]);
+                return NV_ENC_ERR_INVALID_PARAM;
+            }
+        }
+        else if (stricmp(argv[i], "-intraRefreshPeriod") == 0)
+        {
+            if (++i >= argc || sscanf(argv[i], "%d", &encodeConfig->intraRefreshPeriod) != 1)
+            {
+                PRINTERR("invalid parameter for %s\n", argv[i - 1]);
+                return NV_ENC_ERR_INVALID_PARAM;
+            }
+        }
+        else if (stricmp(argv[i], "-intraRefreshDuration") == 0)
+        {
+            if (++i >= argc || sscanf(argv[i], "%d", &encodeConfig->intraRefreshDuration) != 1)
+            {
+                PRINTERR("invalid parameter for %s\n", argv[i - 1]);
+                return NV_ENC_ERR_INVALID_PARAM;
+            }
+        }
+        else if (stricmp(argv[i], "-picStruct") == 0)
+        {
+            if (++i >= argc || sscanf(argv[i], "%d", &encodeConfig->pictureStruct) != 1)
+            {
+                PRINTERR("invalid parameter for %s\n", argv[i - 1]);
+                return NV_ENC_ERR_INVALID_PARAM;
+            }
+        }
+        else if (stricmp(argv[i], "-deviceID") == 0)
+        {
+            if (++i >= argc || sscanf(argv[i], "%d", &encodeConfig->deviceID) != 1)
+            {
+                PRINTERR("invalid parameter for %s\n", argv[i - 1]);
+                return NV_ENC_ERR_INVALID_PARAM;
+            }
+        }
+        else if (stricmp(argv[i], "-inputFormat") == 0)
+        {
+            int inputFormatIndex = 0;
+            NV_ENC_BUFFER_FORMAT aFormatTable[] = { NV_ENC_BUFFER_FORMAT_NV12, NV_ENC_BUFFER_FORMAT_YUV444, NV_ENC_BUFFER_FORMAT_YUV420_10BIT, NV_ENC_BUFFER_FORMAT_YUV444_10BIT };
+            if (++i >= argc || sscanf(argv[i], "%d", &inputFormatIndex) != 1 || inputFormatIndex < 0 || inputFormatIndex >= (sizeof(aFormatTable) / sizeof(aFormatTable[0])))
+            {
+                PRINTERR("invalid parameter for %s\n", argv[i - 1]);
+                return NV_ENC_ERR_INVALID_PARAM;
+            }
+            encodeConfig->inputFormat = aFormatTable[inputFormatIndex];
+        }
+        else if (stricmp(argv[i], "-qpDeltaMapFile") == 0)
+        {
+            if (++i >= argc)
+            {
+                PRINTERR("invalid parameter for %s\n", argv[i - 1]);
+                return NV_ENC_ERR_INVALID_PARAM;
+            }
+            encodeConfig->qpDeltaMapFile = argv[i];
+        }
+        else if (stricmp(argv[i], "-meonly") == 0)
+        {
+            if (++i >= argc || sscanf(argv[i], "%d", &encodeConfig->enableMEOnly) != 1)
+            {
+                PRINTERR("invalid parameter for %s\n", argv[i - 1]);
+                return NV_ENC_ERR_INVALID_PARAM;
+            }
+            if (encodeConfig->enableMEOnly != 1 && encodeConfig->enableMEOnly != 2)
+            {
+                PRINTERR("invalid enableMEOnly value = %d (permissive value 1 and 2)\n", encodeConfig->enableMEOnly);
+                return NV_ENC_ERR_INVALID_PARAM;
+            }
+        }
+        else if (stricmp(argv[i], "-preloadedFrameCount") == 0)
+        {
+            if (++i >= argc || sscanf(argv[i], "%d", &encodeConfig->preloadedFrameCount) != 1)
+            {
+                PRINTERR("invalid parameter for %s\n", argv[i - 1]);
+                return NV_ENC_ERR_INVALID_PARAM;
+            }
+            if (encodeConfig->preloadedFrameCount <= 1)
+            {
+                PRINTERR("invalid preloadedFrameQueueSize value = %d (permissive value 2 and above)\n", encodeConfig->preloadedFrameCount);
+                return NV_ENC_ERR_INVALID_PARAM;
+            }
+        }
+        else if (stricmp(argv[i], "-temporalAQ") == 0)
+        {
+            if (++i >= argc || sscanf(argv[i], "%d", &encodeConfig->enableTemporalAQ) != 1)
+            {
+                PRINTERR("invalid parameter for %s\n", argv[i - 1]);
+                return NV_ENC_ERR_INVALID_PARAM;
+            }
+        }
+        else if (stricmp(argv[i], "-help") == 0)
+        {
+            return NV_ENC_ERR_INVALID_PARAM;
+        }
+        else
+        {
+            PRINTERR("invalid parameter  %s\n", argv[i++]);
+            return NV_ENC_ERR_INVALID_PARAM;
+        }
+    }
+
+    return NV_ENC_SUCCESS;
+}
diff --git a/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.cc b/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.cc
index 84bfafb..5111d5d 100644
--- a/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.cc
+++ b/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.cc
@@ -1,503 +1,1014 @@
-/*
- *  Copyright (c) 2015 The WebRTC project authors. All Rights Reserved.
- *
- *  Use of this source code is governed by a BSD-style license
- *  that can be found in the LICENSE file in the root of the source
- *  tree. An additional intellectual property rights grant can be found
- *  in the file PATENTS.  All contributing project authors may
- *  be found in the AUTHORS file in the root of the source tree.
- *
- */
-
-#include "webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.h"
-
-#include <limits>
-#include <string>
-
-#include "third_party/openh264/src/codec/api/svc/codec_api.h"
-#include "third_party/openh264/src/codec/api/svc/codec_app_def.h"
-#include "third_party/openh264/src/codec/api/svc/codec_def.h"
-#include "third_party/openh264/src/codec/api/svc/codec_ver.h"
-
-#include "webrtc/base/checks.h"
-#include "webrtc/base/logging.h"
-#include "webrtc/common_video/libyuv/include/webrtc_libyuv.h"
-#include "webrtc/media/base/mediaconstants.h"
-#include "webrtc/system_wrappers/include/metrics.h"
-
-namespace webrtc {
-
-namespace {
-
-const bool kOpenH264EncoderDetailedLogging = false;
-
-// Used by histograms. Values of entries should not be changed.
-enum H264EncoderImplEvent {
-  kH264EncoderEventInit = 0,
-  kH264EncoderEventError = 1,
-  kH264EncoderEventMax = 16,
-};
-
-int NumberOfThreads(int width, int height, int number_of_cores) {
-  // TODO(hbos): In Chromium, multiple threads do not work with sandbox on Mac,
-  // see crbug.com/583348. Until further investigated, only use one thread.
-//  if (width * height >= 1920 * 1080 && number_of_cores > 8) {
-//    return 8;  // 8 threads for 1080p on high perf machines.
-//  } else if (width * height > 1280 * 960 && number_of_cores >= 6) {
-//    return 3;  // 3 threads for 1080p.
-//  } else if (width * height > 640 * 480 && number_of_cores >= 3) {
-//    return 2;  // 2 threads for qHD/HD.
-//  } else {
-//    return 1;  // 1 thread for VGA or less.
-//  }
-// TODO(sprang): Also check sSliceArgument.uiSliceNum om GetEncoderPrams(),
-//               before enabling multithreading here.
-  return 1;
-}
-
-FrameType ConvertToVideoFrameType(EVideoFrameType type) {
-  switch (type) {
-    case videoFrameTypeIDR:
-      return kVideoFrameKey;
-    case videoFrameTypeSkip:
-    case videoFrameTypeI:
-    case videoFrameTypeP:
-    case videoFrameTypeIPMixed:
-      return kVideoFrameDelta;
-    case videoFrameTypeInvalid:
-      break;
-  }
-  RTC_NOTREACHED() << "Unexpected/invalid frame type: " << type;
-  return kEmptyFrame;
-}
-
-}  // namespace
-
-// Helper method used by H264EncoderImpl::Encode.
-// Copies the encoded bytes from |info| to |encoded_image| and updates the
-// fragmentation information of |frag_header|. The |encoded_image->_buffer| may
-// be deleted and reallocated if a bigger buffer is required.
-//
-// After OpenH264 encoding, the encoded bytes are stored in |info| spread out
-// over a number of layers and "NAL units". Each NAL unit is a fragment starting
-// with the four-byte start code {0,0,0,1}. All of this data (including the
-// start codes) is copied to the |encoded_image->_buffer| and the |frag_header|
-// is updated to point to each fragment, with offsets and lengths set as to
-// exclude the start codes.
-static void RtpFragmentize(EncodedImage* encoded_image,
-                           std::unique_ptr<uint8_t[]>* encoded_image_buffer,
-                           const VideoFrameBuffer& frame_buffer,
-                           SFrameBSInfo* info,
-                           RTPFragmentationHeader* frag_header) {
-  // Calculate minimum buffer size required to hold encoded data.
-  size_t required_size = 0;
-  size_t fragments_count = 0;
-  for (int layer = 0; layer < info->iLayerNum; ++layer) {
-    const SLayerBSInfo& layerInfo = info->sLayerInfo[layer];
-    for (int nal = 0; nal < layerInfo.iNalCount; ++nal, ++fragments_count) {
-      RTC_CHECK_GE(layerInfo.pNalLengthInByte[nal], 0);
-      // Ensure |required_size| will not overflow.
-      RTC_CHECK_LE(layerInfo.pNalLengthInByte[nal],
-                   std::numeric_limits<size_t>::max() - required_size);
-      required_size += layerInfo.pNalLengthInByte[nal];
-    }
-  }
-  if (encoded_image->_size < required_size) {
-    // Increase buffer size. Allocate enough to hold an unencoded image, this
-    // should be more than enough to hold any encoded data of future frames of
-    // the same size (avoiding possible future reallocation due to variations in
-    // required size).
-    encoded_image->_size =
-        CalcBufferSize(kI420, frame_buffer.width(), frame_buffer.height());
-    if (encoded_image->_size < required_size) {
-      // Encoded data > unencoded data. Allocate required bytes.
-      LOG(LS_WARNING) << "Encoding produced more bytes than the original image "
-                      << "data! Original bytes: " << encoded_image->_size
-                      << ", encoded bytes: " << required_size << ".";
-      encoded_image->_size = required_size;
-    }
-    encoded_image->_buffer = new uint8_t[encoded_image->_size];
-    encoded_image_buffer->reset(encoded_image->_buffer);
-  }
-
-  // Iterate layers and NAL units, note each NAL unit as a fragment and copy
-  // the data to |encoded_image->_buffer|.
-  const uint8_t start_code[4] = {0, 0, 0, 1};
-  frag_header->VerifyAndAllocateFragmentationHeader(fragments_count);
-  size_t frag = 0;
-  encoded_image->_length = 0;
-  for (int layer = 0; layer < info->iLayerNum; ++layer) {
-    const SLayerBSInfo& layerInfo = info->sLayerInfo[layer];
-    // Iterate NAL units making up this layer, noting fragments.
-    size_t layer_len = 0;
-    for (int nal = 0; nal < layerInfo.iNalCount; ++nal, ++frag) {
-      // Because the sum of all layer lengths, |required_size|, fits in a
-      // |size_t|, we know that any indices in-between will not overflow.
-      RTC_DCHECK_GE(layerInfo.pNalLengthInByte[nal], 4);
-      RTC_DCHECK_EQ(layerInfo.pBsBuf[layer_len+0], start_code[0]);
-      RTC_DCHECK_EQ(layerInfo.pBsBuf[layer_len+1], start_code[1]);
-      RTC_DCHECK_EQ(layerInfo.pBsBuf[layer_len+2], start_code[2]);
-      RTC_DCHECK_EQ(layerInfo.pBsBuf[layer_len+3], start_code[3]);
-      frag_header->fragmentationOffset[frag] =
-          encoded_image->_length + layer_len + sizeof(start_code);
-      frag_header->fragmentationLength[frag] =
-          layerInfo.pNalLengthInByte[nal] - sizeof(start_code);
-      layer_len += layerInfo.pNalLengthInByte[nal];
-    }
-    // Copy the entire layer's data (including start codes).
-    memcpy(encoded_image->_buffer + encoded_image->_length,
-           layerInfo.pBsBuf,
-           layer_len);
-    encoded_image->_length += layer_len;
-  }
-}
-
-H264EncoderImpl::H264EncoderImpl(const cricket::VideoCodec& codec)
-    : openh264_encoder_(nullptr),
-      width_(0),
-      height_(0),
-      max_frame_rate_(0.0f),
-      target_bps_(0),
-      max_bps_(0),
-      mode_(kRealtimeVideo),
-      frame_dropping_on_(false),
-      key_frame_interval_(0),
-      packetization_mode_(H264PacketizationMode::SingleNalUnit),
-      max_payload_size_(0),
-      number_of_cores_(0),
-      encoded_image_callback_(nullptr),
-      has_reported_init_(false),
-      has_reported_error_(false) {
-  RTC_CHECK(cricket::CodecNamesEq(codec.name, cricket::kH264CodecName));
-  std::string packetization_mode_string;
-  if (codec.GetParam(cricket::kH264FmtpPacketizationMode,
-                     &packetization_mode_string) &&
-      packetization_mode_string == "1") {
-    packetization_mode_ = H264PacketizationMode::NonInterleaved;
-  }
-}
-
-H264EncoderImpl::~H264EncoderImpl() {
-  Release();
-}
-
-int32_t H264EncoderImpl::InitEncode(const VideoCodec* codec_settings,
-                                    int32_t number_of_cores,
-                                    size_t max_payload_size) {
-  ReportInit();
-  if (!codec_settings ||
-      codec_settings->codecType != kVideoCodecH264) {
-    ReportError();
-    return WEBRTC_VIDEO_CODEC_ERR_PARAMETER;
-  }
-  if (codec_settings->maxFramerate == 0) {
-    ReportError();
-    return WEBRTC_VIDEO_CODEC_ERR_PARAMETER;
-  }
-  if (codec_settings->width < 1 || codec_settings->height < 1) {
-    ReportError();
-    return WEBRTC_VIDEO_CODEC_ERR_PARAMETER;
-  }
-
-  int32_t release_ret = Release();
-  if (release_ret != WEBRTC_VIDEO_CODEC_OK) {
-    ReportError();
-    return release_ret;
-  }
-  RTC_DCHECK(!openh264_encoder_);
-
-  // Create encoder.
-  if (WelsCreateSVCEncoder(&openh264_encoder_) != 0) {
-    // Failed to create encoder.
-    LOG(LS_ERROR) << "Failed to create OpenH264 encoder";
-    RTC_DCHECK(!openh264_encoder_);
-    ReportError();
-    return WEBRTC_VIDEO_CODEC_ERROR;
-  }
-  RTC_DCHECK(openh264_encoder_);
-  if (kOpenH264EncoderDetailedLogging) {
-    int trace_level = WELS_LOG_DETAIL;
-    openh264_encoder_->SetOption(ENCODER_OPTION_TRACE_LEVEL,
-                                 &trace_level);
-  }
-  // else WELS_LOG_DEFAULT is used by default.
-
-  number_of_cores_ = number_of_cores;
-  // Set internal settings from codec_settings
-  width_ = codec_settings->width;
-  height_ = codec_settings->height;
-  max_frame_rate_ = static_cast<float>(codec_settings->maxFramerate);
-  mode_ = codec_settings->mode;
-  frame_dropping_on_ = codec_settings->H264().frameDroppingOn;
-  key_frame_interval_ = codec_settings->H264().keyFrameInterval;
-  max_payload_size_ = max_payload_size;
-
-  // Codec_settings uses kbits/second; encoder uses bits/second.
-  max_bps_ = codec_settings->maxBitrate * 1000;
-  if (codec_settings->targetBitrate == 0)
-    target_bps_ = codec_settings->startBitrate * 1000;
-  else
-    target_bps_ = codec_settings->targetBitrate * 1000;
-
-  SEncParamExt encoder_params = CreateEncoderParams();
-
-  // Initialize.
-  if (openh264_encoder_->InitializeExt(&encoder_params) != 0) {
-    LOG(LS_ERROR) << "Failed to initialize OpenH264 encoder";
-    Release();
-    ReportError();
-    return WEBRTC_VIDEO_CODEC_ERROR;
-  }
-  // TODO(pbos): Base init params on these values before submitting.
-  int video_format = EVideoFormatType::videoFormatI420;
-  openh264_encoder_->SetOption(ENCODER_OPTION_DATAFORMAT,
-                               &video_format);
-
-  // Initialize encoded image. Default buffer size: size of unencoded data.
-  encoded_image_._size =
-      CalcBufferSize(kI420, codec_settings->width, codec_settings->height);
-  encoded_image_._buffer = new uint8_t[encoded_image_._size];
-  encoded_image_buffer_.reset(encoded_image_._buffer);
-  encoded_image_._completeFrame = true;
-  encoded_image_._encodedWidth = 0;
-  encoded_image_._encodedHeight = 0;
-  encoded_image_._length = 0;
-  return WEBRTC_VIDEO_CODEC_OK;
-}
-
-int32_t H264EncoderImpl::Release() {
-  if (openh264_encoder_) {
-    RTC_CHECK_EQ(0, openh264_encoder_->Uninitialize());
-    WelsDestroySVCEncoder(openh264_encoder_);
-    openh264_encoder_ = nullptr;
-  }
-  encoded_image_._buffer = nullptr;
-  encoded_image_buffer_.reset();
-  return WEBRTC_VIDEO_CODEC_OK;
-}
-
-int32_t H264EncoderImpl::RegisterEncodeCompleteCallback(
-    EncodedImageCallback* callback) {
-  encoded_image_callback_ = callback;
-  return WEBRTC_VIDEO_CODEC_OK;
-}
-
-int32_t H264EncoderImpl::SetRateAllocation(
-    const BitrateAllocation& bitrate_allocation,
-    uint32_t framerate) {
-  if (bitrate_allocation.get_sum_bps() <= 0 || framerate <= 0)
-    return WEBRTC_VIDEO_CODEC_ERR_PARAMETER;
-
-  target_bps_ = bitrate_allocation.get_sum_bps();
-  max_frame_rate_ = static_cast<float>(framerate);
-
-  SBitrateInfo target_bitrate;
-  memset(&target_bitrate, 0, sizeof(SBitrateInfo));
-  target_bitrate.iLayer = SPATIAL_LAYER_ALL,
-  target_bitrate.iBitrate = target_bps_;
-  openh264_encoder_->SetOption(ENCODER_OPTION_BITRATE,
-                               &target_bitrate);
-  openh264_encoder_->SetOption(ENCODER_OPTION_FRAME_RATE, &max_frame_rate_);
-  return WEBRTC_VIDEO_CODEC_OK;
-}
-
-int32_t H264EncoderImpl::Encode(const VideoFrame& input_frame,
-                                const CodecSpecificInfo* codec_specific_info,
-                                const std::vector<FrameType>* frame_types) {
-  if (!IsInitialized()) {
-    ReportError();
-    return WEBRTC_VIDEO_CODEC_UNINITIALIZED;
-  }
-  if (!encoded_image_callback_) {
-    LOG(LS_WARNING) << "InitEncode() has been called, but a callback function "
-                    << "has not been set with RegisterEncodeCompleteCallback()";
-    ReportError();
-    return WEBRTC_VIDEO_CODEC_UNINITIALIZED;
-  }
-
-  bool force_key_frame = false;
-  if (frame_types != nullptr) {
-    // We only support a single stream.
-    RTC_DCHECK_EQ(frame_types->size(), 1);
-    // Skip frame?
-    if ((*frame_types)[0] == kEmptyFrame) {
-      return WEBRTC_VIDEO_CODEC_OK;
-    }
-    // Force key frame?
-    force_key_frame = (*frame_types)[0] == kVideoFrameKey;
-  }
-  if (force_key_frame) {
-    // API doc says ForceIntraFrame(false) does nothing, but calling this
-    // function forces a key frame regardless of the |bIDR| argument's value.
-    // (If every frame is a key frame we get lag/delays.)
-    openh264_encoder_->ForceIntraFrame(true);
-  }
-  rtc::scoped_refptr<const VideoFrameBuffer> frame_buffer =
-      input_frame.video_frame_buffer();
-  // EncodeFrame input.
-  SSourcePicture picture;
-  memset(&picture, 0, sizeof(SSourcePicture));
-  picture.iPicWidth = frame_buffer->width();
-  picture.iPicHeight = frame_buffer->height();
-  picture.iColorFormat = EVideoFormatType::videoFormatI420;
-  picture.uiTimeStamp = input_frame.ntp_time_ms();
-  picture.iStride[0] = frame_buffer->StrideY();
-  picture.iStride[1] = frame_buffer->StrideU();
-  picture.iStride[2] = frame_buffer->StrideV();
-  picture.pData[0] = const_cast<uint8_t*>(frame_buffer->DataY());
-  picture.pData[1] = const_cast<uint8_t*>(frame_buffer->DataU());
-  picture.pData[2] = const_cast<uint8_t*>(frame_buffer->DataV());
-
-  // EncodeFrame output.
-  SFrameBSInfo info;
-  memset(&info, 0, sizeof(SFrameBSInfo));
-
-  // Encode!
-  int enc_ret = openh264_encoder_->EncodeFrame(&picture, &info);
-  if (enc_ret != 0) {
-    LOG(LS_ERROR) << "OpenH264 frame encoding failed, EncodeFrame returned "
-                  << enc_ret << ".";
-    ReportError();
-    return WEBRTC_VIDEO_CODEC_ERROR;
-  }
-
-  encoded_image_._encodedWidth = frame_buffer->width();
-  encoded_image_._encodedHeight = frame_buffer->height();
-  encoded_image_._timeStamp = input_frame.timestamp();
-  encoded_image_.ntp_time_ms_ = input_frame.ntp_time_ms();
-  encoded_image_.capture_time_ms_ = input_frame.render_time_ms();
-  encoded_image_.rotation_ = input_frame.rotation();
-  encoded_image_._frameType = ConvertToVideoFrameType(info.eFrameType);
-
-  // Split encoded image up into fragments. This also updates |encoded_image_|.
-  RTPFragmentationHeader frag_header;
-  RtpFragmentize(&encoded_image_, &encoded_image_buffer_, *frame_buffer, &info,
-                 &frag_header);
-
-  // Encoder can skip frames to save bandwidth in which case
-  // |encoded_image_._length| == 0.
-  if (encoded_image_._length > 0) {
-    // Parse QP.
-    h264_bitstream_parser_.ParseBitstream(encoded_image_._buffer,
-                                          encoded_image_._length);
-    h264_bitstream_parser_.GetLastSliceQp(&encoded_image_.qp_);
-
-    // Deliver encoded image.
-    CodecSpecificInfo codec_specific;
-    codec_specific.codecType = kVideoCodecH264;
-    codec_specific.codecSpecific.H264.packetization_mode = packetization_mode_;
-    encoded_image_callback_->OnEncodedImage(encoded_image_, &codec_specific,
-                                            &frag_header);
-  }
-  return WEBRTC_VIDEO_CODEC_OK;
-}
-
-const char* H264EncoderImpl::ImplementationName() const {
-  return "OpenH264";
-}
-
-bool H264EncoderImpl::IsInitialized() const {
-  return openh264_encoder_ != nullptr;
-}
-
-// Initialization parameters.
-// There are two ways to initialize. There is SEncParamBase (cleared with
-// memset(&p, 0, sizeof(SEncParamBase)) used in Initialize, and SEncParamExt
-// which is a superset of SEncParamBase (cleared with GetDefaultParams) used
-// in InitializeExt.
-SEncParamExt H264EncoderImpl::CreateEncoderParams() const {
-  RTC_DCHECK(openh264_encoder_);
-  SEncParamExt encoder_params;
-  openh264_encoder_->GetDefaultParams(&encoder_params);
-  if (mode_ == kRealtimeVideo) {
-    encoder_params.iUsageType = CAMERA_VIDEO_REAL_TIME;
-  } else if (mode_ == kScreensharing) {
-    encoder_params.iUsageType = SCREEN_CONTENT_REAL_TIME;
-  } else {
-    RTC_NOTREACHED();
-  }
-  encoder_params.iPicWidth = width_;
-  encoder_params.iPicHeight = height_;
-  encoder_params.iTargetBitrate = target_bps_;
-  encoder_params.iMaxBitrate = max_bps_;
-  // Rate Control mode
-  encoder_params.iRCMode = RC_BITRATE_MODE;
-  encoder_params.fMaxFrameRate = max_frame_rate_;
-
-  // The following parameters are extension parameters (they're in SEncParamExt,
-  // not in SEncParamBase).
-  encoder_params.bEnableFrameSkip = frame_dropping_on_;
-  // |uiIntraPeriod|    - multiple of GOP size
-  // |keyFrameInterval| - number of frames
-  encoder_params.uiIntraPeriod = key_frame_interval_;
-  encoder_params.uiMaxNalSize = 0;
-  // Threading model: use auto.
-  //  0: auto (dynamic imp. internal encoder)
-  //  1: single thread (default value)
-  // >1: number of threads
-  encoder_params.iMultipleThreadIdc = NumberOfThreads(
-      encoder_params.iPicWidth, encoder_params.iPicHeight, number_of_cores_);
-  // The base spatial layer 0 is the only one we use.
-  encoder_params.sSpatialLayers[0].iVideoWidth = encoder_params.iPicWidth;
-  encoder_params.sSpatialLayers[0].iVideoHeight = encoder_params.iPicHeight;
-  encoder_params.sSpatialLayers[0].fFrameRate = encoder_params.fMaxFrameRate;
-  encoder_params.sSpatialLayers[0].iSpatialBitrate =
-      encoder_params.iTargetBitrate;
-  encoder_params.sSpatialLayers[0].iMaxSpatialBitrate =
-      encoder_params.iMaxBitrate;
-  LOG(INFO) << "OpenH264 version is " << OPENH264_MAJOR << "."
-            << OPENH264_MINOR;
-  switch (packetization_mode_) {
-    case H264PacketizationMode::SingleNalUnit:
-      // Limit the size of the packets produced.
-      encoder_params.sSpatialLayers[0].sSliceArgument.uiSliceNum = 1;
-      encoder_params.sSpatialLayers[0].sSliceArgument.uiSliceMode =
-          SM_SIZELIMITED_SLICE;
-      encoder_params.sSpatialLayers[0].sSliceArgument.uiSliceSizeConstraint =
-          static_cast<unsigned int>(max_payload_size_);
-      break;
-    case H264PacketizationMode::NonInterleaved:
-      // When uiSliceMode = SM_FIXEDSLCNUM_SLICE, uiSliceNum = 0 means auto
-      // design it with cpu core number.
-      // TODO(sprang): Set to 0 when we understand why the rate controller borks
-      //               when uiSliceNum > 1.
-      encoder_params.sSpatialLayers[0].sSliceArgument.uiSliceNum = 1;
-      encoder_params.sSpatialLayers[0].sSliceArgument.uiSliceMode =
-          SM_FIXEDSLCNUM_SLICE;
-      break;
-  }
-  return encoder_params;
-}
-
-void H264EncoderImpl::ReportInit() {
-  if (has_reported_init_)
-    return;
-  RTC_HISTOGRAM_ENUMERATION("WebRTC.Video.H264EncoderImpl.Event",
-                            kH264EncoderEventInit,
-                            kH264EncoderEventMax);
-  has_reported_init_ = true;
-}
-
-void H264EncoderImpl::ReportError() {
-  if (has_reported_error_)
-    return;
-  RTC_HISTOGRAM_ENUMERATION("WebRTC.Video.H264EncoderImpl.Event",
-                            kH264EncoderEventError,
-                            kH264EncoderEventMax);
-  has_reported_error_ = true;
-}
-
-int32_t H264EncoderImpl::SetChannelParameters(
-    uint32_t packet_loss, int64_t rtt) {
-  return WEBRTC_VIDEO_CODEC_OK;
-}
-
-int32_t H264EncoderImpl::SetPeriodicKeyFrames(bool enable) {
-  return WEBRTC_VIDEO_CODEC_OK;
-}
-
-VideoEncoder::ScalingSettings H264EncoderImpl::GetScalingSettings() const {
-  return VideoEncoder::ScalingSettings(true);
-}
-
-}  // namespace webrtc
+/*
+ *  Copyright (c) 2015 The WebRTC project authors. All Rights Reserved.
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ *
+ */
+
+#include "webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.h"
+#include "webrtc/modules/video_coding/codecs/h264/h264_decoder_impl.h"
+#include "webrtc/common_video/h264/h264_common.h"
+#include "webrtc/base/win32socketserver.h"
+
+#include <limits>
+#include <string>
+
+#include "third_party/openh264/src/codec/api/svc/codec_api.h"
+#include "third_party/openh264/src/codec/api/svc/codec_app_def.h"
+#include "third_party/openh264/src/codec/api/svc/codec_def.h"
+#include "third_party/openh264/src/codec/api/svc/codec_ver.h"
+
+#include "webrtc/base/checks.h"
+#include "webrtc/base/logging.h"
+#include "webrtc/common_video/libyuv/include/webrtc_libyuv.h"
+#include "webrtc/media/base/mediaconstants.h"
+#include "webrtc/system_wrappers/include/metrics.h"
+
+#include <memory>
+#include <utility>
+#include <vector>
+#include <iostream>
+#include <fstream>
+#include "webrtc/base/thread.h"
+#include "webrtc/base/bind.h"
+#include "webrtc/base/asyncinvoker.h"
+
+namespace webrtc {
+
+namespace {
+
+// Used by histograms. Values of entries should not be changed.
+enum H264EncoderImplEvent {
+  kH264EncoderEventInit = 0,
+  kH264EncoderEventError = 1,
+  kH264EncoderEventMax = 16,
+};
+
+const bool kOpenH264EncoderDetailedLogging = false;
+
+int NumberOfThreads(int width, int height, int number_of_cores) {
+  // TODO(hbos): In Chromium, multiple threads do not work with sandbox on Mac,
+  // see crbug.com/583348. Until further investigated, only use one thread.
+//  if (width * height >= 1920 * 1080 && number_of_cores > 8) {
+//    return 8;  // 8 threads for 1080p on high perf machines.
+//  } else if (width * height > 1280 * 960 && number_of_cores >= 6) {
+//    return 3;  // 3 threads for 1080p.
+//  } else if (width * height > 640 * 480 && number_of_cores >= 3) {
+//    return 2;  // 2 threads for qHD/HD.
+//  } else {
+//    return 1;  // 1 thread for VGA or less.
+//  }
+// TODO(sprang): Also check sSliceArgument.uiSliceNum om GetEncoderPrams(),
+//               before enabling multithreading here.
+  return 1;
+}
+
+FrameType ConvertToVideoFrameType(EVideoFrameType type) {
+  switch (type) {
+    case videoFrameTypeIDR:
+      return kVideoFrameKey;
+    case videoFrameTypeSkip:
+    case videoFrameTypeI:
+    case videoFrameTypeP:
+    case videoFrameTypeIPMixed:
+      return kVideoFrameDelta;
+    case videoFrameTypeInvalid:
+      break;
+  }
+  RTC_NOTREACHED() << "Unexpected/invalid frame type: " << type;
+  return kEmptyFrame;
+}
+}  // namespace
+
+// Helper method used by H264EncoderImpl::Encode.
+// Copies the encoded bytes from |info| to |encoded_image| and updates the
+// fragmentation information of |frag_header|. The |encoded_image->_buffer| may
+// be deleted and reallocated if a bigger buffer is required.
+//
+// After OpenH264 encoding, the encoded bytes are stored in |info| spread out
+// over a number of layers and "NAL units". Each NAL unit is a fragment starting
+// with the four-byte start code {0,0,0,1}. All of this data (including the
+// start codes) is copied to the |encoded_image->_buffer| and the |frag_header|
+// is updated to point to each fragment, with offsets and lengths set as to
+// exclude the start codes.
+static void RtpFragmentize(EncodedImage* encoded_image,
+                           std::unique_ptr<uint8_t[]>* encoded_image_buffer,
+                           const VideoFrameBuffer& frame_buffer,
+                           SFrameBSInfo* info,
+                           RTPFragmentationHeader* frag_header) {
+  // Calculate minimum buffer size required to hold encoded data.
+  size_t required_size = 0;
+  size_t fragments_count = 0;
+  for (int layer = 0; layer < info->iLayerNum; ++layer) {
+    const SLayerBSInfo& layerInfo = info->sLayerInfo[layer];
+    for (int nal = 0; nal < layerInfo.iNalCount; ++nal, ++fragments_count) {
+      RTC_CHECK_GE(layerInfo.pNalLengthInByte[nal], 0);
+      // Ensure |required_size| will not overflow.
+      RTC_CHECK_LE(layerInfo.pNalLengthInByte[nal],
+                   std::numeric_limits<size_t>::max() - required_size);
+      required_size += layerInfo.pNalLengthInByte[nal];
+    }
+  }
+  if (encoded_image->_size < required_size) {
+    // Increase buffer size. Allocate enough to hold an unencoded image, this
+    // should be more than enough to hold any encoded data of future frames of
+    // the same size (avoiding possible future reallocation due to variations in
+    // required size).
+    encoded_image->_size =
+        CalcBufferSize(kI420, frame_buffer.width(), frame_buffer.height());
+    if (encoded_image->_size < required_size) {
+      // Encoded data > unencoded data. Allocate required bytes.
+      LOG(LS_WARNING) << "Encoding produced more bytes than the original image "
+                      << "data! Original bytes: " << encoded_image->_size
+                      << ", encoded bytes: " << required_size << ".";
+      encoded_image->_size = required_size;
+    }
+    encoded_image->_buffer = new uint8_t[encoded_image->_size];
+    encoded_image_buffer->reset(encoded_image->_buffer);
+  }
+
+  // Iterate layers and NAL units, note each NAL unit as a fragment and copy
+  // the data to |encoded_image->_buffer|.
+  const uint8_t start_code[4] = {0, 0, 0, 1};
+  frag_header->VerifyAndAllocateFragmentationHeader(fragments_count);
+  size_t frag = 0;
+  encoded_image->_length = 0;
+  for (int layer = 0; layer < info->iLayerNum; ++layer) {
+    const SLayerBSInfo& layerInfo = info->sLayerInfo[layer];
+    // Iterate NAL units making up this layer, noting fragments.
+    size_t layer_len = 0;
+    for (int nal = 0; nal < layerInfo.iNalCount; ++nal, ++frag) {
+      // Because the sum of all layer lengths, |required_size|, fits in a
+      // |size_t|, we know that any indices in-between will not overflow.
+      RTC_DCHECK_GE(layerInfo.pNalLengthInByte[nal], 4);
+      RTC_DCHECK_EQ(layerInfo.pBsBuf[layer_len+0], start_code[0]);
+      RTC_DCHECK_EQ(layerInfo.pBsBuf[layer_len+1], start_code[1]);
+      RTC_DCHECK_EQ(layerInfo.pBsBuf[layer_len+2], start_code[2]);
+      RTC_DCHECK_EQ(layerInfo.pBsBuf[layer_len+3], start_code[3]);
+      frag_header->fragmentationOffset[frag] =
+          encoded_image->_length + layer_len + sizeof(start_code);
+      frag_header->fragmentationLength[frag] =
+          layerInfo.pNalLengthInByte[nal] - sizeof(start_code);
+      layer_len += layerInfo.pNalLengthInByte[nal];
+    }
+    // Copy the entire layer's data (including start codes).
+    memcpy(encoded_image->_buffer + encoded_image->_length,
+           layerInfo.pBsBuf,
+           layer_len);
+    encoded_image->_length += layer_len;
+  }
+}
+
+ID3D11Device * webrtc::H264EncoderImpl::m_d3dDevice = nullptr;
+ID3D11DeviceContext * webrtc::H264EncoderImpl::m_d3dContext = nullptr;
+
+H264EncoderImpl::H264EncoderImpl(const cricket::VideoCodec& codec)
+	:
+	encoder_(nullptr),
+	number_of_cores_(0),
+	width_(0),
+	height_(0),
+	max_frame_rate_(0.0f),
+	target_bps_(0),
+	max_bps_(0),
+	mode_(kRealtimeVideo),
+	frame_dropping_on_(false),
+	m_use_software_encoding(true),
+	m_first_frame_sent(false),
+	m_pNvHWEncoder(NULL),
+	key_frame_interval_(0),
+	packetization_mode_(H264PacketizationMode::SingleNalUnit),
+	max_payload_size_(0),
+	encoded_image_callback_(nullptr),
+	has_reported_init_(false),
+	has_reported_error_(false) {
+	RTC_CHECK(cricket::CodecNamesEq(codec.name, cricket::kH264CodecName));
+	std::string packetization_mode_string;
+	if (codec.GetParam(cricket::kH264FmtpPacketizationMode,
+		&packetization_mode_string) &&
+		packetization_mode_string == "1") {
+		packetization_mode_ = H264PacketizationMode::NonInterleaved;
+	}
+
+	int useNvencode;
+	if (codec.GetParam(cricket::kH264UseHWNvencode,
+		&useNvencode) &&
+		useNvencode == 1) {
+		m_use_software_encoding = false;
+	}
+}
+
+H264EncoderImpl::~H264EncoderImpl() {
+  Release();
+}
+
+int32_t H264EncoderImpl::InitEncode(const VideoCodec* codec_settings,
+                                    int32_t number_of_cores,
+                                    size_t max_payload_size) {
+  ReportInit();
+  if (!codec_settings ||
+      codec_settings->codecType != kVideoCodecH264) {
+    ReportError();
+    return WEBRTC_VIDEO_CODEC_ERR_PARAMETER;
+  }
+  if (codec_settings->maxFramerate == 0) {
+    ReportError();
+    return WEBRTC_VIDEO_CODEC_ERR_PARAMETER;
+  }
+  if (codec_settings->width < 1 || codec_settings->height < 1) {
+    ReportError();
+    return WEBRTC_VIDEO_CODEC_ERR_PARAMETER;
+  }
+
+  Json::Reader reader;
+  Json::Value root = NULL;
+  auto encoderConfigPath = ExePath("nvEncConfig.json");
+  std::ifstream file(encoderConfigPath);
+  if (file.good())
+  {
+	  file >> root;
+	  reader.parse(file, root, true);
+
+	  if (root.isMember("useSoftwareEncoding")) {
+		  m_use_software_encoding = root.get("useSoftwareEncoding", false).asBool();
+	  }
+  }
+
+  int32_t release_ret = Release();
+  if (release_ret != WEBRTC_VIDEO_CODEC_OK) {
+    ReportError();
+    return release_ret;
+  }
+	RTC_DCHECK(!encoder_);
+
+	if (m_use_software_encoding)
+	{
+		//codec_settings
+		// Create encoder.
+		if (WelsCreateSVCEncoder(&encoder_) != 0) {
+			// Failed to create encoder.
+			LOG(LS_ERROR) << "Failed to create OpenH264 encoder";
+			RTC_DCHECK(!encoder_);
+			ReportError();
+			return WEBRTC_VIDEO_CODEC_ERROR;
+		}
+		RTC_DCHECK(encoder_);
+		if (kOpenH264EncoderDetailedLogging) {
+			int trace_level = WELS_LOG_DETAIL;
+			encoder_->SetOption(ENCODER_OPTION_TRACE_LEVEL,
+				&trace_level);
+		}
+		// else WELS_LOG_DEFAULT is used by default.
+		
+		number_of_cores_ = number_of_cores;
+		// Set internal settings from codec_settings
+		width_ = codec_settings->width;
+		height_ = codec_settings->height;
+		max_frame_rate_ = static_cast<float>(codec_settings->maxFramerate);
+		mode_ = codec_settings->mode;
+		frame_dropping_on_ = codec_settings->H264().frameDroppingOn;
+		key_frame_interval_ = codec_settings->H264().keyFrameInterval;
+		max_payload_size_ = max_payload_size;
+
+		// Codec_settings uses kbits/second; encoder uses bits/second.
+		max_bps_ = codec_settings->maxBitrate * 1000;
+		if (codec_settings->targetBitrate == 0)
+			target_bps_ = codec_settings->startBitrate * 1000;
+		else
+			target_bps_ = codec_settings->targetBitrate * 1000;
+
+		SEncParamExt encoder_params = CreateEncoderParams();
+
+		// Initialize.
+		if (encoder_->InitializeExt(&encoder_params) != 0) {
+			LOG(LS_ERROR) << "Failed to initialize OpenH264 encoder";
+			Release();
+			ReportError();
+			return WEBRTC_VIDEO_CODEC_ERROR;
+		}
+		// TODO(pbos): Base init params on these values before submitting.
+		int video_format = EVideoFormatType::videoFormatI420;
+		encoder_->SetOption(ENCODER_OPTION_DATAFORMAT,
+			&video_format);
+	}
+	else
+	{
+		packetization_mode_ = H264PacketizationMode::NonInterleaved;
+		m_first_frame_sent = false;
+
+		rtc::Win32Thread w32_thread;
+		rtc::ThreadManager::Instance()->SetCurrentThread(&w32_thread);
+
+		memset(&m_encodeConfig, 0, sizeof(EncodeConfig));
+
+		GetDefaultNvencodeConfig(m_encodeConfig, root);
+		m_encodeConfig.width = codec_settings->width;
+		m_encodeConfig.height = codec_settings->height;
+		m_pNvHWEncoder = new CNvHWEncoder();
+
+		m_pNvHWEncoder->Initialize((void*)m_d3dDevice, NV_ENC_DEVICE_TYPE_DIRECTX);
+		m_encodeConfig.presetGUID = m_pNvHWEncoder->GetPresetGUID(m_encodeConfig.encoderPreset, m_encodeConfig.codec);
+
+		m_pNvHWEncoder->m_stEncodeConfig.profileGUID = NV_ENC_PRESET_LOW_LATENCY_HQ_GUID;
+
+		//	//H264 level sets maximum bitrate limits.  4.1 supported by almost all mobile devices.
+		m_pNvHWEncoder->m_stEncodeConfig.encodeCodecConfig.h264Config.level = NV_ENC_LEVEL_H264_41;
+
+		// Creates the encoder.
+		m_pNvHWEncoder->CreateEncoder(&m_encodeConfig);
+		m_uEncodeBufferCount = 4;
+
+		AllocateIOBuffers(m_encodeConfig.width, m_encodeConfig.height);
+	}
+
+  // Initialize encoded image. Default buffer size: size of unencoded data.
+  encoded_image_._size =
+      CalcBufferSize(kI420, codec_settings->width, codec_settings->height);
+  encoded_image_._buffer = new uint8_t[encoded_image_._size];
+  encoded_image_buffer_.reset(encoded_image_._buffer);
+  encoded_image_._completeFrame = true;
+  encoded_image_._encodedWidth = 0;
+  encoded_image_._encodedHeight = 0;
+  encoded_image_._length = 0;
+  return WEBRTC_VIDEO_CODEC_OK;
+}
+
+int32_t H264EncoderImpl::Release() {
+	if (encoder_) {
+		RTC_CHECK_EQ(0, encoder_->Uninitialize());
+		WelsDestroySVCEncoder(encoder_);
+		encoder_ = nullptr;
+	}
+
+	if (m_pNvHWEncoder)
+	{
+		Deinitialize();
+
+		if (m_pNvHWEncoder)
+		{
+			delete m_pNvHWEncoder;
+			m_pNvHWEncoder = NULL;
+		}
+	}
+
+	encoded_image_._buffer = nullptr;
+	encoded_image_buffer_.reset();
+	return WEBRTC_VIDEO_CODEC_OK;
+}
+
+// Cleanup resources.
+NVENCSTATUS H264EncoderImpl::Deinitialize()
+{
+	NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+
+	if (!m_pNvHWEncoder)
+		return nvStatus;
+
+	FlushEncoder();
+	ReleaseIOBuffers();
+	nvStatus = m_pNvHWEncoder->NvEncDestroyEncoder();
+	return nvStatus;
+}
+
+NVENCSTATUS H264EncoderImpl::FlushEncoder()
+{
+	NVENCSTATUS nvStatus = m_pNvHWEncoder->NvEncFlushEncoderQueue(m_stEOSOutputBfr.hOutputEvent);
+	if (nvStatus != NV_ENC_SUCCESS)
+	{
+		assert(0);
+		return nvStatus;
+	}
+
+	EncodeBuffer *pEncodeBuffer = m_EncodeBufferQueue.GetPending();
+	while (pEncodeBuffer)
+	{
+		m_pNvHWEncoder->ProcessOutput(pEncodeBuffer);
+		pEncodeBuffer = m_EncodeBufferQueue.GetPending();
+
+		// UnMap the input buffer after frame is done.
+		if (pEncodeBuffer && pEncodeBuffer->stInputBfr.hInputSurface)
+		{
+			nvStatus = m_pNvHWEncoder->NvEncUnmapInputResource(pEncodeBuffer->stInputBfr.hInputSurface);
+			pEncodeBuffer->stInputBfr.hInputSurface = NULL;
+		}
+	}
+
+	if (WaitForSingleObject(m_stEOSOutputBfr.hOutputEvent, 500) != WAIT_OBJECT_0)
+	{
+		assert(0);
+		nvStatus = NV_ENC_ERR_GENERIC;
+	}
+
+	return nvStatus;
+}
+
+int32_t H264EncoderImpl::RegisterEncodeCompleteCallback(
+    EncodedImageCallback* callback) {
+  encoded_image_callback_ = callback;
+  return WEBRTC_VIDEO_CODEC_OK;
+}
+
+int32_t H264EncoderImpl::SetRateAllocation(
+    const BitrateAllocation& bitrate_allocation,
+    uint32_t framerate) {
+  if (bitrate_allocation.get_sum_bps() <= 0 || framerate <= 0)
+    return WEBRTC_VIDEO_CODEC_ERR_PARAMETER;
+
+  target_bps_ = bitrate_allocation.get_sum_bps();
+  max_frame_rate_ = static_cast<float>(framerate);
+
+  if (m_use_software_encoding)
+  {
+	  SBitrateInfo target_bitrate;
+	  memset(&target_bitrate, 0, sizeof(SBitrateInfo));
+	  target_bitrate.iLayer = SPATIAL_LAYER_ALL,
+		  target_bitrate.iBitrate = target_bps_;
+
+	  encoder_->SetOption(ENCODER_OPTION_BITRATE,
+		  &target_bitrate);
+	  encoder_->SetOption(ENCODER_OPTION_FRAME_RATE, &max_frame_rate_);
+  }
+  else
+  {
+	  m_encodeConfig.fps = max_frame_rate_;
+	  m_encodeConfig.bitrate = target_bps_;
+
+	  if (m_pNvHWEncoder != nullptr && m_encodeConfig.minBitrate < (int)target_bps_)
+	  {
+		  NvEncPictureCommand pEncPicCommand;
+		  pEncPicCommand.bBitrateChangePending = true;
+		  pEncPicCommand.bForceIDR = false;
+		  pEncPicCommand.bResolutionChangePending = false;
+		  pEncPicCommand.bInvalidateRefFrames = false;
+		  pEncPicCommand.bForceIntraRefresh = false;
+
+		  pEncPicCommand.newBitrate = m_encodeConfig.bitrate;
+		  pEncPicCommand.intraRefreshDuration = m_encodeConfig.fps;
+		  pEncPicCommand.newVBVSize = 0;
+
+		  m_pNvHWEncoder->NvEncReconfigureEncoder(&pEncPicCommand);
+	  }
+  }
+  return WEBRTC_VIDEO_CODEC_OK;
+}
+
+NVENCSTATUS H264EncoderImpl::SetNvencodeProfile(int profileIndex)
+{
+	GUID choice;
+	switch (profileIndex)
+	{
+		//Main should be used for most cases
+	case 1:
+		choice = NV_ENC_H264_PROFILE_MAIN_GUID;
+		break;
+
+		//Low latency and HQ
+	case 2:
+		choice = NV_ENC_PRESET_LOW_LATENCY_HQ_GUID;
+		break;
+
+		//Enables stereo frame packing
+	case 3:
+		choice = NV_ENC_H264_PROFILE_STEREO_GUID;
+		break;
+
+		//Fallback to auto, avoid this.
+	case 0:
+	default: choice = NV_ENC_CODEC_PROFILE_AUTOSELECT_GUID;
+		break;
+	}
+	return NV_ENC_SUCCESS;
+}
+
+void H264EncoderImpl::GetDefaultNvencodeConfig(EncodeConfig &nvEncodeConfig, Json::Value rootValue = NULL)
+{
+	//Populate with default values
+	{
+		nvEncodeConfig.startFrameIdx = 0;
+		nvEncodeConfig.bitrate = 3000000;
+		nvEncodeConfig.minBitrate = 3000000;
+		nvEncodeConfig.endFrameIdx = INT_MAX;
+		nvEncodeConfig.rcMode = NV_ENC_PARAMS_RC_CBR_LOWDELAY_HQ;
+		nvEncodeConfig.encoderPreset = "lowLatencyHQ";
+
+		//Infinite needed for low latency encoding.
+		nvEncodeConfig.gopLength = NVENC_INFINITE_GOPLENGTH;
+
+		//DeviceType 1 = Force CUDA.
+		nvEncodeConfig.deviceType = 0;
+
+		//Only supported codec for WebRTC.
+		nvEncodeConfig.codec = NV_ENC_H264;
+		nvEncodeConfig.fps = 60;
+
+		//Quantization Parameter - must be 0 for lossless.
+		nvEncodeConfig.qp = 5;
+
+		//Must be set to frame.
+		nvEncodeConfig.pictureStruct = NV_ENC_PIC_STRUCT_FRAME;
+
+		//Initial QP factors for bitrate spinup.
+		nvEncodeConfig.i_quant_factor = DEFAULT_I_QFACTOR;
+		nvEncodeConfig.b_quant_factor = DEFAULT_B_QFACTOR;
+		nvEncodeConfig.i_quant_offset = DEFAULT_I_QOFFSET;
+		nvEncodeConfig.b_quant_offset = DEFAULT_B_QOFFSET;
+		nvEncodeConfig.intraRefreshPeriod = 60;
+		nvEncodeConfig.intraRefreshEnableFlag = true;
+		nvEncodeConfig.intraRefreshDuration = 6;
+
+		// Enable temporal Adaptive Quantization
+		// Shifts quantization matrix based on complexity of frame over time
+		nvEncodeConfig.enableTemporalAQ = false;
+
+		//Need this to be able to recover from stream drops
+		//Client needs to send back a last good timestamp, and we call
+		//NvEncInvalidateRefFrames(encoder,timestamp) to reissue I frame
+		nvEncodeConfig.invalidateRefFramesEnableFlag = true;
+		//NV_ENC_PRESET_LOW_LATENCY_HP_GUID
+		SetNvencodeProfile(2);
+	}
+
+	if (rootValue != NULL && rootValue.isMember("NvencodeSettings"))
+	{
+		auto nvencodeRoot = rootValue.get("NvencodeSettings", NULL);
+		if (nvencodeRoot == NULL)
+			return;
+
+		if (nvencodeRoot.isMember("bitrate"))
+		{
+			nvEncodeConfig.bitrate = nvencodeRoot.get("bitrate", nvEncodeConfig.bitrate).asInt();
+		}
+
+		if (nvencodeRoot.isMember("minBitrate"))
+		{
+			nvEncodeConfig.minBitrate = nvencodeRoot.get("minBitrate", nvEncodeConfig.minBitrate).asInt();
+		}
+
+		if (nvencodeRoot.isMember("fps"))
+		{
+			nvEncodeConfig.fps = nvencodeRoot.get("fps", nvEncodeConfig.fps).asInt();
+		}
+
+		if (nvencodeRoot.isMember("qp"))
+		{
+			nvEncodeConfig.qp = nvencodeRoot.get("qp", nvEncodeConfig.qp).asInt();
+		}
+
+		if (nvencodeRoot.isMember("intraRefreshPeriod"))
+		{
+			nvEncodeConfig.intraRefreshPeriod = nvencodeRoot.get("intraRefreshPeriod", nvEncodeConfig.intraRefreshPeriod).asInt();
+		}
+
+		if (nvencodeRoot.isMember("intraRefreshEnableFlag"))
+		{
+			nvEncodeConfig.intraRefreshEnableFlag = nvencodeRoot.get("intraRefreshEnableFlag", nvEncodeConfig.intraRefreshEnableFlag).asBool();
+		}
+
+		if (nvencodeRoot.isMember("intraRefreshDuration"))
+		{
+			nvEncodeConfig.intraRefreshDuration = nvencodeRoot.get("intraRefreshDuration", nvEncodeConfig.intraRefreshDuration).asInt();
+		}
+
+		if (nvencodeRoot.isMember("enableTemporalAQ"))
+		{
+			nvEncodeConfig.enableTemporalAQ = nvencodeRoot.get("enableTemporalAQ", nvEncodeConfig.enableTemporalAQ).asBool();
+		}
+
+		if (nvencodeRoot.isMember("invalidateRefFramesEnableFlag"))
+		{
+			nvEncodeConfig.invalidateRefFramesEnableFlag = nvencodeRoot.get("invalidateRefFramesEnableFlag", nvEncodeConfig.invalidateRefFramesEnableFlag).asBool();
+		}
+
+		if (nvencodeRoot.isMember("nvEncodeProfile"))
+		{
+			auto profile = nvencodeRoot.get("nvEncodeProfile", 2).asInt();
+			SetNvencodeProfile(profile);
+		}
+	}
+}
+
+void H264EncoderImpl::Capture(ID3D11Texture2D* frameBuffer, bool forceIntra)
+{
+	if (!m_d3dContext || !m_pNvHWEncoder)
+		return;
+
+	// Try to process the pending input buffers.
+	NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+	EncodeBuffer* pEncodeBuffer = m_EncodeBufferQueue.GetAvailable();
+
+	if (!pEncodeBuffer)
+	{
+		pEncodeBuffer = m_EncodeBufferQueue.GetPending();
+		m_pNvHWEncoder->ProcessOutput(pEncodeBuffer);
+
+		// UnMap the input buffer after frame done
+		if (pEncodeBuffer->stInputBfr.hInputSurface)
+		{
+			nvStatus = m_pNvHWEncoder->NvEncUnmapInputResource(pEncodeBuffer->stInputBfr.hInputSurface);
+			pEncodeBuffer->stInputBfr.hInputSurface = NULL;
+		}
+
+		pEncodeBuffer = m_EncodeBufferQueue.GetAvailable();
+	}
+
+	// Copies the frame buffer to the encode input buffer.
+	m_d3dContext->CopyResource(pEncodeBuffer->stInputBfr.pARGBSurface, frameBuffer);
+	frameBuffer->Release();
+	nvStatus = m_pNvHWEncoder->NvEncMapInputResource(pEncodeBuffer->stInputBfr.nvRegisteredResource, &pEncodeBuffer->stInputBfr.hInputSurface);
+	if (nvStatus != NV_ENC_SUCCESS)
+	{
+		PRINTERR("Failed to Map input buffer %p\n", pEncodeBuffer->stInputBfr.hInputSurface);
+		return;
+	}
+
+	NvEncPictureCommand pEncPicCommand;
+	pEncPicCommand.bForceIntraRefresh = forceIntra;
+	pEncPicCommand.bForceIDR = forceIntra;
+	pEncPicCommand.intraRefreshDuration = m_encodeConfig.intraRefreshDuration;
+
+	nvStatus = m_pNvHWEncoder->NvEncEncodeFrame(pEncodeBuffer, forceIntra ? &pEncPicCommand : nullptr, m_encodeConfig.width, m_encodeConfig.height);
+	if (nvStatus != NV_ENC_SUCCESS  && nvStatus != NV_ENC_ERR_NEED_MORE_INPUT)
+	{
+		return;
+	}
+}
+
+NVENCSTATUS H264EncoderImpl::AllocateIOBuffers(uint32_t uInputWidth, uint32_t uInputHeight)
+{
+	ID3D11Texture2D* pVPSurfaces[16];
+
+	// Initializes the encode buffer queue.
+	m_EncodeBufferQueue.Initialize(m_stEncodeBuffer, m_uEncodeBufferCount);
+
+	// Finds the suitable format for buffer.
+	DXGI_FORMAT format = DXGI_FORMAT_R8G8B8A8_UNORM;
+
+	for (uint32_t i = 0; i < m_uEncodeBufferCount; i++)
+	{
+		// Initializes the input buffer, backed by ID3D11Texture2D*.
+		D3D11_TEXTURE2D_DESC desc = { 0 };
+		desc.ArraySize = 1;
+		desc.Format = format;
+		desc.Width = uInputWidth;
+		desc.Height = uInputHeight;
+		desc.MipLevels = 1;
+		desc.SampleDesc.Count = 1;
+		desc.Usage = D3D11_USAGE_DEFAULT;
+		m_d3dDevice->CreateTexture2D(&desc, nullptr, &pVPSurfaces[i]);
+
+		// Registers the input buffer with NvEnc.
+		m_pNvHWEncoder->NvEncRegisterResource(
+			NV_ENC_INPUT_RESOURCE_TYPE_DIRECTX,
+			(void*)pVPSurfaces[i],
+			uInputWidth,
+			uInputHeight,
+			m_stEncodeBuffer[i].stInputBfr.uARGBStride,
+			&m_stEncodeBuffer[i].stInputBfr.nvRegisteredResource);
+
+		// Maps the buffer format to the relevant NvEnc encoder format
+		switch (format)
+		{
+		case DXGI_FORMAT_B8G8R8A8_UNORM:
+			m_stEncodeBuffer[i].stInputBfr.bufferFmt = NV_ENC_BUFFER_FORMAT_ARGB;
+			break;
+
+		case DXGI_FORMAT_R10G10B10A2_UNORM:
+			if (m_pNvHWEncoder->m_stEncodeConfig.encodeCodecConfig.h264Config.qpPrimeYZeroTransformBypassFlag == 1)
+				m_stEncodeBuffer[i].stInputBfr.bufferFmt = NV_ENC_BUFFER_FORMAT_YUV444_10BIT;
+			else
+				m_stEncodeBuffer[i].stInputBfr.bufferFmt = NV_ENC_BUFFER_FORMAT_YUV420_10BIT;
+			break;
+
+		default:
+			m_stEncodeBuffer[i].stInputBfr.bufferFmt = NV_ENC_BUFFER_FORMAT_ABGR;
+			break;
+		}
+
+		m_stEncodeBuffer[i].stInputBfr.dwWidth = uInputWidth;
+		m_stEncodeBuffer[i].stInputBfr.dwHeight = uInputHeight;
+		m_stEncodeBuffer[i].stInputBfr.pARGBSurface = pVPSurfaces[i];
+
+		// Initializes the output buffer.
+		m_pNvHWEncoder->NvEncCreateBitstreamBuffer(2 * 1024 * 1024, &m_stEncodeBuffer[i].stOutputBfr.hBitstreamBuffer);
+		m_stEncodeBuffer[i].stOutputBfr.dwBitstreamBufferSize = 2 * 1024 * 1024;
+
+		// Registers for the output event.
+		m_pNvHWEncoder->NvEncRegisterAsyncEvent(&m_stEncodeBuffer[i].stOutputBfr.hOutputEvent);
+		m_stEncodeBuffer[i].stOutputBfr.bWaitOnEvent = true;
+	}
+
+	m_stEOSOutputBfr.bEOSFlag = TRUE;
+
+	// Registers for the output event.
+	m_pNvHWEncoder->NvEncRegisterAsyncEvent(&m_stEOSOutputBfr.hOutputEvent);
+
+	return NV_ENC_SUCCESS;
+}
+
+NVENCSTATUS H264EncoderImpl::ReleaseIOBuffers()
+{
+	for (uint32_t i = 0; i < m_uEncodeBufferCount; i++)
+	{
+		if (m_stEncodeBuffer[i].stInputBfr.pARGBSurface)
+		{
+			m_stEncodeBuffer[i].stInputBfr.pARGBSurface->Release();
+			m_stEncodeBuffer[i].stInputBfr.pARGBSurface = nullptr;
+		}
+
+		m_pNvHWEncoder->NvEncDestroyBitstreamBuffer(m_stEncodeBuffer[i].stOutputBfr.hBitstreamBuffer);
+		m_stEncodeBuffer[i].stOutputBfr.hBitstreamBuffer = NULL;
+
+		m_pNvHWEncoder->NvEncUnregisterAsyncEvent(m_stEncodeBuffer[i].stOutputBfr.hOutputEvent);
+		CloseHandle(m_stEncodeBuffer[i].stOutputBfr.hOutputEvent);
+		m_stEncodeBuffer[i].stOutputBfr.hOutputEvent = NULL;
+	}
+
+	if (m_stEOSOutputBfr.hOutputEvent)
+	{
+		m_pNvHWEncoder->NvEncUnregisterAsyncEvent(m_stEOSOutputBfr.hOutputEvent);
+		CloseHandle(m_stEOSOutputBfr.hOutputEvent);
+		m_stEOSOutputBfr.hOutputEvent = NULL;
+	}
+
+	return NV_ENC_SUCCESS;
+}
+
+// Captures encoded frames from NvEncoder.
+void H264EncoderImpl::GetEncodedFrame(void** buffer, int* size, _NV_ENC_PIC_TYPE* keyFrameType)
+{
+	*buffer = m_pNvHWEncoder->m_lockBitstreamData.bitstreamBufferPtr;
+	*size = m_pNvHWEncoder->m_lockBitstreamData.bitstreamSizeInBytes;
+	*keyFrameType = m_pNvHWEncoder->m_lockBitstreamData.pictureType;
+}
+
+int32_t H264EncoderImpl::Encode(const VideoFrame& input_frame,
+	const CodecSpecificInfo* codec_specific_info,
+	const std::vector<FrameType>* frame_types) {
+
+	rtc::scoped_refptr<const VideoFrameBuffer> frame_buffer = input_frame.video_frame_buffer();
+	SFrameBSInfo info;
+	RTPFragmentationHeader frag_header;
+
+	if (m_use_software_encoding && !IsInitialized())
+	{
+		ReportError();
+		return WEBRTC_VIDEO_CODEC_UNINITIALIZED;
+	}
+
+	if (!encoded_image_callback_) {
+		LOG(LS_WARNING) << "InitEncode() has been called, but a callback function "
+			<< "has not been set with RegisterEncodeCompleteCallback()";
+		ReportError();
+		return WEBRTC_VIDEO_CODEC_UNINITIALIZED;
+	}
+
+	bool force_key_frame = false;
+	if (frame_types != nullptr) {
+		// We only support a single stream.
+		RTC_DCHECK_EQ(frame_types->size(), 1);
+		// Skip frame?
+		if ((*frame_types)[0] == kEmptyFrame) {
+			return WEBRTC_VIDEO_CODEC_OK;
+		}
+		// Force key frame?
+		force_key_frame = (*frame_types)[0] == kVideoFrameKey;
+	}
+	if (force_key_frame) {
+		// API doc says ForceIntraFrame(false) does nothing, but calling this
+		// function forces a key frame regardless of the |bIDR| argument's value.
+		// (If every frame is a key frame we get lag/delays.)
+		if (m_use_software_encoding)
+		{
+			encoder_->ForceIntraFrame(true);
+		}
+	}
+
+	if (m_use_software_encoding)
+	{
+		// EncodeFrame input.
+		SSourcePicture picture;
+		memset(&picture, 0, sizeof(SSourcePicture));
+		picture.iPicWidth = frame_buffer->width();
+		picture.iPicHeight = frame_buffer->height();
+		picture.iColorFormat = EVideoFormatType::videoFormatI420;
+		picture.uiTimeStamp = input_frame.ntp_time_ms();
+		picture.iStride[0] = frame_buffer->StrideY();
+		picture.iStride[1] = frame_buffer->StrideU();
+		picture.iStride[2] = frame_buffer->StrideV();
+		picture.pData[0] = const_cast<uint8_t*>(frame_buffer->DataY());
+		picture.pData[1] = const_cast<uint8_t*>(frame_buffer->DataU());
+		picture.pData[2] = const_cast<uint8_t*>(frame_buffer->DataV());
+
+		// EncodeFrame output.
+		memset(&info, 0, sizeof(SFrameBSInfo));
+
+		// Encode!
+		int enc_ret = encoder_->EncodeFrame(&picture, &info);
+		if (enc_ret != 0) {
+			LOG(LS_ERROR) << "OpenH264 frame encoding failed, EncodeFrame returned "
+				<< enc_ret << ".";
+			ReportError();
+			return WEBRTC_VIDEO_CODEC_ERROR;
+		}
+
+		encoded_image_._frameType = ConvertToVideoFrameType(info.eFrameType);
+	}
+	else
+	{
+		void* pFrameBuffer = nullptr;
+		int frameSizeInBytes = 0;
+		_NV_ENC_PIC_TYPE frameType;
+		auto texture = input_frame.GetID3D11Texture2D();
+		if (texture == nullptr)
+			return WEBRTC_VIDEO_CODEC_OK;
+
+		// Force a key frame until we send the first one.
+		if (!m_first_frame_sent) force_key_frame = true;
+
+		size_t i_nal = 0;
+		Capture(texture, force_key_frame);
+		GetEncodedFrame(&pFrameBuffer, &frameSizeInBytes, &frameType);
+		if (frameSizeInBytes < 1 || frameSizeInBytes >= 100000000 || frameType == NV_ENC_PIC_TYPE_SKIPPED || frameType == NV_ENC_PIC_TYPE_UNKNOWN)
+			return WEBRTC_VIDEO_CODEC_OK;
+
+		if (!m_first_frame_sent) m_first_frame_sent = true;
+
+		auto p_nal = (uint8_t*)pFrameBuffer;
+		std::vector<H264::NaluIndex> NALUidx;
+
+		NALUidx = H264::FindNaluIndices(p_nal, frameSizeInBytes);
+		if (NALUidx.size() < 1)
+			return WEBRTC_VIDEO_CODEC_OK;
+
+		i_nal = NALUidx.size();
+		if (i_nal == 1)
+		{
+			NALUidx[0].payload_size = frameSizeInBytes - NALUidx[0].payload_start_offset;
+		}
+		else for (size_t i = 0; i < i_nal; i++)
+		{
+			NALUidx[i].payload_size = i + 1 >= i_nal ? frameSizeInBytes - NALUidx[i].payload_start_offset : NALUidx[i + 1].start_offset - NALUidx[i].payload_start_offset;
+		}
+
+		frag_header.VerifyAndAllocateFragmentationHeader(i_nal);
+		encoded_image_.qp_ = m_encodeConfig.qp;
+		encoded_image_._frameType = frameType == NV_ENC_PIC_TYPE_IDR ? kVideoFrameKey : kVideoFrameDelta;
+
+		uint32_t totalNaluIndex = 0;
+		for (size_t nal_index = 0; nal_index < i_nal; nal_index++)
+		{
+			size_t currentNaluSize = 0;
+			currentNaluSize = NALUidx[nal_index].payload_size; //i_frame_size
+
+			frag_header.fragmentationOffset[totalNaluIndex] = NALUidx[nal_index].payload_start_offset;
+			frag_header.fragmentationLength[totalNaluIndex] = currentNaluSize;
+			frag_header.fragmentationPlType[totalNaluIndex] = H264::ParseNaluType(p_nal[NALUidx[nal_index].payload_start_offset]);
+			frag_header.fragmentationTimeDiff[totalNaluIndex] = 0;
+			totalNaluIndex++;
+		}
+
+		memcpy(encoded_image_._buffer, p_nal, frameSizeInBytes);
+		encoded_image_._length = frameSizeInBytes;
+	}
+
+	encoded_image_._encodedWidth = frame_buffer->width();
+	encoded_image_._encodedHeight = frame_buffer->height();
+	encoded_image_._timeStamp = input_frame.timestamp();
+	encoded_image_.ntp_time_ms_ = input_frame.ntp_time_ms();
+	encoded_image_.capture_time_ms_ = input_frame.render_time_ms();
+	encoded_image_.rotation_ = input_frame.rotation();
+
+	// Split encoded image up into fragments. This also updates |encoded_image_|.
+	if (m_use_software_encoding)
+	{
+		RtpFragmentize(&encoded_image_, &encoded_image_buffer_, *frame_buffer, &info,
+			&frag_header);
+	}
+
+	// Encoder can skip frames to save bandwidth in which case
+	// |encoded_image_._length| == 0.
+	if (encoded_image_._length > 0) {
+
+		// Deliver encoded image.
+		CodecSpecificInfo codec_specific;
+		codec_specific.codecType = kVideoCodecH264;
+		codec_specific.codecSpecific.H264.packetization_mode = H264PacketizationMode::NonInterleaved;
+		encoded_image_callback_->OnEncodedImage(encoded_image_, &codec_specific,
+			&frag_header);
+	}
+	return WEBRTC_VIDEO_CODEC_OK;
+}
+
+const char* H264EncoderImpl::ImplementationName() const {
+  return "OpenH264";
+}
+
+bool H264EncoderImpl::IsInitialized() const {
+	return encoder_ != nullptr;
+}
+
+// Initialization parameters.
+// There are two ways to initialize. There is SEncParamBase (cleared with
+// memset(&p, 0, sizeof(SEncParamBase)) used in Initialize, and SEncParamExt
+// which is a superset of SEncParamBase (cleared with GetDefaultParams) used
+// in InitializeExt.
+SEncParamExt H264EncoderImpl::CreateEncoderParams() const {
+  RTC_DCHECK(encoder_);
+  SEncParamExt encoder_params;
+  encoder_->GetDefaultParams(&encoder_params);
+  if (mode_ == kRealtimeVideo) {
+    encoder_params.iUsageType = CAMERA_VIDEO_REAL_TIME;
+  } else if (mode_ == kScreensharing) {
+    encoder_params.iUsageType = SCREEN_CONTENT_REAL_TIME;
+  } else {
+    RTC_NOTREACHED();
+  }
+  encoder_params.iPicWidth = width_;
+  encoder_params.iPicHeight = height_;
+  encoder_params.iTargetBitrate = target_bps_;
+  encoder_params.iMaxBitrate = max_bps_;
+  // Rate Control mode
+  encoder_params.iRCMode = RC_BITRATE_MODE;
+  encoder_params.fMaxFrameRate = max_frame_rate_;
+
+  // The following parameters are extension parameters (they're in SEncParamExt,
+  // not in SEncParamBase).
+  encoder_params.bEnableFrameSkip = frame_dropping_on_;
+  // |uiIntraPeriod|    - multiple of GOP size
+  // |keyFrameInterval| - number of frames
+  encoder_params.uiIntraPeriod = key_frame_interval_;
+  encoder_params.uiMaxNalSize = 0;
+  // Threading model: use auto.
+  //  0: auto (dynamic imp. internal encoder)
+  //  1: single thread (default value)
+  // >1: number of threads
+  encoder_params.iMultipleThreadIdc = NumberOfThreads(
+      encoder_params.iPicWidth, encoder_params.iPicHeight, number_of_cores_);
+  // The base spatial layer 0 is the only one we use.
+  encoder_params.sSpatialLayers[0].iVideoWidth = encoder_params.iPicWidth;
+  encoder_params.sSpatialLayers[0].iVideoHeight = encoder_params.iPicHeight;
+  encoder_params.sSpatialLayers[0].fFrameRate = encoder_params.fMaxFrameRate;
+  encoder_params.sSpatialLayers[0].iSpatialBitrate =
+      encoder_params.iTargetBitrate;
+  encoder_params.sSpatialLayers[0].iMaxSpatialBitrate =
+      encoder_params.iMaxBitrate;
+  LOG(INFO) << "OpenH264 version is " << OPENH264_MAJOR << "."
+            << OPENH264_MINOR;
+  switch (packetization_mode_) {
+    case H264PacketizationMode::SingleNalUnit:
+      // Limit the size of the packets produced.
+      encoder_params.sSpatialLayers[0].sSliceArgument.uiSliceNum = 1;
+      encoder_params.sSpatialLayers[0].sSliceArgument.uiSliceMode =
+          SM_SIZELIMITED_SLICE;
+      encoder_params.sSpatialLayers[0].sSliceArgument.uiSliceSizeConstraint =
+          static_cast<unsigned int>(max_payload_size_);
+      break;
+    case H264PacketizationMode::NonInterleaved:
+      // When uiSliceMode = SM_FIXEDSLCNUM_SLICE, uiSliceNum = 0 means auto
+      // design it with cpu core number.
+      // TODO(sprang): Set to 0 when we understand why the rate controller borks
+      //               when uiSliceNum > 1.
+      encoder_params.sSpatialLayers[0].sSliceArgument.uiSliceNum = 1;
+      encoder_params.sSpatialLayers[0].sSliceArgument.uiSliceMode =
+          SM_FIXEDSLCNUM_SLICE;
+      break;
+  }
+  return encoder_params;
+}
+
+void H264EncoderImpl::ReportInit() {
+  if (has_reported_init_)
+    return;
+  RTC_HISTOGRAM_ENUMERATION("WebRTC.Video.H264EncoderImpl.Event",
+                            kH264EncoderEventInit,
+                            kH264EncoderEventMax);
+  has_reported_init_ = true;
+}
+
+void H264EncoderImpl::ReportError() {
+  if (has_reported_error_)
+    return;
+  RTC_HISTOGRAM_ENUMERATION("WebRTC.Video.H264EncoderImpl.Event",
+                            kH264EncoderEventError,
+                            kH264EncoderEventMax);
+  has_reported_error_ = true;
+}
+
+int32_t H264EncoderImpl::SetChannelParameters(
+    uint32_t packet_loss, int64_t rtt) {
+  return WEBRTC_VIDEO_CODEC_OK;
+}
+
+int32_t H264EncoderImpl::SetPeriodicKeyFrames(bool enable) {
+  return WEBRTC_VIDEO_CODEC_OK;
+}
+
+VideoEncoder::ScalingSettings H264EncoderImpl::GetScalingSettings() const {
+  return VideoEncoder::ScalingSettings(true);
+}
+
+}  // namespace webrtc
diff --git a/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.h b/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.h
index a455259..d2ede06 100644
--- a/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.h
+++ b/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.h
@@ -1,104 +1,221 @@
-/*
- *  Copyright (c) 2015 The WebRTC project authors. All Rights Reserved.
- *
- *  Use of this source code is governed by a BSD-style license
- *  that can be found in the LICENSE file in the root of the source
- *  tree. An additional intellectual property rights grant can be found
- *  in the file PATENTS.  All contributing project authors may
- *  be found in the AUTHORS file in the root of the source tree.
- *
- */
-
-#ifndef WEBRTC_MODULES_VIDEO_CODING_CODECS_H264_H264_ENCODER_IMPL_H_
-#define WEBRTC_MODULES_VIDEO_CODING_CODECS_H264_H264_ENCODER_IMPL_H_
-
-#include <memory>
-#include <vector>
-
-#include "webrtc/common_video/h264/h264_bitstream_parser.h"
-#include "webrtc/modules/video_coding/codecs/h264/include/h264.h"
-#include "webrtc/modules/video_coding/utility/quality_scaler.h"
-
-#include "third_party/openh264/src/codec/api/svc/codec_app_def.h"
-
-class ISVCEncoder;
-
-namespace webrtc {
-
-class H264EncoderImpl : public H264Encoder {
- public:
-  explicit H264EncoderImpl(const cricket::VideoCodec& codec);
-  ~H264EncoderImpl() override;
-
-  // |max_payload_size| is ignored.
-  // The following members of |codec_settings| are used. The rest are ignored.
-  // - codecType (must be kVideoCodecH264)
-  // - targetBitrate
-  // - maxFramerate
-  // - width
-  // - height
-  int32_t InitEncode(const VideoCodec* codec_settings,
-                     int32_t number_of_cores,
-                     size_t max_payload_size) override;
-  int32_t Release() override;
-
-  int32_t RegisterEncodeCompleteCallback(
-      EncodedImageCallback* callback) override;
-  int32_t SetRateAllocation(const BitrateAllocation& bitrate_allocation,
-                            uint32_t framerate) override;
-
-  // The result of encoding - an EncodedImage and RTPFragmentationHeader - are
-  // passed to the encode complete callback.
-  int32_t Encode(const VideoFrame& frame,
-                 const CodecSpecificInfo* codec_specific_info,
-                 const std::vector<FrameType>* frame_types) override;
-
-  const char* ImplementationName() const override;
-
-  VideoEncoder::ScalingSettings GetScalingSettings() const override;
-
-  // Unsupported / Do nothing.
-  int32_t SetChannelParameters(uint32_t packet_loss, int64_t rtt) override;
-  int32_t SetPeriodicKeyFrames(bool enable) override;
-
-  // Exposed for testing.
-  H264PacketizationMode PacketizationModeForTesting() const {
-    return packetization_mode_;
-  }
-
- private:
-  bool IsInitialized() const;
-  SEncParamExt CreateEncoderParams() const;
-
-  webrtc::H264BitstreamParser h264_bitstream_parser_;
-  // Reports statistics with histograms.
-  void ReportInit();
-  void ReportError();
-
-  ISVCEncoder* openh264_encoder_;
-  // Settings that are used by this encoder.
-  int width_;
-  int height_;
-  float max_frame_rate_;
-  uint32_t target_bps_;
-  uint32_t max_bps_;
-  VideoCodecMode mode_;
-  // H.264 specifc parameters
-  bool frame_dropping_on_;
-  int key_frame_interval_;
-  H264PacketizationMode packetization_mode_;
-
-  size_t max_payload_size_;
-  int32_t number_of_cores_;
-
-  EncodedImage encoded_image_;
-  std::unique_ptr<uint8_t[]> encoded_image_buffer_;
-  EncodedImageCallback* encoded_image_callback_;
-
-  bool has_reported_init_;
-  bool has_reported_error_;
-};
-
-}  // namespace webrtc
-
-#endif  // WEBRTC_MODULES_VIDEO_CODING_CODECS_H264_H264_ENCODER_IMPL_H_
+/*
+ *  Copyright (c) 2015 The WebRTC project authors. All Rights Reserved.
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ *
+ */
+
+#ifndef WEBRTC_MODULES_VIDEO_CODING_CODECS_H264_H264_ENCODER_IMPL_H_
+#define WEBRTC_MODULES_VIDEO_CODING_CODECS_H264_H264_ENCODER_IMPL_H_
+
+#include <memory>
+#include <vector>
+
+#include "webrtc/common_video/h264/h264_bitstream_parser.h"
+#include "webrtc/modules/video_coding/codecs/h264/include/h264.h"
+#include "webrtc/modules/video_coding/codecs/h264/include/NvHWEncoder.h"
+#include "webrtc/modules/video_coding/utility/quality_scaler.h"
+#include "third_party/jsoncpp/source/include/json/json.h"
+
+
+#include "third_party/openh264/src/codec/api/svc/codec_app_def.h"
+class ISVCEncoder;
+
+namespace webrtc {
+
+	static std::string ExePath(std::string fileName = "") {
+		TCHAR buffer[MAX_PATH];
+		GetModuleFileName(NULL, buffer, MAX_PATH);
+		char charPath[MAX_PATH];
+		wcstombs(charPath, buffer, wcslen(buffer) + 1);
+
+		std::string::size_type pos = std::string(charPath).find_last_of("\\/");
+		return std::string(charPath).substr(0, pos + 1) + fileName;
+	}
+
+	template<class T>
+	class CNvQueue
+	{
+		T** m_pBuffer;
+		unsigned int m_uSize;
+		unsigned int m_uPendingCount;
+		unsigned int m_uAvailableIdx;
+		unsigned int m_uPendingndex;
+
+	public:
+		CNvQueue() :
+			m_uSize(0),
+			m_uPendingCount(0),
+			m_uAvailableIdx(0),
+			m_uPendingndex(0),
+			m_pBuffer(NULL)
+		{
+		}
+
+		~CNvQueue()
+		{
+			delete[] m_pBuffer;
+		}
+
+		bool Initialize(T* pItems, unsigned int uSize)
+		{
+			m_uSize = uSize;
+			m_uPendingCount = 0;
+			m_uAvailableIdx = 0;
+			m_uPendingndex = 0;
+			m_pBuffer = new T*[m_uSize];
+			for (unsigned int i = 0; i < m_uSize; i++)
+			{
+				m_pBuffer[i] = &pItems[i];
+			}
+
+			return true;
+		}
+
+		T* GetAvailable()
+		{
+			T* pItem = NULL;
+			if (m_uPendingCount == m_uSize)
+			{
+				return NULL;
+			}
+
+			pItem = m_pBuffer[m_uAvailableIdx];
+			m_uAvailableIdx = (m_uAvailableIdx + 1) % m_uSize;
+			m_uPendingCount += 1;
+			return pItem;
+		}
+
+		T* GetPending()
+		{
+			if (m_uPendingCount == 0)
+			{
+				return NULL;
+			}
+
+			T* pItem = m_pBuffer[m_uPendingndex];
+			m_uPendingndex = (m_uPendingndex + 1) % m_uSize;
+			m_uPendingCount -= 1;
+			return pItem;
+		}
+	};
+
+typedef struct _EncodeFrameConfig
+{
+	ID3D11Texture2D* pRGBTexture;
+	uint32_t width;
+	uint32_t height;
+} EncodeFrameConfig;
+
+class H264EncoderImpl : public H264Encoder {
+ public:
+  explicit H264EncoderImpl(const cricket::VideoCodec& codec);
+  ~H264EncoderImpl() override;
+
+  static void SetDevice(ID3D11Device* device)
+  {
+	  m_d3dDevice = device;
+  }
+
+  static void SetContext(ID3D11DeviceContext* context)
+  {
+	  m_d3dContext = context;
+  }
+  // |max_payload_size| is ignored.
+  // The following members of |codec_settings| are used. The rest are ignored.
+  // - codecType (must be kVideoCodecH264)
+  // - targetBitrate
+  // - maxFramerate
+  // - width
+  // - height
+  int32_t InitEncode(const VideoCodec* codec_settings,
+                     int32_t number_of_cores,
+                     size_t max_payload_size) override;
+  int32_t Release() override;
+
+  int32_t RegisterEncodeCompleteCallback(
+      EncodedImageCallback* callback) override;
+  int32_t SetRateAllocation(const BitrateAllocation& bitrate_allocation,
+                            uint32_t framerate) override;
+
+  // The result of encoding - an EncodedImage and RTPFragmentationHeader - are
+  // passed to the encode complete callback.
+  int32_t Encode(const VideoFrame& frame,
+                 const CodecSpecificInfo* codec_specific_info,
+                 const std::vector<FrameType>* frame_types) override;
+
+  const char* ImplementationName() const override;
+
+  VideoEncoder::ScalingSettings GetScalingSettings() const override;
+
+  // Unsupported / Do nothing.
+  int32_t SetChannelParameters(uint32_t packet_loss, int64_t rtt) override;
+  int32_t SetPeriodicKeyFrames(bool enable) override;
+
+  // Exposed for testing.
+  H264PacketizationMode PacketizationModeForTesting() const {
+    return packetization_mode_;
+  }
+
+ private:
+  bool IsInitialized() const;
+  SEncParamExt CreateEncoderParams() const;
+
+  NVENCSTATUS SetNvencodeProfile(int profileIndex);
+  void GetDefaultNvencodeConfig(EncodeConfig &nvEncodeConfig, Json::Value rootValue);
+
+  void Capture(ID3D11Texture2D* frameBuffer, bool forceIntra);
+  void GetEncodedFrame(void** buffer, int* size, _NV_ENC_PIC_TYPE* keyFrameType);
+  NVENCSTATUS AllocateIOBuffers(uint32_t uInputWidth, uint32_t uInputHeight);
+  NVENCSTATUS Deinitialize();
+  NVENCSTATUS ReleaseIOBuffers();
+  NVENCSTATUS FlushEncoder();
+
+  webrtc::H264BitstreamParser h264_bitstream_parser_;
+  // Reports statistics with histograms.
+  void ReportInit();
+  void ReportError();
+
+  ISVCEncoder* encoder_;
+  // Settings that are used by this encoder.
+  int width_;
+  int height_;
+  float max_frame_rate_;
+  uint32_t target_bps_;
+  uint32_t max_bps_;
+  VideoCodecMode mode_;
+  // H.264 specifc parameters
+  bool frame_dropping_on_;
+  int key_frame_interval_;
+  H264PacketizationMode packetization_mode_;
+
+  size_t max_payload_size_;
+  int32_t number_of_cores_;
+  CNvHWEncoder*             m_pNvHWEncoder;
+  uint32_t                  m_uEncodeBufferCount;
+  EncodeOutputBuffer		m_stEOSOutputBfr;
+  EncodeBuffer				m_stEncodeBuffer[32];
+  CNvQueue<EncodeBuffer>    m_EncodeBufferQueue;
+  EncodeConfig				m_encodeConfig;
+  bool						m_encoderInitialized;
+  bool						m_use_software_encoding;
+  bool						m_first_frame_sent;
+
+  EncodedImage encoded_image_;
+  std::unique_ptr<uint8_t[]> encoded_image_buffer_;
+  EncodedImageCallback* encoded_image_callback_;
+
+  bool has_reported_init_;
+  bool has_reported_error_;
+
+  static ID3D11Device*	m_d3dDevice;
+  static ID3D11DeviceContext* m_d3dContext;
+};
+
+}  // namespace webrtc
+
+#endif  // WEBRTC_MODULES_VIDEO_CODING_CODECS_H264_H264_ENCODER_IMPL_H_
diff --git a/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl_unittest.cc b/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl_unittest.cc
index 2d236cf..32998fd 100644
--- a/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl_unittest.cc
+++ b/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl_unittest.cc
@@ -8,76 +8,346 @@
  *  be found in the AUTHORS file in the root of the source tree.
  *
  */
-
 #include "webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.h"
-
 #include "webrtc/test/gtest.h"
+#include "webrtc/base/criticalsection.h"
+#include "webrtc/base/event.h"
+#include "webrtc/base/thread_annotations.h"
+#include "webrtc/common_video/libyuv/include/webrtc_libyuv.h"
+#include "webrtc/test/frame_utils.h"
+#include "webrtc/test/testsupport/fileutils.h"
+#include "webrtc/modules/desktop_capture/win/d3d_device.h"
+#include "libyuv/convert_argb.h"
 
 namespace webrtc {
 
-namespace {
-
-const int kMaxPayloadSize = 1024;
-const int kNumCores = 1;
-
-void SetDefaultSettings(VideoCodec* codec_settings) {
-  codec_settings->codecType = kVideoCodecH264;
-  codec_settings->maxFramerate = 60;
-  codec_settings->width = 640;
-  codec_settings->height = 480;
-  // If frame dropping is false, we get a warning that bitrate can't
-  // be controlled for RC_QUALITY_MODE; RC_BITRATE_MODE and RC_TIMESTAMP_MODE
-  codec_settings->H264()->frameDroppingOn = true;
-  codec_settings->targetBitrate = 2000;
-  codec_settings->maxBitrate = 4000;
-}
-
-TEST(H264EncoderImplTest, CanInitializeWithDefaultParameters) {
-  H264EncoderImpl encoder(cricket::VideoCodec("H264"));
-  VideoCodec codec_settings;
-  SetDefaultSettings(&codec_settings);
-  EXPECT_EQ(WEBRTC_VIDEO_CODEC_OK,
-            encoder.InitEncode(&codec_settings, kNumCores, kMaxPayloadSize));
-  EXPECT_EQ(H264PacketizationMode::NonInterleaved,
-            encoder.PacketizationModeForTesting());
-}
-
-TEST(H264EncoderImplTest, CanInitializeWithNonInterleavedModeExplicitly) {
-  cricket::VideoCodec codec("H264");
-  codec.SetParam(cricket::kH264FmtpPacketizationMode, "1");
-  H264EncoderImpl encoder(codec);
-  VideoCodec codec_settings;
-  SetDefaultSettings(&codec_settings);
-  EXPECT_EQ(WEBRTC_VIDEO_CODEC_OK,
-            encoder.InitEncode(&codec_settings, kNumCores, kMaxPayloadSize));
-  EXPECT_EQ(H264PacketizationMode::NonInterleaved,
-            encoder.PacketizationModeForTesting());
-}
-
-TEST(H264EncoderImplTest, CanInitializeWithSingleNalUnitModeExplicitly) {
-  cricket::VideoCodec codec("H264");
-  codec.SetParam(cricket::kH264FmtpPacketizationMode, "0");
-  H264EncoderImpl encoder(codec);
-  VideoCodec codec_settings;
-  SetDefaultSettings(&codec_settings);
-  EXPECT_EQ(WEBRTC_VIDEO_CODEC_OK,
-            encoder.InitEncode(&codec_settings, kNumCores, kMaxPayloadSize));
-  EXPECT_EQ(H264PacketizationMode::SingleNalUnit,
-            encoder.PacketizationModeForTesting());
-}
-
-TEST(H264EncoderImplTest, CanInitializeWithRemovedParameter) {
-  cricket::VideoCodec codec("H264");
-  codec.RemoveParam(cricket::kH264FmtpPacketizationMode);
-  H264EncoderImpl encoder(codec);
-  VideoCodec codec_settings;
-  SetDefaultSettings(&codec_settings);
-  EXPECT_EQ(WEBRTC_VIDEO_CODEC_OK,
-            encoder.InitEncode(&codec_settings, kNumCores, kMaxPayloadSize));
-  EXPECT_EQ(H264PacketizationMode::SingleNalUnit,
-            encoder.PacketizationModeForTesting());
-}
-
-}  // anonymous namespace
+	const int kMaxPayloadSize = 1024;
+	const int kNumCores = 1;
+	static const int kEncodeTimeoutMs = 100;
+	static const int kDecodeTimeoutMs = 25;
+
+	void SetDefaultSettings(VideoCodec* codec_settings) {
+		codec_settings->codecType = kVideoCodecH264;
+		codec_settings->maxFramerate = 60;
+		codec_settings->width = 1280;
+		codec_settings->height = 720;
+		codec_settings->targetBitrate = 4000;
+		codec_settings->maxBitrate = 8000;
+		codec_settings->H264()->frameDroppingOn = true;
+	}
+
+	class H264TestImpl : public ::testing::Test {
+	public:
+		H264TestImpl()
+			: encode_complete_callback_(this),
+			decode_complete_callback_(this),
+			encoded_frame_event_(false /* manual reset */,
+				false /* initially signaled */),
+			decoded_frame_event_(false /* manual reset */,
+				false /* initially signaled */)
+		{
+			defaultCodec = cricket::VideoCodec("H264");
+		}
+
+		void SetEncoderHWEnabled(bool value)
+		{
+			defaultCodec.SetParam(cricket::kH264UseHWNvencode, value);
+			if (value)
+			{
+				// Create a hardware D3D device and context
+				HRESULT hr = S_OK;
+				UINT createDeviceFlags = 0;
+
+				// Creates D3D11 device.
+				hr = D3D11CreateDevice(
+					nullptr,
+					D3D_DRIVER_TYPE_HARDWARE,
+					nullptr,
+					createDeviceFlags,
+					nullptr,
+					0,
+					D3D11_SDK_VERSION,
+					&device,
+					nullptr,
+					&context);
+
+				if (FAILED(hr))
+				{
+					return;
+				}
+
+				H264EncoderImpl::SetDevice(device);
+				H264EncoderImpl::SetContext(context);
+			}
+
+			SetUp();
+		}
+
+	protected:
+		class FakeEncodeCompleteCallback : public webrtc::EncodedImageCallback {
+		public:
+			explicit FakeEncodeCompleteCallback(H264TestImpl* test) : test_(test) {}
+
+			Result OnEncodedImage(const EncodedImage& frame,
+				const CodecSpecificInfo* codec_specific_info,
+				const RTPFragmentationHeader* fragmentation) {
+				rtc::CritScope lock(&test_->encoded_frame_section_);
+				test_->encoded_frame_ = rtc::Optional<EncodedImage>(frame);
+				test_->encoded_frame_event_.Set();
+				return Result(Result::OK);
+			}
+
+		private:
+			H264TestImpl* const test_;
+		};
+
+		class FakeDecodeCompleteCallback : public webrtc::DecodedImageCallback {
+		public:
+			explicit FakeDecodeCompleteCallback(H264TestImpl* test) : test_(test) {}
+
+			int32_t Decoded(VideoFrame& frame) override {
+
+				test_->decoded_frame_ = rtc::Optional<VideoFrame>(frame);
+				test_->decoded_frame_event_.Set();
+
+				return WEBRTC_VIDEO_CODEC_OK;
+			}
+			int32_t Decoded(VideoFrame& frame, int64_t decode_time_ms) override {
+				RTC_NOTREACHED();
+				return -1;
+			}
+			void Decoded(VideoFrame& frame,
+				rtc::Optional<int32_t> decode_time_ms,
+				rtc::Optional<uint8_t> qp) override {
+				rtc::CritScope lock(&test_->decoded_frame_section_);
+				test_->decoded_frame_ = rtc::Optional<VideoFrame>(frame);
+				test_->decoded_qp_ = qp;
+				test_->decoded_frame_event_.Set();
+			}
+
+		private:
+			H264TestImpl* const test_;
+		};
+
+		void SetUp() override {
+			VideoCodec codec_inst_;
+			SetDefaultSettings(&codec_inst_);
+			auto path = ExePath() + "..\\..\\..\\resources\\paris_qcif.yuv";
+			// Using a QCIF image. Processing only one frame.
+			FILE* source_file_ =
+				fopen(path.c_str(), "rb");
+			ASSERT_TRUE(source_file_ != NULL);
+			rtc::scoped_refptr<VideoFrameBuffer> video_frame_buffer(
+				test::ReadI420Buffer(codec_inst_.width, codec_inst_.height, source_file_));
+			
+			input_frame_.reset(new VideoFrame(video_frame_buffer, kVideoRotation_0, 0));
+			fclose(source_file_);
+
+			encoder_.reset(H264Encoder::Create(defaultCodec));
+			decoder_.reset(H264Decoder::Create());
+			encoder_->RegisterEncodeCompleteCallback(&encode_complete_callback_);
+			decoder_->RegisterDecodeCompleteCallback(&decode_complete_callback_);
+
+			EXPECT_EQ(WEBRTC_VIDEO_CODEC_OK,
+				encoder_->InitEncode(&codec_inst_, 1 /* number of cores */,
+					0 /* max payload size (unused) */));
+			EXPECT_EQ(WEBRTC_VIDEO_CODEC_OK,
+				decoder_->InitDecode(&codec_inst_, 1 /* number of cores */));
+		}
+
+		bool WaitForEncodedFrame(EncodedImage* frame) {
+			bool ret = encoded_frame_event_.Wait(kEncodeTimeoutMs);
+			if (!ret)
+				return false;
+
+			// This becomes unsafe if there are multiple threads waiting for frames.
+			rtc::CritScope lock(&encoded_frame_section_);
+			if (encoded_frame_) {
+				*frame = std::move(*encoded_frame_);
+				encoded_frame_.reset();
+				return true;
+			}
+			else {
+				return false;
+			}
+		}
+
+		bool WaitForDecodedFrame(std::unique_ptr<VideoFrame>* frame,
+			rtc::Optional<uint8_t>* qp) {
+			bool ret = decoded_frame_event_.Wait(kDecodeTimeoutMs);
+			EXPECT_TRUE(ret) << "Timed out while waiting for a decoded frame.";
+			// This becomes unsafe if there are multiple threads waiting for frames.
+			rtc::CritScope lock(&decoded_frame_section_);
+			EXPECT_TRUE(decoded_frame_);
+			if (decoded_frame_) {
+				frame->reset(new VideoFrame(std::move(*decoded_frame_)));
+				decoded_frame_.reset();
+				return true;
+			}
+			else {
+				return false;
+			}
+		}
+
+		std::unique_ptr<VideoFrame> input_frame_;
+
+		std::unique_ptr<VideoEncoder> encoder_;
+		std::unique_ptr<VideoDecoder> decoder_;
+
+		ID3D11Device* device = nullptr;
+		ID3D11DeviceContext* context = nullptr;
+
+	private:
+		FakeEncodeCompleteCallback encode_complete_callback_;
+		FakeDecodeCompleteCallback decode_complete_callback_;
+
+		cricket::VideoCodec defaultCodec;
+
+		rtc::Event encoded_frame_event_;
+		rtc::CriticalSection encoded_frame_section_;
+		rtc::Optional<EncodedImage> encoded_frame_ GUARDED_BY(encoded_frame_section_);
+
+		rtc::Event decoded_frame_event_;
+		rtc::CriticalSection decoded_frame_section_;
+		rtc::Optional<VideoFrame> decoded_frame_ GUARDED_BY(decoded_frame_section_);
+		rtc::Optional<uint8_t> decoded_qp_ GUARDED_BY(decoded_frame_section_);
+	};
+
+	TEST(H264EncoderImplTest, CanInitializeWithDefaultParameters) {
+		H264EncoderImpl encoder(cricket::VideoCodec("H264"));
+		VideoCodec codec_settings;
+		SetDefaultSettings(&codec_settings);
+		EXPECT_EQ(WEBRTC_VIDEO_CODEC_OK,
+			encoder.InitEncode(&codec_settings, kNumCores, kMaxPayloadSize));
+		EXPECT_EQ(H264PacketizationMode::NonInterleaved,
+			encoder.PacketizationModeForTesting());
+	}
+
+	TEST(H264EncoderImplTest, CanInitializeWithNonInterleavedModeExplicitly) {
+		cricket::VideoCodec codec("H264");
+		codec.SetParam(cricket::kH264FmtpPacketizationMode, "1");
+		H264EncoderImpl encoder(codec);
+		VideoCodec codec_settings;
+		SetDefaultSettings(&codec_settings);
+		EXPECT_EQ(WEBRTC_VIDEO_CODEC_OK,
+			encoder.InitEncode(&codec_settings, kNumCores, kMaxPayloadSize));
+		EXPECT_EQ(H264PacketizationMode::NonInterleaved,
+			encoder.PacketizationModeForTesting());
+	}
+
+	TEST(H264EncoderImplTest, CanInitializeWithSingleNalUnitModeExplicitly) {
+		cricket::VideoCodec codec("H264");
+		codec.SetParam(cricket::kH264FmtpPacketizationMode, "0");
+		H264EncoderImpl encoder(codec);
+		VideoCodec codec_settings;
+		SetDefaultSettings(&codec_settings);
+		EXPECT_EQ(WEBRTC_VIDEO_CODEC_OK,
+			encoder.InitEncode(&codec_settings, kNumCores, kMaxPayloadSize));
+		EXPECT_EQ(H264PacketizationMode::SingleNalUnit,
+			encoder.PacketizationModeForTesting());
+	}
+
+	TEST(H264EncoderImplTest, CanInitializeWithRemovedParameter) {
+		cricket::VideoCodec codec("H264");
+		codec.RemoveParam(cricket::kH264FmtpPacketizationMode);
+		H264EncoderImpl encoder(codec);
+		VideoCodec codec_settings;
+		SetDefaultSettings(&codec_settings);
+		EXPECT_EQ(WEBRTC_VIDEO_CODEC_OK,
+			encoder.InitEncode(&codec_settings, kNumCores, kMaxPayloadSize));
+		EXPECT_EQ(H264PacketizationMode::SingleNalUnit,
+			encoder.PacketizationModeForTesting());
+	}
+
+	TEST_F(H264TestImpl, SoftwareEncodeDecode) {
+		SetEncoderHWEnabled(false);
+		EXPECT_EQ(WEBRTC_VIDEO_CODEC_OK,
+			encoder_->Encode(*input_frame_, nullptr, nullptr));
+		EncodedImage encoded_frame;
+		ASSERT_TRUE(WaitForEncodedFrame(&encoded_frame));
+		// First frame should be a key frame.
+		encoded_frame._frameType = kVideoFrameKey;
+		EXPECT_EQ(WEBRTC_VIDEO_CODEC_OK,
+			decoder_->Decode(encoded_frame, false, nullptr));
+		std::unique_ptr<VideoFrame> decoded_frame;
+		rtc::Optional<uint8_t> decoded_qp;
+		ASSERT_TRUE(WaitForDecodedFrame(&decoded_frame, &decoded_qp));
+		ASSERT_TRUE(decoded_frame);
+		EXPECT_GT(I420PSNR(input_frame_.get(), decoded_frame.get()), 25);
+	}
+
+	TEST_F(H264TestImpl, HardwareNvencodeEncodeDecode) {
+		SetEncoderHWEnabled(true);
+
+		rtc::scoped_refptr<webrtc::VideoFrameBuffer> buffer(
+			input_frame_.get()->video_frame_buffer());
+
+		D3D11_SUBRESOURCE_DATA initData;
+		size_t NumBytes = buffer->width() * buffer->height() * 4;
+		size_t RowBytes = buffer->width() * 4;
+
+		initData.pSysMem = new uint8_t[NumBytes];
+		initData.SysMemPitch = static_cast<UINT>(RowBytes);
+		initData.SysMemSlicePitch = static_cast<UINT>(NumBytes);
+
+		libyuv::I420ToABGR(buffer->DataY(), buffer->StrideY(),
+			buffer->DataU(), buffer->StrideU(),
+			buffer->DataV(), buffer->StrideV(),
+			(uint8_t*)initData.pSysMem,
+			RowBytes,
+			buffer->width(), buffer->height());
+
+		// Create texture
+		D3D11_TEXTURE2D_DESC desc;
+		desc.Width = buffer->width();
+		desc.Height = buffer->height();
+		desc.MipLevels = 1;
+		desc.ArraySize = 1;
+		desc.Format = DXGI_FORMAT_B8G8R8A8_UNORM;
+		desc.SampleDesc.Count = 1;
+		desc.SampleDesc.Quality = 0;
+		desc.Usage = D3D11_USAGE_STAGING;
+		desc.BindFlags = 0;
+		desc.CPUAccessFlags = D3D11_CPU_ACCESS_READ;
+		desc.MiscFlags = 0;
+
+		ID3D11Texture2D* tex = nullptr;
+		device->CreateTexture2D(&desc, &initData, &tex);
+		input_frame_.get()->SetID3D11Texture2D(tex);
+
+		// Nvencode needs a few frames before it starts to output the encoded frame
+		int retryForEncodeFrame = 100;
+		EncodedImage encoded_frame;
+		while (retryForEncodeFrame > 0)
+		{
+			EXPECT_EQ(WEBRTC_VIDEO_CODEC_OK,
+				encoder_->Encode(*input_frame_, nullptr, nullptr));
+			if (WaitForEncodedFrame(&encoded_frame))
+			{
+				retryForEncodeFrame = -1;
+			}
+			else
+			{
+				retryForEncodeFrame--;
+
+				ID3D11Texture2D* tex = nullptr;
+				device->CreateTexture2D(&desc, &initData, &tex);
+				input_frame_.get()->SetID3D11Texture2D(tex);
+			}
+		}
+
+		// First frame should be a key frame.
+		encoded_frame._frameType = kVideoFrameKey;
+		EXPECT_EQ(WEBRTC_VIDEO_CODEC_OK,
+			decoder_->Decode(encoded_frame, false, nullptr));
+		std::unique_ptr<VideoFrame> decoded_frame;
+		rtc::Optional<uint8_t> decoded_qp;
+		ASSERT_TRUE(WaitForDecodedFrame(&decoded_frame, &decoded_qp));
+		ASSERT_TRUE(decoded_frame);
+		EXPECT_GT(I420PSNR(input_frame_.get(), decoded_frame.get()), 5);
+		
+		// Test correct release of hardware encoder
+		EXPECT_EQ(WEBRTC_VIDEO_CODEC_OK, encoder_->Release());
+	}
 
 }  // namespace webrtc
diff --git a/webrtc/modules/video_coding/codecs/h264/include/NvHWEncoder.h b/webrtc/modules/video_coding/codecs/h264/include/NvHWEncoder.h
new file mode 100644
index 0000000..a96695e
--- /dev/null
+++ b/webrtc/modules/video_coding/codecs/h264/include/NvHWEncoder.h
@@ -0,0 +1,234 @@
+/*
+ * Copyright 1993-2015 NVIDIA Corporation.  All rights reserved.
+ *
+ * Please refer to the NVIDIA end user license agreement (EULA) associated
+ * with this source code for terms and conditions that govern your use of
+ * this software. Any use, reproduction, disclosure, or distribution of
+ * this software and related documentation outside the terms of the EULA
+ * is strictly prohibited.
+ *
+ */
+
+#include <stdlib.h>
+#include <stdio.h>
+#include <assert.h>
+
+#include "dynlink_cuda.h" // <cuda.h>
+
+#include "nvEncodeAPI.h"
+#include "nvUtils.h"
+
+#define SET_VER(configStruct, type) {configStruct.version = type##_VER;}
+
+#if defined (NV_WINDOWS)
+    #include "d3d11_1.h"
+    #define NVENCAPI __stdcall
+    #pragma warning(disable : 4996)
+#elif defined (NV_UNIX)
+    #include <dlfcn.h>
+    #include <string.h>
+    #define NVENCAPI
+#endif
+
+#define DEFAULT_I_QFACTOR -0.8f
+#define DEFAULT_B_QFACTOR 1.25f
+#define DEFAULT_I_QOFFSET 0.f
+#define DEFAULT_B_QOFFSET 1.25f
+
+typedef struct _EncodeConfig
+{
+    int              width;
+    int              height;
+    int              maxWidth;
+    int              maxHeight;
+    int              fps;
+	int				 minBitrate;
+    int              bitrate;
+    int              vbvMaxBitrate;
+    int              vbvSize;
+    int              rcMode;
+    int              qp;
+    float            i_quant_factor;
+    float            b_quant_factor;
+    float            i_quant_offset;
+    float            b_quant_offset;
+    GUID             presetGUID;
+    FILE            *fOutput;
+    int              codec;
+    int              invalidateRefFramesEnableFlag;
+    int              intraRefreshEnableFlag;
+    int              intraRefreshPeriod;
+    int              intraRefreshDuration;
+    int              deviceType;
+    int              startFrameIdx;
+    int              endFrameIdx;
+    int              gopLength;
+    int              numB;
+    int              pictureStruct;
+    int              deviceID;
+    NV_ENC_BUFFER_FORMAT inputFormat;
+    char            *qpDeltaMapFile;
+    char* inputFileName;
+    char* outputFileName;
+    char* encoderPreset;
+    char* inputFilePath;
+    char *encCmdFileName;
+    int  enableMEOnly;
+    int  enableAsyncMode;
+    int  preloadedFrameCount;
+    int  enableTemporalAQ;
+}EncodeConfig;
+
+typedef struct _EncodeInputBuffer
+{
+    unsigned int      dwWidth;
+    unsigned int      dwHeight;
+#if defined (NV_WINDOWS)
+	ID3D11Texture2D	  *pARGBSurface;
+#endif
+    CUdeviceptr       pARGBdevPtr;
+    uint32_t          uARGBStride;
+    CUdeviceptr       pARGBTempdevPtr;
+    uint32_t          uARGBTempStride;
+    void*             nvRegisteredResource;
+    NV_ENC_INPUT_PTR  hInputSurface;
+    NV_ENC_BUFFER_FORMAT bufferFmt;
+}EncodeInputBuffer;
+
+typedef struct _EncodeOutputBuffer
+{
+    unsigned int          dwBitstreamBufferSize;
+    NV_ENC_OUTPUT_PTR     hBitstreamBuffer;
+    HANDLE                hOutputEvent;
+    bool                  bWaitOnEvent;
+    bool                  bEOSFlag;
+}EncodeOutputBuffer;
+
+typedef struct _EncodeBuffer
+{
+    EncodeOutputBuffer      stOutputBfr;
+    EncodeInputBuffer       stInputBfr;
+}EncodeBuffer;
+
+typedef struct _MotionEstimationBuffer
+{
+    EncodeOutputBuffer      stOutputBfr;
+    EncodeInputBuffer       stInputBfr[2];
+    unsigned int            inputFrameIndex;
+    unsigned int            referenceFrameIndex;
+}MotionEstimationBuffer;
+
+typedef struct _NvEncPictureCommand
+{
+    bool bResolutionChangePending;
+    bool bBitrateChangePending;
+    bool bForceIDR;
+    bool bForceIntraRefresh;
+    bool bInvalidateRefFrames;
+
+    uint32_t newWidth;
+    uint32_t newHeight;
+
+    uint32_t newBitrate;
+    uint32_t newVBVSize;
+
+    uint32_t  intraRefreshDuration;
+
+    uint32_t  numRefFramesToInvalidate;
+    uint32_t  refFrameNumbers[16];
+}NvEncPictureCommand;
+
+enum
+{
+    NV_ENC_H264 = 0,
+    NV_ENC_HEVC = 1,
+};
+
+struct MEOnlyConfig
+{
+    unsigned char *yuv[2][3];
+    unsigned int stride[3];
+    unsigned int width;
+    unsigned int height;
+    unsigned int inputFrameIndex;
+    unsigned int referenceFrameIndex;
+};
+
+class CNvHWEncoder
+{
+public:
+    uint32_t                                             m_EncodeIdx;
+    FILE                                                *m_fOutput;
+    uint32_t                                             m_uMaxWidth;
+    uint32_t                                             m_uMaxHeight;
+    uint32_t                                             m_uCurWidth;
+    uint32_t                                             m_uCurHeight;
+	NV_ENC_LOCK_BITSTREAM								 m_lockBitstreamData;
+	NV_ENC_CONFIG                                        m_stEncodeConfig;
+
+protected:
+    bool                                                 m_bEncoderInitialized;
+    GUID                                                 codecGUID;
+
+    NV_ENCODE_API_FUNCTION_LIST*                         m_pEncodeAPI;
+    HMODULE												m_hinstLib;
+    void                                                *m_hEncoder;
+    NV_ENC_INITIALIZE_PARAMS                             m_stCreateEncodeParams;
+
+public:
+    NVENCSTATUS NvEncOpenEncodeSession(void* device, uint32_t deviceType);
+    NVENCSTATUS NvEncGetEncodeGUIDCount(uint32_t* encodeGUIDCount);
+    NVENCSTATUS NvEncGetEncodeProfileGUIDCount(GUID encodeGUID, uint32_t* encodeProfileGUIDCount);
+    NVENCSTATUS NvEncGetEncodeProfileGUIDs(GUID encodeGUID, GUID* profileGUIDs, uint32_t guidArraySize, uint32_t* GUIDCount);
+    NVENCSTATUS NvEncGetEncodeGUIDs(GUID* GUIDs, uint32_t guidArraySize, uint32_t* GUIDCount);
+    NVENCSTATUS NvEncGetInputFormatCount(GUID encodeGUID, uint32_t* inputFmtCount);
+    NVENCSTATUS NvEncGetInputFormats(GUID encodeGUID, NV_ENC_BUFFER_FORMAT* inputFmts, uint32_t inputFmtArraySize, uint32_t* inputFmtCount);
+    NVENCSTATUS NvEncGetEncodeCaps(GUID encodeGUID, NV_ENC_CAPS_PARAM* capsParam, int* capsVal);
+    NVENCSTATUS NvEncGetEncodePresetCount(GUID encodeGUID, uint32_t* encodePresetGUIDCount);
+    NVENCSTATUS NvEncGetEncodePresetGUIDs(GUID encodeGUID, GUID* presetGUIDs, uint32_t guidArraySize, uint32_t* encodePresetGUIDCount);
+    NVENCSTATUS NvEncGetEncodePresetConfig(GUID encodeGUID, GUID  presetGUID, NV_ENC_PRESET_CONFIG* presetConfig);
+    NVENCSTATUS NvEncCreateInputBuffer(uint32_t width, uint32_t height, void** inputBuffer, NV_ENC_BUFFER_FORMAT inputFormat);
+    NVENCSTATUS NvEncDestroyInputBuffer(NV_ENC_INPUT_PTR inputBuffer);
+    NVENCSTATUS NvEncCreateBitstreamBuffer(uint32_t size, void** bitstreamBuffer);
+    NVENCSTATUS NvEncDestroyBitstreamBuffer(NV_ENC_OUTPUT_PTR bitstreamBuffer);
+    NVENCSTATUS NvEncCreateMVBuffer(uint32_t size, void** bitstreamBuffer);
+    NVENCSTATUS NvEncDestroyMVBuffer(NV_ENC_OUTPUT_PTR bitstreamBuffer);
+    NVENCSTATUS NvRunMotionEstimationOnly(MotionEstimationBuffer *pMEBuffer, MEOnlyConfig *pMEOnly);
+    NVENCSTATUS NvEncLockBitstream(NV_ENC_LOCK_BITSTREAM* lockBitstreamBufferParams);
+    NVENCSTATUS NvEncUnlockBitstream(NV_ENC_OUTPUT_PTR bitstreamBuffer);
+    NVENCSTATUS NvEncLockInputBuffer(void* inputBuffer, void** bufferDataPtr, uint32_t* pitch);
+    NVENCSTATUS NvEncUnlockInputBuffer(NV_ENC_INPUT_PTR inputBuffer);
+    NVENCSTATUS NvEncGetEncodeStats(NV_ENC_STAT* encodeStats);
+    NVENCSTATUS NvEncGetSequenceParams(NV_ENC_SEQUENCE_PARAM_PAYLOAD* sequenceParamPayload);
+    NVENCSTATUS NvEncRegisterAsyncEvent(void** completionEvent);
+    NVENCSTATUS NvEncUnregisterAsyncEvent(void* completionEvent);
+    NVENCSTATUS NvEncMapInputResource(void* registeredResource, void** mappedResource);
+    NVENCSTATUS NvEncUnmapInputResource(NV_ENC_INPUT_PTR mappedInputBuffer);
+    NVENCSTATUS NvEncDestroyEncoder();
+    NVENCSTATUS NvEncInvalidateRefFrames(const NvEncPictureCommand *pEncPicCommand);
+    NVENCSTATUS NvEncOpenEncodeSessionEx(void* device, NV_ENC_DEVICE_TYPE deviceType);
+    NVENCSTATUS NvEncRegisterResource(NV_ENC_INPUT_RESOURCE_TYPE resourceType, void* resourceToRegister, uint32_t width, uint32_t height, uint32_t pitch, void** registeredResource);
+    NVENCSTATUS NvEncUnregisterResource(NV_ENC_REGISTERED_PTR registeredRes);
+    NVENCSTATUS NvEncReconfigureEncoder(const NvEncPictureCommand *pEncPicCommand);
+    NVENCSTATUS NvEncFlushEncoderQueue(void *hEOSEvent);
+
+    CNvHWEncoder();
+    virtual ~CNvHWEncoder();
+    NVENCSTATUS                                          Initialize(void* device, NV_ENC_DEVICE_TYPE deviceType);
+    NVENCSTATUS                                          Deinitialize();
+    NVENCSTATUS                                          NvEncEncodeFrame(EncodeBuffer *pEncodeBuffer, NvEncPictureCommand *encPicCommand,
+                                                                          uint32_t width, uint32_t height,
+                                                                          NV_ENC_PIC_STRUCT ePicStruct = NV_ENC_PIC_STRUCT_FRAME,
+                                                                          int8_t *qpDeltaMapArray = NULL, uint32_t qpDeltaMapArraySize = 0);
+    NVENCSTATUS                                          CreateEncoder(EncodeConfig *pEncCfg);
+    GUID                                                 GetPresetGUID(char* encoderPreset, int codec);
+    NVENCSTATUS                                          ProcessOutput(const EncodeBuffer *pEncodeBuffer);
+    NVENCSTATUS                                          ProcessMVOutput(const MotionEstimationBuffer *pEncodeBuffer);
+    NVENCSTATUS                                          FlushEncoder();
+    NVENCSTATUS                                          ValidateEncodeGUID(GUID inputCodecGuid);
+    NVENCSTATUS                                          ValidatePresetGUID(GUID presetCodecGuid, GUID inputCodecGuid);
+	NV_ENC_LOCK_BITSTREAM								 GetLockBitStream();
+    static NVENCSTATUS                                   ParseArguments(EncodeConfig *encodeConfig, int argc, char *argv[]);
+};
+
+typedef NVENCSTATUS (NVENCAPI *MYPROC)(NV_ENCODE_API_FUNCTION_LIST*); 
diff --git a/webrtc/modules/video_coding/codecs/h264/include/cudaModuleMgr.h b/webrtc/modules/video_coding/codecs/h264/include/cudaModuleMgr.h
new file mode 100644
index 0000000..b7eef04
--- /dev/null
+++ b/webrtc/modules/video_coding/codecs/h264/include/cudaModuleMgr.h
@@ -0,0 +1,111 @@
+/*
+ * Copyright 1993-2015 NVIDIA Corporation.  All rights reserved.
+ *
+ * Please refer to the NVIDIA end user license agreement (EULA) associated
+ * with this source code for terms and conditions that govern your use of
+ * this software. Any use, reproduction, disclosure, or distribution of
+ * this software and related documentation outside the terms of the EULA
+ * is strictly prohibited.
+ *
+ */
+
+/* cudaModuleMgr has the C and C++ implementations.  These functions manage:
+ *   loading CUBIN/PTX
+ *    initializing CUBIN/PTX kernel function,
+ *    mapping CUDA kernel function pointers,
+ *    and obtaining global memory pointers and texture references
+ */
+
+#ifndef _CUDAMODULEMGR_H_
+#define _CUDAMODULEMGR_H_
+
+#include <memory>
+#include <iostream>
+#include <cassert>
+
+// Function String Name and a pointer to the CUDA Kernel Function
+typedef struct CudaKernels
+{
+    CUfunction fpCuda;
+    std::string func_name;
+} *pCudaKernels_;
+
+// Global Memory Name and the device pointer
+typedef struct CudaGlobalMem
+{
+    CUdeviceptr  devicePtr;
+    size_t       nBytes;
+    std::string  address_name;
+} *pGlobalMem_;
+
+// TexReference and the CUtexref pointer
+typedef struct CudaTexRef
+{
+    CUtexref     texRef;
+    unsigned int nBytes;
+    std::string  texref_name;
+} *pTexRef_;
+
+// We have a C++ and a C version so that developers can choose to use either depending on their preferences
+typedef struct _sCUModuleContext
+{
+    std::string mModuleName;
+
+    int nMaxKernels_;    // maximum number of kernels
+    int nMaxGlobalMem_;  // maximum number of global constants
+    int nMaxTexRef_;     // maximum number of texture references
+
+    int nLastKernel_;    // the last kernel
+    int nLastGlobalMem_; // the last global constant used
+    int nLastTexRef_;    // the last texture reference used
+
+    CudaKernels    *pCudaKernels_;  // stores the data, strings for the CUDA kernels
+    CudaGlobalMem *pGlobalMem_;     // stores the data, strings for the Global Memory (Device Pointers)
+    CudaTexRef     *pTexRef_;       // stores the data, strings for the Texture References
+
+    CUmodule    cuModule_;
+} sCtxModule;
+
+// Here is the C implementation for the Module Manager, the C++ class calls the C implementation
+extern "C" bool     modInitCtx(sCtxModule *mCtx, const char *filename, const char *exec_path, int nKernels, int nGlobalMem, int nTexRef);
+extern "C" void     modFreeCtx(sCtxModule *mCtx);
+
+extern "C" CUresult modGetCudaDevicePtr(sCtxModule *mCtx, const char *address_name, CUdeviceptr *pGlobalMem);
+extern "C" CUresult modGetTexRef(sCtxModule *mCtx, const char *texref_name,  CUtexref    *pTexRef);
+extern "C" CUresult modLaunchKernel(sCtxModule *mCtx, CUfunction fpFunc, dim3 block, dim3 grid);
+
+extern "C" int      modFindIndex_CudaKernels(sCtxModule *mCtx, const char *func_name);
+extern "C" int      modFindIndex_GlobalMem(sCtxModule *mCtx, const char *address_name);
+extern "C" int      modFindIndex_TexRef(sCtxModule *mCtx, const char *texref_name);
+
+
+// Here is the C++ Class interface to the Module Manager
+class CUmoduleManager
+{
+    public:
+        // For each CUBIN file loaded, one CUBIN is associated with one CUmodule
+        CUmoduleManager(const char *filename_module, const char *exec_path, int nKernels, int nGlobalMem, int nTexRef);
+        ~CUmoduleManager();
+
+        CUresult GetCudaFunction(const char *func_name,    CUfunction  *fpCudaKernel = 0);
+        CUresult GetCudaDevicePtr(const char *address_name, CUdeviceptr *pGlobalMem = 0);
+        CUresult GetTexRef(const char *texref_name,  CUtexref    *pTexRef = 0);
+
+        int findIndex_CudaKernels(const char *func_name);
+        int findIndex_GlobalMem(const char *address_name);
+        int findIndex_TexRef(const char *texref_name);
+
+        CUresult launchKernel(CUfunction fpFunc, dim3 block, dim3 grid);
+
+        CUmodule getModule()
+        {
+            return mCtx.cuModule_;
+        }
+
+    protected:
+        // This stores all of the relevant data for the Module (PTX or CUBIN)
+        sCtxModule mCtx;
+};
+
+
+#endif
diff --git a/webrtc/modules/video_coding/codecs/h264/include/drvapi_error_string.h b/webrtc/modules/video_coding/codecs/h264/include/drvapi_error_string.h
new file mode 100644
index 0000000..ebeec10
--- /dev/null
+++ b/webrtc/modules/video_coding/codecs/h264/include/drvapi_error_string.h
@@ -0,0 +1,341 @@
+/*
+ * Copyright 1993-2015 NVIDIA Corporation.  All rights reserved.
+ *
+ * Please refer to the NVIDIA end user license agreement (EULA) associated
+ * with this source code for terms and conditions that govern your use of
+ * this software. Any use, reproduction, disclosure, or distribution of
+ * this software and related documentation outside the terms of the EULA
+ * is strictly prohibited.
+ *
+ */
+
+#ifndef _DRVAPI_ERROR_STRING_H_
+#define _DRVAPI_ERROR_STRING_H_
+
+#include <stdio.h>
+#include <string.h>
+#include <stdlib.h>
+
+#ifdef  __cuda_cuda_h__ // check to see if CUDA_H is included above
+
+// Error Code string definitions here
+typedef struct
+{
+    char const *error_string;
+    int  error_id;
+} s_CudaErrorStr;
+
+/**
+ * Error codes
+ */
+static s_CudaErrorStr sCudaDrvErrorString[] =
+{
+    /**
+     * The API call returned with no errors. In the case of query calls, this
+     * can also mean that the operation being queried is complete (see
+     * ::cuEventQuery() and ::cuStreamQuery()).
+     */
+    { "CUDA_SUCCESS", 0 },
+
+    /**
+     * This indicates that one or more of the parameters passed to the API call
+     * is not within an acceptable range of values.
+     */
+    { "CUDA_ERROR_INVALID_VALUE", 1 },
+
+    /**
+     * The API call failed because it was unable to allocate enough memory to
+     * perform the requested operation.
+     */
+    { "CUDA_ERROR_OUT_OF_MEMORY", 2 },
+
+    /**
+     * This indicates that the CUDA driver has not been initialized with
+     * ::cuInit() or that initialization has failed.
+     */
+    { "CUDA_ERROR_NOT_INITIALIZED", 3 },
+
+    /**
+     * This indicates that the CUDA driver is in the process of shutting down.
+     */
+    { "CUDA_ERROR_DEINITIALIZED", 4 },
+
+    /**
+     * This indicates profiling APIs are called while application is running
+     * in visual profiler mode.
+    */
+    { "CUDA_ERROR_PROFILER_DISABLED", 5 },
+    /**
+     * This indicates profiling has not been initialized for this context.
+     * Call cuProfilerInitialize() to resolve this.
+    */
+    { "CUDA_ERROR_PROFILER_NOT_INITIALIZED", 6 },
+    /**
+     * This indicates profiler has already been started and probably
+     * cuProfilerStart() is incorrectly called.
+    */
+    { "CUDA_ERROR_PROFILER_ALREADY_STARTED", 7 },
+    /**
+     * This indicates profiler has already been stopped and probably
+     * cuProfilerStop() is incorrectly called.
+    */
+    { "CUDA_ERROR_PROFILER_ALREADY_STOPPED", 8 },
+    /**
+     * This indicates that no CUDA-capable devices were detected by the installed
+     * CUDA driver.
+     */
+    { "CUDA_ERROR_NO_DEVICE (no CUDA-capable devices were detected)", 100 },
+
+    /**
+     * This indicates that the device ordinal supplied by the user does not
+     * correspond to a valid CUDA device.
+     */
+    { "CUDA_ERROR_INVALID_DEVICE (device specified is not a valid CUDA device)", 101 },
+
+
+    /**
+     * This indicates that the device kernel image is invalid. This can also
+     * indicate an invalid CUDA module.
+     */
+    { "CUDA_ERROR_INVALID_IMAGE", 200 },
+
+    /**
+     * This most frequently indicates that there is no context bound to the
+     * current thread. This can also be returned if the context passed to an
+     * API call is not a valid handle (such as a context that has had
+     * ::cuCtxDestroy() invoked on it). This can also be returned if a user
+     * mixes different API versions (i.e. 3010 context with 3020 API calls).
+     * See ::cuCtxGetApiVersion() for more details.
+     */
+    { "CUDA_ERROR_INVALID_CONTEXT", 201 },
+
+    /**
+     * This indicated that the context being supplied as a parameter to the
+     * API call was already the active context.
+     * \deprecated
+     * This error return is deprecated as of CUDA 3.2. It is no longer an
+     * error to attempt to push the active context via ::cuCtxPushCurrent().
+     */
+    { "CUDA_ERROR_CONTEXT_ALREADY_CURRENT", 202 },
+
+    /**
+     * This indicates that a map or register operation has failed.
+     */
+    { "CUDA_ERROR_MAP_FAILED", 205 },
+
+    /**
+     * This indicates that an unmap or unregister operation has failed.
+     */
+    { "CUDA_ERROR_UNMAP_FAILED", 206 },
+
+    /**
+     * This indicates that the specified array is currently mapped and thus
+     * cannot be destroyed.
+     */
+    { "CUDA_ERROR_ARRAY_IS_MAPPED", 207 },
+
+    /**
+     * This indicates that the resource is already mapped.
+     */
+    { "CUDA_ERROR_ALREADY_MAPPED", 208 },
+
+    /**
+     * This indicates that there is no kernel image available that is suitable
+     * for the device. This can occur when a user specifies code generation
+     * options for a particular CUDA source file that do not include the
+     * corresponding device configuration.
+     */
+    { "CUDA_ERROR_NO_BINARY_FOR_GPU", 209 },
+
+    /**
+     * This indicates that a resource has already been acquired.
+     */
+    { "CUDA_ERROR_ALREADY_ACQUIRED", 210 },
+
+    /**
+     * This indicates that a resource is not mapped.
+     */
+    { "CUDA_ERROR_NOT_MAPPED", 211 },
+
+    /**
+     * This indicates that a mapped resource is not available for access as an
+     * array.
+     */
+    { "CUDA_ERROR_NOT_MAPPED_AS_ARRAY", 212 },
+
+    /**
+     * This indicates that a mapped resource is not available for access as a
+     * pointer.
+     */
+    { "CUDA_ERROR_NOT_MAPPED_AS_POINTER", 213 },
+
+    /**
+     * This indicates that an uncorrectable ECC error was detected during
+     * execution.
+     */
+    { "CUDA_ERROR_ECC_UNCORRECTABLE", 214 },
+
+    /**
+     * This indicates that the ::CUlimit passed to the API call is not
+     * supported by the active device.
+     */
+    { "CUDA_ERROR_UNSUPPORTED_LIMIT", 215 },
+
+    /**
+     * This indicates that the ::CUcontext passed to the API call can
+     * only be bound to a single CPU thread at a time but is already
+     * bound to a CPU thread.
+     */
+    { "CUDA_ERROR_CONTEXT_ALREADY_IN_USE", 216 },
+
+    /**
+     * This indicates that the device kernel source is invalid.
+     */
+    { "CUDA_ERROR_INVALID_SOURCE", 300 },
+
+    /**
+     * This indicates that the file specified was not found.
+     */
+    { "CUDA_ERROR_FILE_NOT_FOUND", 301 },
+
+    /**
+     * This indicates that a link to a shared object failed to resolve.
+     */
+    { "CUDA_ERROR_SHARED_OBJECT_SYMBOL_NOT_FOUND", 302 },
+
+    /**
+     * This indicates that initialization of a shared object failed.
+     */
+    { "CUDA_ERROR_SHARED_OBJECT_INIT_FAILED", 303 },
+
+    /**
+     * This indicates that an OS call failed.
+     */
+    { "CUDA_ERROR_OPERATING_SYSTEM", 304 },
+
+
+    /**
+     * This indicates that a resource handle passed to the API call was not
+     * valid. Resource handles are opaque types like ::CUstream and ::CUevent.
+     */
+    { "CUDA_ERROR_INVALID_HANDLE", 400 },
+
+
+    /**
+     * This indicates that a named symbol was not found. Examples of symbols
+     * are global/constant variable names, texture names }, and surface names.
+     */
+    { "CUDA_ERROR_NOT_FOUND", 500 },
+
+
+    /**
+     * This indicates that asynchronous operations issued previously have not
+     * completed yet. This result is not actually an error, but must be indicated
+     * differently than ::CUDA_SUCCESS (which indicates completion). Calls that
+     * may return this value include ::cuEventQuery() and ::cuStreamQuery().
+     */
+    { "CUDA_ERROR_NOT_READY", 600 },
+
+
+    /**
+     * An exception occurred on the device while executing a kernel. Common
+     * causes include dereferencing an invalid device pointer and accessing
+     * out of bounds shared memory. The context cannot be used }, so it must
+     * be destroyed (and a new one should be created). All existing device
+     * memory allocations from this context are invalid and must be
+     * reconstructed if the program is to continue using CUDA.
+     */
+    { "CUDA_ERROR_LAUNCH_FAILED", 700 },
+
+    /**
+     * This indicates that a launch did not occur because it did not have
+     * appropriate resources. This error usually indicates that the user has
+     * attempted to pass too many arguments to the device kernel, or the
+     * kernel launch specifies too many threads for the kernel's register
+     * count. Passing arguments of the wrong size (i.e. a 64-bit pointer
+     * when a 32-bit int is expected) is equivalent to passing too many
+     * arguments and can also result in this error.
+     */
+    { "CUDA_ERROR_LAUNCH_OUT_OF_RESOURCES", 701 },
+
+    /**
+     * This indicates that the device kernel took too long to execute. This can
+     * only occur if timeouts are enabled - see the device attribute
+     * ::CU_DEVICE_ATTRIBUTE_KERNEL_EXEC_TIMEOUT for more information. The
+     * context cannot be used (and must be destroyed similar to
+     * ::CUDA_ERROR_LAUNCH_FAILED). All existing device memory allocations from
+     * this context are invalid and must be reconstructed if the program is to
+     * continue using CUDA.
+     */
+    { "CUDA_ERROR_LAUNCH_TIMEOUT", 702 },
+
+    /**
+     * This error indicates a kernel launch that uses an incompatible texturing
+     * mode.
+     */
+    { "CUDA_ERROR_LAUNCH_INCOMPATIBLE_TEXTURING", 703 },
+
+    /**
+     * This error indicates that a call to ::cuCtxEnablePeerAccess() is
+     * trying to re-enable peer access to a context which has already
+     * had peer access to it enabled.
+     */
+    { "CUDA_ERROR_PEER_ACCESS_ALREADY_ENABLED", 704 },
+
+    /**
+     * This error indicates that ::cuCtxDisablePeerAccess() is
+     * trying to disable peer access which has not been enabled yet
+     * via ::cuCtxEnablePeerAccess().
+     */
+    { "CUDA_ERROR_PEER_ACCESS_NOT_ENABLED", 705 },
+
+    /**
+     * This error indicates that the primary context for the specified device
+     * has already been initialized.
+     */
+    { "CUDA_ERROR_PRIMARY_CONTEXT_ACTIVE", 708 },
+
+    /**
+     * This error indicates that the context current to the calling thread
+     * has been destroyed using ::cuCtxDestroy }, or is a primary context which
+     * has not yet been initialized.
+     */
+    { "CUDA_ERROR_CONTEXT_IS_DESTROYED", 709 },
+
+    /**
+     * A device-side assert triggered during kernel execution. The context
+     * cannot be used anymore, and must be destroyed. All existing device
+     * memory allocations from this context are invalid and must be
+     * reconstructed if the program is to continue using CUDA.
+     */
+    { "CUDA_ERROR_ASSERT", 710 },
+
+    /**
+     * This indicates that an unknown internal error has occurred.
+     */
+    { "CUDA_ERROR_UNKNOWN", 999 },
+    { NULL, -1 }
+};
+
+// This is just a linear search through the array, since the error_id's are not
+// always ocurring consecutively
+inline const char *getCudaDrvErrorString(CUresult error_id)
+{
+    int index = 0;
+
+    while (sCudaDrvErrorString[index].error_id != error_id &&
+           sCudaDrvErrorString[index].error_id != -1)
+    {
+        index++;
+    }
+
+    if (sCudaDrvErrorString[index].error_id == error_id)
+        return (const char *)sCudaDrvErrorString[index].error_string;
+    else
+        return (const char *)"CUDA_ERROR not found!";
+}
+
+#endif // __cuda_cuda_h__
+
+
+#endif
diff --git a/webrtc/modules/video_coding/codecs/h264/include/dynlink_builtin_types.h b/webrtc/modules/video_coding/codecs/h264/include/dynlink_builtin_types.h
new file mode 100644
index 0000000..db93385
--- /dev/null
+++ b/webrtc/modules/video_coding/codecs/h264/include/dynlink_builtin_types.h
@@ -0,0 +1,763 @@
+/*
+ * Copyright 1993-2015 NVIDIA Corporation.  All rights reserved.
+ *
+ * Please refer to the NVIDIA end user license agreement (EULA) associated
+ * with this source code for terms and conditions that govern your use of
+ * this software. Any use, reproduction, disclosure, or distribution of
+ * this software and related documentation outside the terms of the EULA
+ * is strictly prohibited.
+ *
+ */
+
+#pragma once
+
+#if defined(__GNUC__) || defined(__CUDA_LIBDEVICE__)
+
+#define __no_return__ \
+        __attribute__((noreturn))
+        
+#if defined(__CUDACC__) || defined(__CUDA_ARCH__)
+/* gcc allows users to define attributes with underscores, 
+   e.g., __attribute__((__noinline__)).
+   Consider a non-CUDA source file (e.g. .cpp) that has the 
+   above attribute specification, and includes this header file. In that case,
+   defining __noinline__ as below  would cause a gcc compilation error.
+   Hence, only define __noinline__ when the code is being processed
+   by a  CUDA compiler component.
+*/   
+#define __noinline__ \
+        __attribute__((noinline))
+#endif /* __CUDACC__  || __CUDA_ARCH__ */       
+        
+#define __forceinline__ \
+        __inline__ __attribute__((always_inline))
+#define __align__(n) \
+        __attribute__((aligned(n)))
+#define __thread__ \
+        __thread
+#define __import__
+#define __export__
+#define __cdecl
+#define __annotate__(a) \
+        __attribute__((a))
+#define __location__(a) \
+        __annotate__(a)
+#define CUDARTAPI
+
+#elif defined(_MSC_VER)
+
+#if _MSC_VER >= 1400
+
+#define __restrict__ \
+        __restrict
+
+#else /* _MSC_VER >= 1400 */
+
+#define __restrict__
+
+#endif /* _MSC_VER >= 1400 */
+
+#define __inline__ \
+        __inline
+#define __no_return__ \
+        __declspec(noreturn)
+#define __noinline__ \
+        __declspec(noinline)
+#define __forceinline__ \
+        __forceinline
+#define __align__(n) \
+        __declspec(align(n))
+#define __thread__ \
+        __declspec(thread)
+#define __import__ \
+        __declspec(dllimport)
+#define __export__ \
+        __declspec(dllexport)
+#define __annotate__(a) \
+        __declspec(a)
+#define __location__(a) \
+        __annotate__(__##a##__)
+#define CUDARTAPI \
+        __stdcall
+
+#else /* __GNUC__ || __CUDA_LIBDEVICE__ */
+
+#define __inline__
+
+#if !defined(__align__)
+
+#error --- !!! UNKNOWN COMPILER: please provide a CUDA compatible definition for '__align__' !!! ---
+
+#endif /* !__align__ */
+
+#if !defined(CUDARTAPI)
+
+#error --- !!! UNKNOWN COMPILER: please provide a CUDA compatible definition for 'CUDARTAPI' !!! ---
+
+#endif /* !CUDARTAPI */
+
+#endif /* !__GNUC__ */
+
+#if !defined(__GNUC__) || __GNUC__ < 4 || (__GNUC__ == 4 && __GNUC_MINOR__ < 3 && !defined(__clang__) )
+
+#define __specialization_static \
+        static
+
+#else /* !__GNUC__ || __GNUC__ < 4 || (__GNUC__ == 4 && __GNUC_MINOR__ < 3) */
+
+#define __specialization_static
+
+#endif /* !__GNUC__ || __GNUC__ < 4 || (__GNUC__ == 4 && __GNUC_MINOR__ < 3) */
+
+#if !defined(__CUDACC__) && !defined(__CUDABE__)
+
+#undef __annotate__
+#define __annotate__(a)
+
+#else /* !__CUDACC__ && !__CUDABE__ */
+
+#define __launch_bounds__(...) \
+        __annotate__(launch_bounds(__VA_ARGS__))
+
+#endif /* !__CUDACC__ && !__CUDABE__ */
+
+#if defined(__CUDACC__) || defined(__CUDABE__) || \
+    defined(__GNUC__) || defined(_WIN64)
+
+#define __builtin_align__(a) \
+        __align__(a)
+
+#else /* __CUDACC__ || __CUDABE__ || __GNUC__ || _WIN64 */
+
+#define __builtin_align__(a)
+
+#endif /* __CUDACC__ || __CUDABE__ || __GNUC__  || _WIN64 */
+
+#define __host__ \
+        __location__(host)
+#define __device__ \
+        __location__(device)
+#define __global__ \
+        __location__(global)
+#define __shared__ \
+        __location__(shared)
+#define __constant__ \
+        __location__(constant)
+#define __managed__ \
+        __location__(managed)
+        
+#if defined(__CUDABE__) || !defined(__CUDACC__)
+#define __device_builtin__
+#define __device_builtin_texture_type__
+#define __device_builtin_surface_type__
+#define __cudart_builtin__
+#else /* __CUDABE__  || !__CUDACC__ */
+#define __device_builtin__ \
+        __location__(device_builtin)
+#define __device_builtin_texture_type__ \
+        __location__(device_builtin_texture_type)
+#define __device_builtin_surface_type__ \
+        __location__(device_builtin_surface_type)
+#define __cudart_builtin__ \
+        __location__(cudart_builtin)
+#endif /* __CUDABE__ || !__CUDACC__ */
+
+struct __device_builtin__ dim3
+{
+	unsigned int x, y, z;
+#if defined(__cplusplus)
+	__host__ __device__ dim3(unsigned int vx = 1, unsigned int vy = 1, unsigned int vz = 1) : x(vx), y(vy), z(vz) {}
+#endif /* __cplusplus */
+};
+
+typedef __device_builtin__ struct dim3 dim3;
+
+/**
+ * CUDA error types
+ */
+enum __device_builtin__ cudaError
+{
+    /**
+     * The API call returned with no errors. In the case of query calls, this
+     * can also mean that the operation being queried is complete (see
+     * ::cudaEventQuery() and ::cudaStreamQuery()).
+     */
+    cudaSuccess                           =      0,
+  
+    /**
+     * The device function being invoked (usually via ::cudaLaunch()) was not
+     * previously configured via the ::cudaConfigureCall() function.
+     */
+    cudaErrorMissingConfiguration         =      1,
+  
+    /**
+     * The API call failed because it was unable to allocate enough memory to
+     * perform the requested operation.
+     */
+    cudaErrorMemoryAllocation             =      2,
+  
+    /**
+     * The API call failed because the CUDA driver and runtime could not be
+     * initialized.
+     */
+    cudaErrorInitializationError          =      3,
+  
+    /**
+     * An exception occurred on the device while executing a kernel. Common
+     * causes include dereferencing an invalid device pointer and accessing
+     * out of bounds shared memory. The device cannot be used until
+     * ::cudaThreadExit() is called. All existing device memory allocations
+     * are invalid and must be reconstructed if the program is to continue
+     * using CUDA.
+     */
+    cudaErrorLaunchFailure                =      4,
+  
+    /**
+     * This indicated that a previous kernel launch failed. This was previously
+     * used for device emulation of kernel launches.
+     * \deprecated
+     * This error return is deprecated as of CUDA 3.1. Device emulation mode was
+     * removed with the CUDA 3.1 release.
+     */
+    cudaErrorPriorLaunchFailure           =      5,
+  
+    /**
+     * This indicates that the device kernel took too long to execute. This can
+     * only occur if timeouts are enabled - see the device property
+     * \ref ::cudaDeviceProp::kernelExecTimeoutEnabled "kernelExecTimeoutEnabled"
+     * for more information. The device cannot be used until ::cudaThreadExit()
+     * is called. All existing device memory allocations are invalid and must be
+     * reconstructed if the program is to continue using CUDA.
+     */
+    cudaErrorLaunchTimeout                =      6,
+  
+    /**
+     * This indicates that a launch did not occur because it did not have
+     * appropriate resources. Although this error is similar to
+     * ::cudaErrorInvalidConfiguration, this error usually indicates that the
+     * user has attempted to pass too many arguments to the device kernel, or the
+     * kernel launch specifies too many threads for the kernel's register count.
+     */
+    cudaErrorLaunchOutOfResources         =      7,
+  
+    /**
+     * The requested device function does not exist or is not compiled for the
+     * proper device architecture.
+     */
+    cudaErrorInvalidDeviceFunction        =      8,
+  
+    /**
+     * This indicates that a kernel launch is requesting resources that can
+     * never be satisfied by the current device. Requesting more shared memory
+     * per block than the device supports will trigger this error, as will
+     * requesting too many threads or blocks. See ::cudaDeviceProp for more
+     * device limitations.
+     */
+    cudaErrorInvalidConfiguration         =      9,
+  
+    /**
+     * This indicates that the device ordinal supplied by the user does not
+     * correspond to a valid CUDA device.
+     */
+    cudaErrorInvalidDevice                =     10,
+  
+    /**
+     * This indicates that one or more of the parameters passed to the API call
+     * is not within an acceptable range of values.
+     */
+    cudaErrorInvalidValue                 =     11,
+  
+    /**
+     * This indicates that one or more of the pitch-related parameters passed
+     * to the API call is not within the acceptable range for pitch.
+     */
+    cudaErrorInvalidPitchValue            =     12,
+  
+    /**
+     * This indicates that the symbol name/identifier passed to the API call
+     * is not a valid name or identifier.
+     */
+    cudaErrorInvalidSymbol                =     13,
+  
+    /**
+     * This indicates that the buffer object could not be mapped.
+     */
+    cudaErrorMapBufferObjectFailed        =     14,
+  
+    /**
+     * This indicates that the buffer object could not be unmapped.
+     */
+    cudaErrorUnmapBufferObjectFailed      =     15,
+  
+    /**
+     * This indicates that at least one host pointer passed to the API call is
+     * not a valid host pointer.
+     */
+    cudaErrorInvalidHostPointer           =     16,
+  
+    /**
+     * This indicates that at least one device pointer passed to the API call is
+     * not a valid device pointer.
+     */
+    cudaErrorInvalidDevicePointer         =     17,
+  
+    /**
+     * This indicates that the texture passed to the API call is not a valid
+     * texture.
+     */
+    cudaErrorInvalidTexture               =     18,
+  
+    /**
+     * This indicates that the texture binding is not valid. This occurs if you
+     * call ::cudaGetTextureAlignmentOffset() with an unbound texture.
+     */
+    cudaErrorInvalidTextureBinding        =     19,
+  
+    /**
+     * This indicates that the channel descriptor passed to the API call is not
+     * valid. This occurs if the format is not one of the formats specified by
+     * ::cudaChannelFormatKind, or if one of the dimensions is invalid.
+     */
+    cudaErrorInvalidChannelDescriptor     =     20,
+  
+    /**
+     * This indicates that the direction of the memcpy passed to the API call is
+     * not one of the types specified by ::cudaMemcpyKind.
+     */
+    cudaErrorInvalidMemcpyDirection       =     21,
+  
+    /**
+     * This indicated that the user has taken the address of a constant variable,
+     * which was forbidden up until the CUDA 3.1 release.
+     * \deprecated
+     * This error return is deprecated as of CUDA 3.1. Variables in constant
+     * memory may now have their address taken by the runtime via
+     * ::cudaGetSymbolAddress().
+     */
+    cudaErrorAddressOfConstant            =     22,
+  
+    /**
+     * This indicated that a texture fetch was not able to be performed.
+     * This was previously used for device emulation of texture operations.
+     * \deprecated
+     * This error return is deprecated as of CUDA 3.1. Device emulation mode was
+     * removed with the CUDA 3.1 release.
+     */
+    cudaErrorTextureFetchFailed           =     23,
+  
+    /**
+     * This indicated that a texture was not bound for access.
+     * This was previously used for device emulation of texture operations.
+     * \deprecated
+     * This error return is deprecated as of CUDA 3.1. Device emulation mode was
+     * removed with the CUDA 3.1 release.
+     */
+    cudaErrorTextureNotBound              =     24,
+  
+    /**
+     * This indicated that a synchronization operation had failed.
+     * This was previously used for some device emulation functions.
+     * \deprecated
+     * This error return is deprecated as of CUDA 3.1. Device emulation mode was
+     * removed with the CUDA 3.1 release.
+     */
+    cudaErrorSynchronizationError         =     25,
+  
+    /**
+     * This indicates that a non-float texture was being accessed with linear
+     * filtering. This is not supported by CUDA.
+     */
+    cudaErrorInvalidFilterSetting         =     26,
+  
+    /**
+     * This indicates that an attempt was made to read a non-float texture as a
+     * normalized float. This is not supported by CUDA.
+     */
+    cudaErrorInvalidNormSetting           =     27,
+  
+    /**
+     * Mixing of device and device emulation code was not allowed.
+     * \deprecated
+     * This error return is deprecated as of CUDA 3.1. Device emulation mode was
+     * removed with the CUDA 3.1 release.
+     */
+    cudaErrorMixedDeviceExecution         =     28,
+  
+    /**
+     * This indicates that a CUDA Runtime API call cannot be executed because
+     * it is being called during process shut down, at a point in time after
+     * CUDA driver has been unloaded.
+     */
+    cudaErrorCudartUnloading              =     29,
+  
+    /**
+     * This indicates that an unknown internal error has occurred.
+     */
+    cudaErrorUnknown                      =     30,
+
+    /**
+     * This indicates that the API call is not yet implemented. Production
+     * releases of CUDA will never return this error.
+     * \deprecated
+     * This error return is deprecated as of CUDA 4.1.
+     */
+    cudaErrorNotYetImplemented            =     31,
+  
+    /**
+     * This indicated that an emulated device pointer exceeded the 32-bit address
+     * range.
+     * \deprecated
+     * This error return is deprecated as of CUDA 3.1. Device emulation mode was
+     * removed with the CUDA 3.1 release.
+     */
+    cudaErrorMemoryValueTooLarge          =     32,
+  
+    /**
+     * This indicates that a resource handle passed to the API call was not
+     * valid. Resource handles are opaque types like ::cudaStream_t and
+     * ::cudaEvent_t.
+     */
+    cudaErrorInvalidResourceHandle        =     33,
+  
+    /**
+     * This indicates that asynchronous operations issued previously have not
+     * completed yet. This result is not actually an error, but must be indicated
+     * differently than ::cudaSuccess (which indicates completion). Calls that
+     * may return this value include ::cudaEventQuery() and ::cudaStreamQuery().
+     */
+    cudaErrorNotReady                     =     34,
+  
+    /**
+     * This indicates that the installed NVIDIA CUDA driver is older than the
+     * CUDA runtime library. This is not a supported configuration. Users should
+     * install an updated NVIDIA display driver to allow the application to run.
+     */
+    cudaErrorInsufficientDriver           =     35,
+  
+    /**
+     * This indicates that the user has called ::cudaSetValidDevices(),
+     * ::cudaSetDeviceFlags(), ::cudaD3D9SetDirect3DDevice(),
+     * ::cudaD3D10SetDirect3DDevice, ::cudaD3D11SetDirect3DDevice(), or
+     * ::cudaVDPAUSetVDPAUDevice() after initializing the CUDA runtime by
+     * calling non-device management operations (allocating memory and
+     * launching kernels are examples of non-device management operations).
+     * This error can also be returned if using runtime/driver
+     * interoperability and there is an existing ::CUcontext active on the
+     * host thread.
+     */
+    cudaErrorSetOnActiveProcess           =     36,
+  
+    /**
+     * This indicates that the surface passed to the API call is not a valid
+     * surface.
+     */
+    cudaErrorInvalidSurface               =     37,
+  
+    /**
+     * This indicates that no CUDA-capable devices were detected by the installed
+     * CUDA driver.
+     */
+    cudaErrorNoDevice                     =     38,
+  
+    /**
+     * This indicates that an uncorrectable ECC error was detected during
+     * execution.
+     */
+    cudaErrorECCUncorrectable             =     39,
+  
+    /**
+     * This indicates that a link to a shared object failed to resolve.
+     */
+    cudaErrorSharedObjectSymbolNotFound   =     40,
+  
+    /**
+     * This indicates that initialization of a shared object failed.
+     */
+    cudaErrorSharedObjectInitFailed       =     41,
+  
+    /**
+     * This indicates that the ::cudaLimit passed to the API call is not
+     * supported by the active device.
+     */
+    cudaErrorUnsupportedLimit             =     42,
+  
+    /**
+     * This indicates that multiple global or constant variables (across separate
+     * CUDA source files in the application) share the same string name.
+     */
+    cudaErrorDuplicateVariableName        =     43,
+  
+    /**
+     * This indicates that multiple textures (across separate CUDA source
+     * files in the application) share the same string name.
+     */
+    cudaErrorDuplicateTextureName         =     44,
+  
+    /**
+     * This indicates that multiple surfaces (across separate CUDA source
+     * files in the application) share the same string name.
+     */
+    cudaErrorDuplicateSurfaceName         =     45,
+  
+    /**
+     * This indicates that all CUDA devices are busy or unavailable at the current
+     * time. Devices are often busy/unavailable due to use of
+     * ::cudaComputeModeExclusive, ::cudaComputeModeProhibited or when long
+     * running CUDA kernels have filled up the GPU and are blocking new work
+     * from starting. They can also be unavailable due to memory constraints
+     * on a device that already has active CUDA work being performed.
+     */
+    cudaErrorDevicesUnavailable           =     46,
+  
+    /**
+     * This indicates that the device kernel image is invalid.
+     */
+    cudaErrorInvalidKernelImage           =     47,
+  
+    /**
+     * This indicates that there is no kernel image available that is suitable
+     * for the device. This can occur when a user specifies code generation
+     * options for a particular CUDA source file that do not include the
+     * corresponding device configuration.
+     */
+    cudaErrorNoKernelImageForDevice       =     48,
+  
+    /**
+     * This indicates that the current context is not compatible with this
+     * the CUDA Runtime. This can only occur if you are using CUDA
+     * Runtime/Driver interoperability and have created an existing Driver
+     * context using the driver API. The Driver context may be incompatible
+     * either because the Driver context was created using an older version 
+     * of the API, because the Runtime API call expects a primary driver 
+     * context and the Driver context is not primary, or because the Driver 
+     * context has been destroyed. Please see \ref CUDART_DRIVER "Interactions 
+     * with the CUDA Driver API" for more information.
+     */
+    cudaErrorIncompatibleDriverContext    =     49,
+      
+    /**
+     * This error indicates that a call to ::cudaDeviceEnablePeerAccess() is
+     * trying to re-enable peer addressing on from a context which has already
+     * had peer addressing enabled.
+     */
+    cudaErrorPeerAccessAlreadyEnabled     =     50,
+    
+    /**
+     * This error indicates that ::cudaDeviceDisablePeerAccess() is trying to 
+     * disable peer addressing which has not been enabled yet via 
+     * ::cudaDeviceEnablePeerAccess().
+     */
+    cudaErrorPeerAccessNotEnabled         =     51,
+    
+    /**
+     * This indicates that a call tried to access an exclusive-thread device that 
+     * is already in use by a different thread.
+     */
+    cudaErrorDeviceAlreadyInUse           =     54,
+
+    /**
+     * This indicates profiler is not initialized for this run. This can
+     * happen when the application is running with external profiling tools
+     * like visual profiler.
+     */
+    cudaErrorProfilerDisabled             =     55,
+
+    /**
+     * \deprecated
+     * This error return is deprecated as of CUDA 5.0. It is no longer an error
+     * to attempt to enable/disable the profiling via ::cudaProfilerStart or
+     * ::cudaProfilerStop without initialization.
+     */
+    cudaErrorProfilerNotInitialized       =     56,
+
+    /**
+     * \deprecated
+     * This error return is deprecated as of CUDA 5.0. It is no longer an error
+     * to call cudaProfilerStart() when profiling is already enabled.
+     */
+    cudaErrorProfilerAlreadyStarted       =     57,
+
+    /**
+     * \deprecated
+     * This error return is deprecated as of CUDA 5.0. It is no longer an error
+     * to call cudaProfilerStop() when profiling is already disabled.
+     */
+     cudaErrorProfilerAlreadyStopped       =    58,
+
+    /**
+     * An assert triggered in device code during kernel execution. The device
+     * cannot be used again until ::cudaThreadExit() is called. All existing 
+     * allocations are invalid and must be reconstructed if the program is to
+     * continue using CUDA. 
+     */
+    cudaErrorAssert                        =    59,
+  
+    /**
+     * This error indicates that the hardware resources required to enable
+     * peer access have been exhausted for one or more of the devices 
+     * passed to ::cudaEnablePeerAccess().
+     */
+    cudaErrorTooManyPeers                 =     60,
+  
+    /**
+     * This error indicates that the memory range passed to ::cudaHostRegister()
+     * has already been registered.
+     */
+    cudaErrorHostMemoryAlreadyRegistered  =     61,
+        
+    /**
+     * This error indicates that the pointer passed to ::cudaHostUnregister()
+     * does not correspond to any currently registered memory region.
+     */
+    cudaErrorHostMemoryNotRegistered      =     62,
+
+    /**
+     * This error indicates that an OS call failed.
+     */
+    cudaErrorOperatingSystem              =     63,
+
+    /**
+     * This error indicates that P2P access is not supported across the given
+     * devices.
+     */
+    cudaErrorPeerAccessUnsupported        =     64,
+
+    /**
+     * This error indicates that a device runtime grid launch did not occur 
+     * because the depth of the child grid would exceed the maximum supported
+     * number of nested grid launches. 
+     */
+    cudaErrorLaunchMaxDepthExceeded       =     65,
+
+    /**
+     * This error indicates that a grid launch did not occur because the kernel 
+     * uses file-scoped textures which are unsupported by the device runtime. 
+     * Kernels launched via the device runtime only support textures created with 
+     * the Texture Object API's.
+     */
+    cudaErrorLaunchFileScopedTex          =     66,
+
+    /**
+     * This error indicates that a grid launch did not occur because the kernel 
+     * uses file-scoped surfaces which are unsupported by the device runtime.
+     * Kernels launched via the device runtime only support surfaces created with
+     * the Surface Object API's.
+     */
+    cudaErrorLaunchFileScopedSurf         =     67,
+
+    /**
+     * This error indicates that a call to ::cudaDeviceSynchronize made from
+     * the device runtime failed because the call was made at grid depth greater
+     * than than either the default (2 levels of grids) or user specified device 
+     * limit ::cudaLimitDevRuntimeSyncDepth. To be able to synchronize on 
+     * launched grids at a greater depth successfully, the maximum nested 
+     * depth at which ::cudaDeviceSynchronize will be called must be specified 
+     * with the ::cudaLimitDevRuntimeSyncDepth limit to the ::cudaDeviceSetLimit
+     * api before the host-side launch of a kernel using the device runtime. 
+     * Keep in mind that additional levels of sync depth require the runtime 
+     * to reserve large amounts of device memory that cannot be used for 
+     * user allocations.
+     */
+    cudaErrorSyncDepthExceeded            =     68,
+
+    /**
+     * This error indicates that a device runtime grid launch failed because
+     * the launch would exceed the limit ::cudaLimitDevRuntimePendingLaunchCount.
+     * For this launch to proceed successfully, ::cudaDeviceSetLimit must be
+     * called to set the ::cudaLimitDevRuntimePendingLaunchCount to be higher 
+     * than the upper bound of outstanding launches that can be issued to the
+     * device runtime. Keep in mind that raising the limit of pending device
+     * runtime launches will require the runtime to reserve device memory that
+     * cannot be used for user allocations.
+     */
+    cudaErrorLaunchPendingCountExceeded   =     69,
+    
+    /**
+     * This error indicates the attempted operation is not permitted.
+     */
+    cudaErrorNotPermitted                 =     70,
+
+    /**
+     * This error indicates the attempted operation is not supported
+     * on the current system or device.
+     */
+    cudaErrorNotSupported                 =     71,
+
+    /**
+     * Device encountered an error in the call stack during kernel execution,
+     * possibly due to stack corruption or exceeding the stack size limit.
+     * The context cannot be used, so it must be destroyed (and a new one should be created).
+     * All existing device memory allocations from this context are invalid
+     * and must be reconstructed if the program is to continue using CUDA.
+     */
+    cudaErrorHardwareStackError           =     72,
+
+    /**
+     * The device encountered an illegal instruction during kernel execution
+     * The context cannot be used, so it must be destroyed (and a new one should be created).
+     * All existing device memory allocations from this context are invalid
+     * and must be reconstructed if the program is to continue using CUDA.
+     */
+    cudaErrorIllegalInstruction           =     73,
+
+    /**
+     * The device encountered a load or store instruction
+     * on a memory address which is not aligned.
+     * The context cannot be used, so it must be destroyed (and a new one should be created).
+     * All existing device memory allocations from this context are invalid
+     * and must be reconstructed if the program is to continue using CUDA.
+     */
+    cudaErrorMisalignedAddress            =     74,
+
+    /**
+     * While executing a kernel, the device encountered an instruction
+     * which can only operate on memory locations in certain address spaces
+     * (global, shared, or local), but was supplied a memory address not
+     * belonging to an allowed address space.
+     * The context cannot be used, so it must be destroyed (and a new one should be created).
+     * All existing device memory allocations from this context are invalid
+     * and must be reconstructed if the program is to continue using CUDA.
+     */
+    cudaErrorInvalidAddressSpace          =     75,
+
+    /**
+     * The device encountered an invalid program counter.
+     * The context cannot be used, so it must be destroyed (and a new one should be created).
+     * All existing device memory allocations from this context are invalid
+     * and must be reconstructed if the program is to continue using CUDA.
+     */
+    cudaErrorInvalidPc                    =     76,
+
+    /**
+     * The device encountered a load or store instruction on an invalid memory address.
+     * The context cannot be used, so it must be destroyed (and a new one should be created).
+     * All existing device memory allocations from this context are invalid
+     * and must be reconstructed if the program is to continue using CUDA.
+     */
+    cudaErrorIllegalAddress               =     77,
+
+    /**
+     * A PTX compilation failed. The runtime may fall back to compiling PTX if
+     * an application does not contain a suitable binary for the current device.
+     */
+    cudaErrorInvalidPtx                   =     78,
+
+    /**
+     * This indicates an error with the OpenGL or DirectX context.
+     */
+    cudaErrorInvalidGraphicsContext       =     79,
+
+
+    /**
+     * This indicates an internal startup failure in the CUDA runtime.
+     */
+    cudaErrorStartupFailure               =   0x7f,
+
+    /**
+     * Any unhandled CUDA driver error is added to this value and returned via
+     * the runtime. Production releases of CUDA should not return such errors.
+     * \deprecated
+     * This error return is deprecated as of CUDA 4.1.
+     */
+    cudaErrorApiFailureBase               =  10000
+};
diff --git a/webrtc/modules/video_coding/codecs/h264/include/dynlink_cuda.h b/webrtc/modules/video_coding/codecs/h264/include/dynlink_cuda.h
new file mode 100644
index 0000000..13d0d32
--- /dev/null
+++ b/webrtc/modules/video_coding/codecs/h264/include/dynlink_cuda.h
@@ -0,0 +1,21 @@
+/*
+ * Copyright 1993-2015 NVIDIA Corporation.  All rights reserved.
+ *
+ * Please refer to the NVIDIA end user license agreement (EULA) associated
+ * with this source code for terms and conditions that govern your use of
+ * this software. Any use, reproduction, disclosure, or distribution of
+ * this software and related documentation outside the terms of the EULA
+ * is strictly prohibited.
+ *
+ */
+ 
+#ifndef __cuda_h__
+#define __cuda_h__
+
+/**
+* CUDA API version support
+*/
+
+#include "dynlink_cuda_cuda.h"
+
+#endif //__cuda_h__
diff --git a/webrtc/modules/video_coding/codecs/h264/include/dynlink_cudaD3D10.h b/webrtc/modules/video_coding/codecs/h264/include/dynlink_cudaD3D10.h
new file mode 100644
index 0000000..49245af
--- /dev/null
+++ b/webrtc/modules/video_coding/codecs/h264/include/dynlink_cudaD3D10.h
@@ -0,0 +1,179 @@
+/*
+ * Copyright 1993-2015 NVIDIA Corporation.  All rights reserved.
+ *
+ * Please refer to the NVIDIA end user license agreement (EULA) associated
+ * with this source code for terms and conditions that govern your use of
+ * this software. Any use, reproduction, disclosure, or distribution of
+ * this software and related documentation outside the terms of the EULA
+ * is strictly prohibited.
+ *
+ */
+ 
+#ifndef CUDAD3D10_H
+#define CUDAD3D10_H
+
+#if INIT_CUDA_D3D
+
+/**
+ * CUDA API versioning support
+ */
+#if defined(CUDA_FORCE_API_VERSION)
+    #if (CUDA_FORCE_API_VERSION == 3010)
+        #define __CUDA_API_VERSION 3010
+    #else
+        #error "Unsupported value of CUDA_FORCE_API_VERSION"
+    #endif
+#else
+    #define __CUDA_API_VERSION 3020
+#endif /* CUDA_FORCE_API_VERSION */
+
+#if defined(__CUDA_API_VERSION_INTERNAL) || __CUDA_API_VERSION >= 3020
+    #define cuD3D10CtxCreate                    cuD3D10CtxCreate_v2
+    #define cuD3D10ResourceGetSurfaceDimensions cuD3D10ResourceGetSurfaceDimensions_v2
+    #define cuD3D10ResourceGetMappedPointer     cuD3D10ResourceGetMappedPointer_v2
+    #define cuD3D10ResourceGetMappedSize        cuD3D10ResourceGetMappedSize_v2
+    #define cuD3D10ResourceGetMappedPitch       cuD3D10ResourceGetMappedPitch_v2
+#endif /* __CUDA_API_VERSION_INTERNAL || __CUDA_API_VERSION >= 3020 */
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/**
+ * \defgroup CUDA_D3D10 Direct3D 10 Interoperability
+ * \ingroup CUDA_DRIVER
+ *
+ * ___MANBRIEF___ Direct3D 10 interoperability functions of the low-level CUDA
+ * driver API (___CURRENT_FILE___) ___ENDMANBRIEF___
+ *
+ * This section describes the Direct3D 10 interoperability functions of the
+ * low-level CUDA driver application programming interface. Note that mapping 
+ * of Direct3D 10 resources is performed with the graphics API agnostic, resource 
+ * mapping interface described in \ref CUDA_GRAPHICS "Graphics Interoperability".
+ *
+ * @{
+ */
+
+/**
+ * CUDA devices corresponding to a D3D10 device
+ */
+typedef enum CUd3d10DeviceList_enum {
+    CU_D3D10_DEVICE_LIST_ALL            = 0x01, /**< The CUDA devices for all GPUs used by a D3D10 device */
+    CU_D3D10_DEVICE_LIST_CURRENT_FRAME  = 0x02, /**< The CUDA devices for the GPUs used by a D3D10 device in its currently rendering frame */
+    CU_D3D10_DEVICE_LIST_NEXT_FRAME     = 0x03, /**< The CUDA devices for the GPUs to be used by a D3D10 device in the next frame */
+} CUd3d10DeviceList;
+
+typedef CUresult CUDAAPI tcuD3D10GetDevice(CUdevice *pCudaDevice, IDXGIAdapter *pAdapter);
+typedef CUresult CUDAAPI tcuD3D10GetDevices(unsigned int *pCudaDeviceCount, CUdevice *pCudaDevices, unsigned int cudaDeviceCount, ID3D10Device *pD3D10Device, CUd3d10DeviceList deviceList);
+typedef CUresult CUDAAPI tcuGraphicsD3D10RegisterResource(CUgraphicsResource *pCudaResource, ID3D10Resource *pD3DResource, unsigned int Flags);
+
+/**
+ * \defgroup CUDA_D3D10_DEPRECATED Direct3D 10 Interoperability [DEPRECATED]
+ *
+ * ___MANBRIEF___ deprecated Direct3D 10 interoperability functions of the 
+ * low-level CUDA driver API (___CURRENT_FILE___) ___ENDMANBRIEF___
+ *
+ * This section describes deprecated Direct3D 10 interoperability functionality.
+ * @{
+ */
+
+/** Flags to register a resource */
+typedef enum CUD3D10register_flags_enum {
+    CU_D3D10_REGISTER_FLAGS_NONE  = 0x00,
+    CU_D3D10_REGISTER_FLAGS_ARRAY = 0x01,
+} CUD3D10register_flags;
+
+/** Flags to map or unmap a resource */
+typedef enum CUD3D10map_flags_enum {
+    CU_D3D10_MAPRESOURCE_FLAGS_NONE         = 0x00,
+    CU_D3D10_MAPRESOURCE_FLAGS_READONLY     = 0x01,
+    CU_D3D10_MAPRESOURCE_FLAGS_WRITEDISCARD = 0x02,
+} CUD3D10map_flags;
+
+
+#if __CUDA_API_VERSION >= 3020
+    typedef CUresult CUDAAPI tcuD3D10CtxCreate(CUcontext *pCtx, CUdevice *pCudaDevice, unsigned int Flags, ID3D10Device *pD3DDevice);
+    typedef CUresult CUDAAPI tcuD3D10CtxCreateOnDevice(CUcontext *pCtx, unsigned int flags, ID3D10Device *pD3DDevice, CUdevice cudaDevice);
+    typedef CUresult CUDAAPI tcuD3D10GetDirect3DDevice(ID3D10Device **ppD3DDevice);
+#endif /* __CUDA_API_VERSION >= 3020 */
+
+typedef CUresult CUDAAPI tcuD3D10RegisterResource(ID3D10Resource *pResource, unsigned int Flags);
+typedef CUresult CUDAAPI tcuD3D10UnregisterResource(ID3D10Resource *pResource);
+typedef CUresult CUDAAPI tcuD3D10MapResources(unsigned int count, ID3D10Resource **ppResources);
+typedef CUresult CUDAAPI tcuD3D10UnmapResources(unsigned int count, ID3D10Resource **ppResources);
+typedef CUresult CUDAAPI tcuD3D10ResourceSetMapFlags(ID3D10Resource *pResource, unsigned int Flags);
+typedef CUresult CUDAAPI tcuD3D10ResourceGetMappedArray(CUarray *pArray, ID3D10Resource *pResource, unsigned int SubResource);
+
+#if __CUDA_API_VERSION >= 3020
+    typedef CUresult CUDAAPI tcuD3D10ResourceGetMappedPointer(CUdeviceptr *pDevPtr, ID3D10Resource *pResource, unsigned int SubResource);
+    typedef CUresult CUDAAPI tcuD3D10ResourceGetMappedSize(size_t *pSize, ID3D10Resource *pResource, unsigned int SubResource);
+    typedef CUresult CUDAAPI tcuD3D10ResourceGetMappedPitch(size_t *pPitch, size_t *pPitchSlice, ID3D10Resource *pResource, unsigned int SubResource);
+    typedef CUresult CUDAAPI tcuD3D10ResourceGetSurfaceDimensions(size_t *pWidth, size_t *pHeight, size_t *pDepth, ID3D10Resource *pResource, unsigned int SubResource);
+#endif /* __CUDA_API_VERSION >= 3020 */
+
+/** @} */ /* END CUDA_D3D10_DEPRECATED */
+/** @} */ /* END CUDA_D3D10 */
+
+/**
+ * CUDA API versioning support
+ */
+#if defined(__CUDA_API_VERSION_INTERNAL)
+    #undef cuD3D10CtxCreate
+    #undef cuD3D10ResourceGetSurfaceDimensions
+    #undef cuD3D10ResourceGetMappedPointer
+    #undef cuD3D10ResourceGetMappedSize
+    #undef cuD3D10ResourceGetMappedPitch
+#endif /* __CUDA_API_VERSION_INTERNAL */
+
+/**
+ * CUDA API made obselete at API version 3020
+ */
+#if defined(__CUDA_API_VERSION_INTERNAL)
+    #define CUdeviceptr CUdeviceptr_v1
+#endif /* __CUDA_API_VERSION_INTERNAL */
+
+#if defined(__CUDA_API_VERSION_INTERNAL) || __CUDA_API_VERSION < 3020
+    typedef CUresult CUDAAPI tcuD3D10CtxCreate(CUcontext *pCtx, CUdevice *pCudaDevice, unsigned int Flags, ID3D10Device *pD3DDevice);
+    typedef CUresult CUDAAPI tcuD3D10ResourceGetMappedPitch(unsigned int *pPitch, unsigned int *pPitchSlice, ID3D10Resource *pResource, unsigned int SubResource);
+    typedef CUresult CUDAAPI tcuD3D10ResourceGetMappedPointer(CUdeviceptr *pDevPtr, ID3D10Resource *pResource, unsigned int SubResource);
+    typedef CUresult CUDAAPI tcuD3D10ResourceGetMappedSize(unsigned int *pSize, ID3D10Resource *pResource, unsigned int SubResource);
+    typedef CUresult CUDAAPI tcuD3D10ResourceGetSurfaceDimensions(unsigned int *pWidth, unsigned int *pHeight, unsigned int *pDepth, ID3D10Resource *pResource, unsigned int SubResource);
+#endif /* __CUDA_API_VERSION_INTERNAL || __CUDA_API_VERSION < 3020 */
+
+#if defined(__CUDA_API_VERSION_INTERNAL)
+    #undef CUdeviceptr
+#endif /* __CUDA_API_VERSION_INTERNAL */
+
+#ifdef __cplusplus
+};
+#endif
+
+extern tcuD3D10GetDevice                     *cuD3D10GetDevice;
+extern tcuD3D10CtxCreate                     *cuD3D10CtxCreate;
+extern tcuGraphicsD3D10RegisterResource      *cuGraphicsD3D10RegisterResource;
+
+extern tcuD3D10RegisterResource              *cuD3D10RegisterResource;
+extern tcuD3D10UnregisterResource            *cuD3D10UnregisterResource;
+extern tcuD3D10MapResources                  *cuD3D10MapResources;
+extern tcuD3D10UnmapResources                *cuD3D10UnmapResources;
+extern tcuD3D10ResourceSetMapFlags           *cuD3D10ResourceSetMapFlags;
+extern tcuD3D10ResourceGetMappedArray        *cuD3D10ResourceGetMappedArray;
+
+#if (__CUDA_API_VERSION >= 3020)
+extern tcuD3D10ResourceGetMappedPointer      *cuD3D10ResourceGetMappedPointer;
+extern tcuD3D10ResourceGetMappedSize         *cuD3D10ResourceGetMappedSize;
+extern tcuD3D10ResourceGetMappedPitch        *cuD3D10ResourceGetMappedPitch;
+extern tcuD3D10ResourceGetSurfaceDimensions  *cuD3D10ResourceGetSurfaceDimensions;
+#endif /* __CUDA_API_VERSION >= 3020 */
+
+#if (__CUDA_API_VERSION < 3020)
+extern tcuD3D10CtxCreate                     *cuD3D10CtxCreate;
+extern tcuD3D10ResourceGetMappedPitch        *cuD3D10ResourceGetMappedPitch;
+extern tcuD3D10ResourceGetMappedPointer      *cuD3D10ResourceGetMappedPointer;
+extern tcuD3D10ResourceGetMappedSize         *cuD3D10ResourceGetMappedSize;
+extern tcuD3D10ResourceGetSurfaceDimensions  *cuD3D10ResourceGetSurfaceDimensions;
+#endif /* __CUDA_API_VERSION < 3020 */
+
+#endif
+
+#endif
diff --git a/webrtc/modules/video_coding/codecs/h264/include/dynlink_cudaD3D11.h b/webrtc/modules/video_coding/codecs/h264/include/dynlink_cudaD3D11.h
new file mode 100644
index 0000000..50b6e0e
--- /dev/null
+++ b/webrtc/modules/video_coding/codecs/h264/include/dynlink_cudaD3D11.h
@@ -0,0 +1,344 @@
+/*
+ * Copyright 1993-2015 NVIDIA Corporation.  All rights reserved.
+ *
+ * Please refer to the NVIDIA end user license agreement (EULA) associated
+ * with this source code for terms and conditions that govern your use of
+ * this software. Any use, reproduction, disclosure, or distribution of
+ * this software and related documentation outside the terms of the EULA
+ * is strictly prohibited.
+ *
+ */
+ 
+#ifndef CUDAD3D11_H
+#define CUDAD3D11_H
+
+#if INIT_CUDA_D3D11
+
+#include <d3d11.h>
+
+/**
+ * CUDA API versioning support
+ */
+#if defined(CUDA_FORCE_API_VERSION)
+    #if (CUDA_FORCE_API_VERSION == 3010)
+        #define __CUDA_API_VERSION 3010
+    #else
+        #error "Unsupported value of CUDA_FORCE_API_VERSION"
+    #endif
+#else
+#ifndef __CUDA_API_VERSION
+    #define __CUDA_API_VERSION 3020
+#endif
+#endif /* CUDA_FORCE_API_VERSION */
+
+#if defined(__CUDA_API_VERSION_INTERNAL) || __CUDA_API_VERSION >= 3020
+    #define cuD3D11CtxCreate cuD3D11CtxCreate_v2
+#endif /* __CUDA_API_VERSION_INTERNAL || __CUDA_API_VERSION >= 3020 */
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/**
+ * \defgroup CUDA_D3D11 Direct3D 11 Interoperability
+ * \ingroup CUDA_DRIVER
+ *
+ * ___MANBRIEF___ Direct3D 11 interoperability functions of the low-level CUDA
+ * driver API (___CURRENT_FILE___) ___ENDMANBRIEF___
+ *
+ * This section describes the Direct3D 11 interoperability functions of the
+ * low-level CUDA driver application programming interface. Note that mapping 
+ * of Direct3D 11 resources is performed with the graphics API agnostic, resource 
+ * mapping interface described in \ref CUDA_GRAPHICS "Graphics Interoperability".
+ *
+ * @{
+ */
+
+/**
+ * CUDA devices corresponding to a D3D11 device
+ */
+typedef enum CUd3d11DeviceList_enum {
+    CU_D3D11_DEVICE_LIST_ALL            = 0x01, /**< The CUDA devices for all GPUs used by a D3D11 device */
+    CU_D3D11_DEVICE_LIST_CURRENT_FRAME  = 0x02, /**< The CUDA devices for the GPUs used by a D3D11 device in its currently rendering frame */
+    CU_D3D11_DEVICE_LIST_NEXT_FRAME     = 0x03, /**< The CUDA devices for the GPUs to be used by a D3D11 device in the next frame */
+} CUd3d11DeviceList;
+
+/**
+ * \brief Gets the CUDA device corresponding to a display adapter.
+ *
+ * Returns in \p *pCudaDevice the CUDA-compatible device corresponding to the
+ * adapter \p pAdapter obtained from ::IDXGIFactory::EnumAdapters.
+ *
+ * If no device on \p pAdapter is CUDA-compatible the call will return
+ * ::CUDA_ERROR_NO_DEVICE.
+ *
+ * \param pCudaDevice - Returned CUDA device corresponding to \p pAdapter
+ * \param pAdapter    - Adapter to query for CUDA device
+ *
+ * \return
+ * ::CUDA_SUCCESS,
+ * ::CUDA_ERROR_DEINITIALIZED,
+ * ::CUDA_ERROR_NOT_INITIALIZED,
+ * ::CUDA_ERROR_NO_DEVICE,
+ * ::CUDA_ERROR_INVALID_VALUE,
+ * ::CUDA_ERROR_NOT_FOUND,
+ * ::CUDA_ERROR_UNKNOWN
+ * \notefnerr
+ *
+ * \sa
+ * ::cuD3D11GetDevices
+ */
+typedef CUresult CUDAAPI tcuD3D11GetDevice(CUdevice *pCudaDevice, IDXGIAdapter *pAdapter);
+extern tcuD3D11GetDevice *cuD3D11GetDevice;
+
+/**
+ * \brief Gets the CUDA devices corresponding to a Direct3D 11 device
+ *
+ * Returns in \p *pCudaDeviceCount the number of CUDA-compatible device corresponding
+ * to the Direct3D 11 device \p pD3D11Device.
+ * Also returns in \p *pCudaDevices at most \p cudaDeviceCount of the CUDA-compatible devices
+ * corresponding to the Direct3D 11 device \p pD3D11Device.
+ *
+ * If any of the GPUs being used to render \p pDevice are not CUDA capable then the
+ * call will return ::CUDA_ERROR_NO_DEVICE.
+ *
+ * \param pCudaDeviceCount - Returned number of CUDA devices corresponding to \p pD3D11Device
+ * \param pCudaDevices     - Returned CUDA devices corresponding to \p pD3D11Device
+ * \param cudaDeviceCount  - The size of the output device array \p pCudaDevices
+ * \param pD3D11Device     - Direct3D 11 device to query for CUDA devices
+ * \param deviceList       - The set of devices to return.  This set may be
+ *                           ::CU_D3D11_DEVICE_LIST_ALL for all devices,
+ *                           ::CU_D3D11_DEVICE_LIST_CURRENT_FRAME for the devices used to
+ *                           render the current frame (in SLI), or
+ *                           ::CU_D3D11_DEVICE_LIST_NEXT_FRAME for the devices used to
+ *                           render the next frame (in SLI).
+ *
+ * \return
+ * ::CUDA_SUCCESS,
+ * ::CUDA_ERROR_DEINITIALIZED,
+ * ::CUDA_ERROR_NOT_INITIALIZED,
+ * ::CUDA_ERROR_NO_DEVICE,
+ * ::CUDA_ERROR_INVALID_VALUE,
+ * ::CUDA_ERROR_NOT_FOUND,
+ * ::CUDA_ERROR_UNKNOWN
+ * \notefnerr
+ *
+ * \sa
+ * ::cuD3D11GetDevice
+ */
+typedef CUresult CUDAAPI tcuD3D11GetDevices(unsigned int *pCudaDeviceCount, CUdevice *pCudaDevices, unsigned int cudaDeviceCount, ID3D11Device *pD3D11Device, CUd3d11DeviceList deviceList);
+extern tcuD3D11GetDevices *cuD3D11GetDevices;
+
+/**
+ * \brief Register a Direct3D 11 resource for access by CUDA
+ *
+ * Registers the Direct3D 11 resource \p pD3DResource for access by CUDA and
+ * returns a CUDA handle to \p pD3Dresource in \p pCudaResource.
+ * The handle returned in \p pCudaResource may be used to map and unmap this
+ * resource until it is unregistered.
+ * On success this call will increase the internal reference count on
+ * \p pD3DResource. This reference count will be decremented when this
+ * resource is unregistered through ::cuGraphicsUnregisterResource().
+ *
+ * This call is potentially high-overhead and should not be called every frame
+ * in interactive applications.
+ *
+ * The type of \p pD3DResource must be one of the following.
+ * - ::ID3D11Buffer: may be accessed through a device pointer.
+ * - ::ID3D11Texture1D: individual subresources of the texture may be accessed via arrays
+ * - ::ID3D11Texture2D: individual subresources of the texture may be accessed via arrays
+ * - ::ID3D11Texture3D: individual subresources of the texture may be accessed via arrays
+ *
+ * The \p Flags argument may be used to specify additional parameters at register
+ * time.  The valid values for this parameter are
+ * - ::CU_GRAPHICS_REGISTER_FLAGS_NONE: Specifies no hints about how this
+ *   resource will be used.
+ * - ::CU_GRAPHICS_REGISTER_FLAGS_SURFACE_LDST: Specifies that CUDA will
+ *   bind this resource to a surface reference.
+ * - ::CU_GRAPHICS_REGISTER_FLAGS_TEXTURE_GATHER: Specifies that CUDA will perform
+ *   texture gather operations on this resource.
+ *
+ * Not all Direct3D resources of the above types may be used for
+ * interoperability with CUDA.  The following are some limitations.
+ * - The primary rendertarget may not be registered with CUDA.
+ * - Resources allocated as shared may not be registered with CUDA.
+ * - Textures which are not of a format which is 1, 2, or 4 channels of 8, 16,
+ *   or 32-bit integer or floating-point data cannot be shared.
+ * - Surfaces of depth or stencil formats cannot be shared.
+ *
+ * A complete list of supported DXGI formats is as follows. For compactness the
+ * notation A_{B,C,D} represents A_B, A_C, and A_D.
+ * - DXGI_FORMAT_A8_UNORM
+ * - DXGI_FORMAT_B8G8R8A8_UNORM
+ * - DXGI_FORMAT_B8G8R8X8_UNORM
+ * - DXGI_FORMAT_R16_FLOAT
+ * - DXGI_FORMAT_R16G16B16A16_{FLOAT,SINT,SNORM,UINT,UNORM}
+ * - DXGI_FORMAT_R16G16_{FLOAT,SINT,SNORM,UINT,UNORM}
+ * - DXGI_FORMAT_R16_{SINT,SNORM,UINT,UNORM}
+ * - DXGI_FORMAT_R32_FLOAT
+ * - DXGI_FORMAT_R32G32B32A32_{FLOAT,SINT,UINT}
+ * - DXGI_FORMAT_R32G32_{FLOAT,SINT,UINT}
+ * - DXGI_FORMAT_R32_{SINT,UINT}
+ * - DXGI_FORMAT_R8G8B8A8_{SINT,SNORM,UINT,UNORM,UNORM_SRGB}
+ * - DXGI_FORMAT_R8G8_{SINT,SNORM,UINT,UNORM}
+ * - DXGI_FORMAT_R8_{SINT,SNORM,UINT,UNORM}
+ *
+ * If \p pD3DResource is of incorrect type or is already registered then
+ * ::CUDA_ERROR_INVALID_HANDLE is returned.
+ * If \p pD3DResource cannot be registered then ::CUDA_ERROR_UNKNOWN is returned.
+ * If \p Flags is not one of the above specified value then ::CUDA_ERROR_INVALID_VALUE
+ * is returned.
+ *
+ * \param pCudaResource - Returned graphics resource handle
+ * \param pD3DResource  - Direct3D resource to register
+ * \param Flags         - Parameters for resource registration
+ *
+ * \return
+ * ::CUDA_SUCCESS,
+ * ::CUDA_ERROR_DEINITIALIZED,
+ * ::CUDA_ERROR_NOT_INITIALIZED,
+ * ::CUDA_ERROR_INVALID_CONTEXT,
+ * ::CUDA_ERROR_INVALID_VALUE,
+ * ::CUDA_ERROR_INVALID_HANDLE,
+ * ::CUDA_ERROR_OUT_OF_MEMORY,
+ * ::CUDA_ERROR_UNKNOWN
+ * \notefnerr
+ *
+ * \sa
+ * ::cuGraphicsUnregisterResource,
+ * ::cuGraphicsMapResources,
+ * ::cuGraphicsSubResourceGetMappedArray,
+ * ::cuGraphicsResourceGetMappedPointer
+ */
+typedef CUresult CUDAAPI tcuGraphicsD3D11RegisterResource(CUgraphicsResource *pCudaResource, ID3D11Resource *pD3DResource, unsigned int Flags);
+extern tcuGraphicsD3D11RegisterResource *cuGraphicsD3D11RegisterResource;
+
+/**
+ * \defgroup CUDA_D3D11_DEPRECATED Direct3D 11 Interoperability [DEPRECATED]
+ *
+ * ___MANBRIEF___ deprecated Direct3D 11 interoperability functions of the
+ * low-level CUDA driver API (___CURRENT_FILE___) ___ENDMANBRIEF___
+ *
+ * This section describes deprecated Direct3D 11 interoperability functionality.
+ * @{
+ */
+
+#if __CUDA_API_VERSION >= 3020
+
+/**
+ * \brief Create a CUDA context for interoperability with Direct3D 11
+ *
+ * \deprecated This function is deprecated as of CUDA 5.0.
+ *
+ * This function is deprecated and should no longer be used.  It is
+ * no longer necessary to associate a CUDA context with a D3D11
+ * device in order to achieve maximum interoperability performance.
+ *
+ * \param pCtx        - Returned newly created CUDA context
+ * \param pCudaDevice - Returned pointer to the device on which the context was created
+ * \param Flags       - Context creation flags (see ::cuCtxCreate() for details)
+ * \param pD3DDevice  - Direct3D device to create interoperability context with
+ *
+ * \return
+ * ::CUDA_SUCCESS,
+ * ::CUDA_ERROR_DEINITIALIZED,
+ * ::CUDA_ERROR_NOT_INITIALIZED,
+ * ::CUDA_ERROR_INVALID_VALUE,
+ * ::CUDA_ERROR_OUT_OF_MEMORY,
+ * ::CUDA_ERROR_UNKNOWN
+ * \notefnerr
+ *
+ * \sa
+ * ::cuD3D11GetDevice,
+ * ::cuGraphicsD3D11RegisterResource
+ */
+typedef CUresult CUDAAPI tcuD3D11CtxCreate(CUcontext *pCtx, CUdevice *pCudaDevice, unsigned int Flags, ID3D11Device *pD3DDevice);
+typedef CUresult CUDAAPI tcuD3D11CtxCreate_v2(CUcontext *pCtx, CUdevice *pCudaDevice, unsigned int Flags, ID3D11Device *pD3DDevice);
+
+extern tcuD3D11CtxCreate *cuD3D11CtxCreate;
+
+/**
+ * \brief Create a CUDA context for interoperability with Direct3D 11
+ *
+ * \deprecated This function is deprecated as of CUDA 5.0.
+ *
+ * This function is deprecated and should no longer be used.  It is
+ * no longer necessary to associate a CUDA context with a D3D11
+ * device in order to achieve maximum interoperability performance.
+ *
+ * \param pCtx        - Returned newly created CUDA context
+ * \param flags       - Context creation flags (see ::cuCtxCreate() for details)
+ * \param pD3DDevice  - Direct3D device to create interoperability context with
+ * \param cudaDevice  - The CUDA device on which to create the context.  This device
+ *                      must be among the devices returned when querying
+ *                      ::CU_D3D11_DEVICES_ALL from  ::cuD3D11GetDevices.
+ *
+ * \return
+ * ::CUDA_SUCCESS,
+ * ::CUDA_ERROR_DEINITIALIZED,
+ * ::CUDA_ERROR_NOT_INITIALIZED,
+ * ::CUDA_ERROR_INVALID_VALUE,
+ * ::CUDA_ERROR_OUT_OF_MEMORY,
+ * ::CUDA_ERROR_UNKNOWN
+ * \notefnerr
+ *
+ * \sa
+ * ::cuD3D11GetDevices,
+ * ::cuGraphicsD3D11RegisterResource
+ */
+
+typedef CUresult CUDAAPI tcuD3D11CtxCreateOnDevice(CUcontext *pCtx, unsigned int flags, ID3D11Device *pD3DDevice, CUdevice cudaDevice);
+extern tcuD3D11CtxCreateOnDevice *cuD3D11CtxCreateOnDevice;
+
+/**
+ * \brief Get the Direct3D 11 device against which the current CUDA context was
+ * created
+ *
+ * \deprecated This function is deprecated as of CUDA 5.0.
+ *
+ * This function is deprecated and should no longer be used.  It is
+ * no longer necessary to associate a CUDA context with a D3D11
+ * device in order to achieve maximum interoperability performance.
+ *
+ * \param ppD3DDevice - Returned Direct3D device corresponding to CUDA context
+ *
+ * \return
+ * ::CUDA_SUCCESS,
+ * ::CUDA_ERROR_DEINITIALIZED,
+ * ::CUDA_ERROR_NOT_INITIALIZED,
+ * ::CUDA_ERROR_INVALID_CONTEXT
+ * \notefnerr
+ *
+ * \sa
+ * ::cuD3D11GetDevice
+ */
+typedef CUresult CUDAAPI tcuD3D11GetDirect3DDevice(ID3D11Device **ppD3DDevice);
+extern tcuD3D11GetDirect3DDevice *cuD3D11GetDirect3DDevice;
+
+#endif /* __CUDA_API_VERSION >= 3020 */
+
+/** @} */ /* END CUDA_D3D11_DEPRECATED */
+/** @} */ /* END CUDA_D3D11 */
+
+/**
+ * CUDA API versioning support
+ */
+#if defined(__CUDA_API_VERSION_INTERNAL)
+    #undef cuD3D11CtxCreate
+#endif /* __CUDA_API_VERSION_INTERNAL */
+
+/**
+ * CUDA API made obselete at API version 3020
+ */
+#if defined(__CUDA_API_VERSION_INTERNAL) || __CUDA_API_VERSION < 3020
+CUresult CUDAAPI cuD3D11CtxCreate(CUcontext *pCtx, CUdevice *pCudaDevice, unsigned int Flags, ID3D11Device *pD3DDevice);
+#endif /* __CUDA_API_VERSION_INTERNAL || __CUDA_API_VERSION < 3020 */
+
+#ifdef __cplusplus
+};
+#endif
+
+#endif
+
+#endif
+
diff --git a/webrtc/modules/video_coding/codecs/h264/include/dynlink_cudaD3D9.h b/webrtc/modules/video_coding/codecs/h264/include/dynlink_cudaD3D9.h
new file mode 100644
index 0000000..554469d
--- /dev/null
+++ b/webrtc/modules/video_coding/codecs/h264/include/dynlink_cudaD3D9.h
@@ -0,0 +1,196 @@
+/*
+ * Copyright 1993-2015 NVIDIA Corporation.  All rights reserved.
+ *
+ * Please refer to the NVIDIA end user license agreement (EULA) associated
+ * with this source code for terms and conditions that govern your use of
+ * this software. Any use, reproduction, disclosure, or distribution of
+ * this software and related documentation outside the terms of the EULA
+ * is strictly prohibited.
+ *
+ */
+
+#ifndef CUDAD3D9_H
+#define CUDAD3D9_H
+
+#if defined(WIN32) || defined(_WIN32) || defined(WIN64) || defined(_WIN64)
+    #define WINDOWS_LEAN_AND_MEAN
+    #include <windows.h>
+    #include <windowsx.h>
+#endif
+#include <d3dx9.h>
+
+#if __CUDA_API_VERSION >= 3020
+//    #define cuD3D9CtxCreate                    cuD3D9CtxCreate_v2
+//    #define cuD3D9ResourceGetSurfaceDimensions cuD3D9ResourceGetSurfaceDimensions_v2
+//    #define cuD3D9ResourceGetMappedPointer     cuD3D9ResourceGetMappedPointer_v2
+//    #define cuD3D9ResourceGetMappedSize        cuD3D9ResourceGetMappedSize_v2
+//    #define cuD3D9ResourceGetMappedPitch       cuD3D9ResourceGetMappedPitch_v2
+//    #define cuD3D9MapVertexBuffer              cuD3D9MapVertexBuffer_v2
+#endif /* __CUDA_API_VERSION >= 3020 */
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/**
+ * \file dynlink_cudaD3D9.h
+ * \brief Header file for the Direct3D 9 interoperability functions of the
+ * low-level CUDA driver application programming interface.
+ */
+
+/**
+ * \defgroup CUDA_D3D9 Direct3D 9 Interoperability
+ * \ingroup CUDA_DRIVER
+ *
+ * ___MANBRIEF___ Direct3D 9 interoperability functions of the low-level CUDA
+ * driver API (___CURRENT_FILE___) ___ENDMANBRIEF___
+ *
+ * This section describes the Direct3D 9 interoperability functions of the
+ * low-level CUDA driver application programming interface. Note that mapping 
+ * of Direct3D 9 resources is performed with the graphics API agnostic, resource 
+ * mapping interface described in \ref CUDA_GRAPHICS "Graphics Interoperability".
+ *
+ * @{
+ */
+
+/**
+ * CUDA devices corresponding to a D3D9 device
+ */
+typedef enum CUd3d9DeviceList_enum {
+    CU_D3D9_DEVICE_LIST_ALL            = 0x01, /**< The CUDA devices for all GPUs used by a D3D9 device */
+    CU_D3D9_DEVICE_LIST_CURRENT_FRAME  = 0x02, /**< The CUDA devices for the GPUs used by a D3D9 device in its currently rendering frame */
+    CU_D3D9_DEVICE_LIST_NEXT_FRAME     = 0x03, /**< The CUDA devices for the GPUs to be used by a D3D9 device in the next frame */
+} CUd3d9DeviceList;
+
+typedef CUresult CUDAAPI tcuD3D9GetDevice(CUdevice *pCudaDevice, const char *pszAdapterName);
+typedef CUresult CUDAAPI tcuD3D9GetDevices(unsigned int *pCudaDeviceCount, CUdevice *pCudaDevices, unsigned int cudaDeviceCount, IDirect3DDevice9 *pD3D9Device, CUd3d9DeviceList deviceList);
+
+#if __CUDA_API_VERSION >= 3020
+typedef CUresult CUDAAPI tcuD3D9CtxCreate(CUcontext *pCtx, CUdevice *pCudaDevice, unsigned int Flags, IDirect3DDevice9 *pD3DDevice);
+#endif
+
+#if __CUDA_API_VERSION >= 3020
+typedef CUresult CUDAAPI tcuD3D9CtxCreate_v2(CUcontext *pCtx, CUdevice *pCudaDevice, unsigned int Flags, IDirect3DDevice9 *pD3DDevice);
+typedef CUresult CUDAAPI tcuD3D9CtxCreateOnDevice(CUcontext *pCtx, unsigned int flags, IDirect3DDevice9 *pD3DDevice, CUdevice cudaDevice);
+#endif /* __CUDA_API_VERSION >= 3020 */
+
+typedef CUresult CUDAAPI tcuD3D9GetDirect3DDevice(IDirect3DDevice9 **ppD3DDevice);
+typedef CUresult CUDAAPI tcuGraphicsD3D9RegisterResource(CUgraphicsResource *pCudaResource, IDirect3DResource9 *pD3DResource, unsigned int Flags);
+
+/**
+ * \defgroup CUDA_D3D9_DEPRECATED Direct3D 9 Interoperability [DEPRECATED]
+ *
+ * ___MANBRIEF___ deprecated Direct3D 9 interoperability functions of the
+ * low-level CUDA driver API (___CURRENT_FILE___) ___ENDMANBRIEF___
+ *
+ * This section describes deprecated Direct3D 9 interoperability functionality.
+ * @{
+ */
+
+/** Flags to register a resource */
+typedef enum CUd3d9register_flags_enum {
+    CU_D3D9_REGISTER_FLAGS_NONE  = 0x00,
+    CU_D3D9_REGISTER_FLAGS_ARRAY = 0x01,
+} CUd3d9register_flags;
+
+/** Flags to map or unmap a resource */
+typedef enum CUd3d9map_flags_enum {
+    CU_D3D9_MAPRESOURCE_FLAGS_NONE         = 0x00,
+    CU_D3D9_MAPRESOURCE_FLAGS_READONLY     = 0x01,
+    CU_D3D9_MAPRESOURCE_FLAGS_WRITEDISCARD = 0x02,
+} CUd3d9map_flags;
+
+typedef CUresult CUDAAPI tcuD3D9RegisterResource(IDirect3DResource9 *pResource, unsigned int Flags);
+typedef CUresult CUDAAPI tcuD3D9UnregisterResource(IDirect3DResource9 *pResource);
+typedef CUresult CUDAAPI tcuD3D9MapResources(unsigned int count, IDirect3DResource9 **ppResource);
+typedef CUresult CUDAAPI tcuD3D9UnmapResources(unsigned int count, IDirect3DResource9 **ppResource);
+typedef CUresult CUDAAPI tcuD3D9ResourceSetMapFlags(IDirect3DResource9 *pResource, unsigned int Flags);
+
+#if __CUDA_API_VERSION >= 3020
+typedef CUresult CUDAAPI tcuD3D9ResourceGetSurfaceDimensions(size_t *pWidth, size_t *pHeight, size_t *pDepth, IDirect3DResource9 *pResource, unsigned int Face, unsigned int Level);
+#endif /* __CUDA_API_VERSION >= 3020 */
+
+typedef CUresult CUDAAPI tcuD3D9ResourceGetMappedArray(CUarray *pArray, IDirect3DResource9 *pResource, unsigned int Face, unsigned int Level);
+
+#if __CUDA_API_VERSION >= 3020
+typedef CUresult CUDAAPI tcuD3D9ResourceGetMappedPointer(CUdeviceptr *pDevPtr, IDirect3DResource9 *pResource, unsigned int Face, unsigned int Level);
+typedef CUresult CUDAAPI tcuD3D9ResourceGetMappedSize(size_t *pSize, IDirect3DResource9 *pResource, unsigned int Face, unsigned int Level);
+typedef CUresult CUDAAPI tcuD3D9ResourceGetMappedPitch(size_t *pPitch, size_t *pPitchSlice, IDirect3DResource9 *pResource, unsigned int Face, unsigned int Level);
+#endif /* __CUDA_API_VERSION >= 3020 */
+
+/* CUDA 1.x compatibility API. These functions are deprecated, please use the ones above. */
+typedef CUresult CUDAAPI tcuD3D9Begin(IDirect3DDevice9 *pDevice);
+typedef CUresult CUDAAPI tcuD3D9End(void);
+typedef CUresult CUDAAPI tcuD3D9RegisterVertexBuffer(IDirect3DVertexBuffer9 *pVB);
+#if __CUDA_API_VERSION >= 3020
+typedef CUresult CUDAAPI tcuD3D9MapVertexBuffer(CUdeviceptr *pDevPtr, size_t *pSize, IDirect3DVertexBuffer9 *pVB);
+#endif /* __CUDA_API_VERSION >= 3020 */
+typedef CUresult CUDAAPI tcuD3D9UnmapVertexBuffer(IDirect3DVertexBuffer9 *pVB);
+typedef CUresult CUDAAPI tcuD3D9UnregisterVertexBuffer(IDirect3DVertexBuffer9 *pVB);
+
+/** @} */ /* END CUDA_D3D9_DEPRECATED */
+/** @} */ /* END CUDA_D3D9 */
+
+/**
+ * CUDA API versioning support
+ */
+#if defined(__CUDA_API_VERSION_INTERNAL)
+    #undef tcuD3D9CtxCreate
+    #undef tcuD3D9ResourceGetSurfaceDimensions
+    #undef tcuD3D9ResourceGetMappedPointer
+    #undef tcuD3D9ResourceGetMappedSize
+    #undef tcuD3D9ResourceGetMappedPitch
+    #undef tcuD3D9MapVertexBuffer
+#endif /* __CUDA_API_VERSION_INTERNAL */
+
+
+#if defined(WIN32) || defined(_WIN32) || defined(WIN64) || defined(_WIN64)
+#include <Windows.h>
+typedef HMODULE CUDADRIVER;
+#else
+typedef void *CUDADRIVER;
+#endif
+#if defined(WIN32) || defined(_WIN32) || defined(WIN64) || defined(_WIN64)
+#include <Windows.h>
+typedef HMODULE CUDADRIVER;
+#endif
+
+extern tcuD3D9Begin                        *cuD3D9Begin;
+extern tcuD3D9End                          *cuD3D9End;
+extern tcuD3D9RegisterVertexBuffer         *cuD3D9RegisterVertexBuffer;
+extern tcuD3D9MapVertexBuffer              *cuD3D9MapVertexBuffer;
+extern tcuD3D9UnmapVertexBuffer            *cuD3D9UnmapVertexBuffer;
+extern tcuD3D9UnregisterVertexBuffer       *cuD3D9UnregisterVertexBuffer;
+
+extern tcuD3D9GetDevice                    *cuD3D9GetDevice;
+extern tcuD3D9GetDevice                    *cuD3D9GetDevices;
+extern tcuD3D9GetDevice                    *cuD3D9GetDevices_v2;
+extern tcuD3D9CtxCreate                    *cuD3D9CtxCreate;
+extern tcuD3D9CtxCreate                    *cuD3D9CtxCreate_v2;
+extern tcuD3D9CtxCreateOnDevice            *cuD3D9CtxCreateOnDevice;
+extern tcuD3D9ResourceGetSurfaceDimensions *cuD3D9ResourceGetSurfaceDimensions;
+extern tcuD3D9ResourceGetMappedPointer     *cuD3D9ResourceGetMappedPointer;
+extern tcuD3D9ResourceGetMappedArray       *cuD3D9ResourceGetMappedArray;
+extern tcuD3D9ResourceGetMappedSize        *cuD3D9ResourceGetMappedSize;
+extern tcuD3D9ResourceGetMappedPitch       *cuD3D9ResourceGetMappedPitch;
+extern tcuD3D9MapVertexBuffer              *cuD3D9MapVertexBuffer;
+
+extern tcuD3D9RegisterResource             *cuD3D9RegisterResource;
+extern tcuD3D9UnregisterResource           *cuD3D9UnregisterResource;
+extern tcuD3D9MapResources                 *cuD3D9MapResources;
+extern tcuD3D9UnmapResources               *cuD3D9UnmapResources;
+extern tcuD3D9ResourceSetMapFlags          *cuD3D9ResourceSetMapFlags;
+
+extern tcuGraphicsD3D9RegisterResource     *cuGraphicsD3D9RegisterResource;
+extern tcuGraphicsD3D9RegisterResource     *cuGraphicsD3D9RegisterResource_v2;
+
+
+/************************************/
+CUresult CUDAAPI cuInitD3D9(unsigned int Flags, int cudaVersion, CUDADRIVER &CudaDrvLib);
+/************************************/
+
+#ifdef __cplusplus
+};
+#endif
+
+#endif
diff --git a/webrtc/modules/video_coding/codecs/h264/include/dynlink_cudaGL.h b/webrtc/modules/video_coding/codecs/h264/include/dynlink_cudaGL.h
new file mode 100644
index 0000000..a0fa8a2
--- /dev/null
+++ b/webrtc/modules/video_coding/codecs/h264/include/dynlink_cudaGL.h
@@ -0,0 +1,159 @@
+/*
+ * Copyright 1993-2015 NVIDIA Corporation.  All rights reserved.
+ *
+ * Please refer to the NVIDIA end user license agreement (EULA) associated
+ * with this source code for terms and conditions that govern your use of
+ * this software. Any use, reproduction, disclosure, or distribution of
+ * this software and related documentation outside the terms of the EULA
+ * is strictly prohibited.
+ *
+ */
+ 
+#ifndef CUDAGL_H
+#define CUDAGL_H
+
+#if INIT_CUDA_GL
+
+#ifndef __CUDA_API_VERSION
+#define __CUDA_API_VERSION 4000
+#endif
+
+#include <GL/glew.h>
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/**
+ * \file dynlink_cudaGL.h
+ * \brief Header file for the OpenGL interoperability functions of the
+ * low-level CUDA driver application programming interface.
+ */
+
+/**
+ * \defgroup CUDA_GL OpenGL Interoperability
+ * \ingroup CUDA_DRIVER
+ *
+ * ___MANBRIEF___ OpenGL interoperability functions of the low-level CUDA
+ * driver API (___CURRENT_FILE___) ___ENDMANBRIEF___
+ *
+ * This section describes the OpenGL interoperability functions of the
+ * low-level CUDA driver application programming interface. Note that mapping 
+ * of OpenGL resources is performed with the graphics API agnostic, resource 
+ * mapping interface described in \ref CUDA_GRAPHICS "Graphics Interoperability".
+ *
+ * @{
+ */
+
+#if defined(WIN32) || defined(_WIN32) || defined(WIN64) || defined(_WIN64)
+#if !defined(WGL_NV_gpu_affinity)
+typedef void* HGPUNV;
+#endif
+#endif /* _WIN32 */
+
+typedef CUresult CUDAAPI tcuGraphicsGLRegisterBuffer(CUgraphicsResource *pCudaResource, GLuint buffer, unsigned int Flags);
+typedef CUresult CUDAAPI tcuGraphicsGLRegisterImage(CUgraphicsResource *pCudaResource, GLuint image, GLenum target, unsigned int Flags);
+
+#if defined(WIN32) || defined(_WIN32) || defined(WIN64) || defined(_WIN64)
+typedef CUresult CUDAAPI tcuWGLGetDevice(CUdevice *pDevice, HGPUNV hGpu);
+#endif /* _WIN32 */
+
+/**
+ * CUDA devices corresponding to an OpenGL device
+ */
+typedef enum CUGLDeviceList_enum {
+    CU_GL_DEVICE_LIST_ALL            = 0x01, /**< The CUDA devices for all GPUs used by the current OpenGL context */
+    CU_GL_DEVICE_LIST_CURRENT_FRAME  = 0x02, /**< The CUDA devices for the GPUs used by the current OpenGL context in its currently rendering frame */
+    CU_GL_DEVICE_LIST_NEXT_FRAME     = 0x03, /**< The CUDA devices for the GPUs to be used by the current OpenGL context in the next frame */
+} CUGLDeviceList;
+
+#if __CUDA_API_VERSION >= 6050
+typedef CUresult CUDAAPI tcuGLGetDevices(unsigned int *pCudaDeviceCount, CUdevice *pCudaDevices, unsigned int cudaDeviceCount, CUGLDeviceList deviceList);
+#endif /* __CUDA_API_VERSION >= 6050 */
+
+/**
+ * \defgroup CUDA_GL_DEPRECATED OpenGL Interoperability [DEPRECATED]
+ *
+ * ___MANBRIEF___ deprecated OpenGL interoperability functions of the low-level
+ * CUDA driver API (___CURRENT_FILE___) ___ENDMANBRIEF___
+ *
+ * This section describes deprecated OpenGL interoperability functionality.
+ *
+ * @{
+ */
+
+/** Flags to map or unmap a resource */
+typedef enum CUGLmap_flags_enum {
+    CU_GL_MAP_RESOURCE_FLAGS_NONE          = 0x00,
+    CU_GL_MAP_RESOURCE_FLAGS_READ_ONLY     = 0x01,
+    CU_GL_MAP_RESOURCE_FLAGS_WRITE_DISCARD = 0x02,    
+} CUGLmap_flags;
+
+//#if __CUDA_API_VERSION >= 3020
+typedef CUresult CUDAAPI tcuGLCtxCreate(CUcontext *pCtx, unsigned int Flags, CUdevice device);
+//#endif /* __CUDA_API_VERSION >= 3020 */
+
+typedef CUresult CUDAAPI tcuGLInit(void);
+typedef CUresult CUDAAPI tcuGLRegisterBufferObject(GLuint buffer);
+
+#if __CUDA_API_VERSION >= 3020
+typedef CUresult CUDAAPI tcuGLMapBufferObject(CUdeviceptr *dptr, size_t *size, GLuint buffer);
+#endif /* __CUDA_API_VERSION >= 3020 */
+
+typedef CUresult CUDAAPI tcuGLUnmapBufferObject(GLuint buffer);
+typedef CUresult CUDAAPI tcuGLUnregisterBufferObject(GLuint buffer);
+typedef CUresult CUDAAPI tcuGLSetBufferObjectMapFlags(GLuint buffer, unsigned int Flags);
+
+#if __CUDA_API_VERSION >= 3020
+typedef CUresult CUDAAPI tcuGLMapBufferObjectAsync(CUdeviceptr *dptr, size_t *size, GLuint buffer, CUstream hStream);
+#endif /* __CUDA_API_VERSION >= 3020 */
+
+typedef CUresult CUDAAPI tcuGLUnmapBufferObjectAsync(GLuint buffer, CUstream hStream);
+typedef CUresult CUDAAPI tcuGLGetDevices(unsigned int *pCudaDeviceCount, CUdevice *pCudaDevices, unsigned int cudaDeviceCount, CUGLDeviceList deviceList);
+
+#if defined(WIN32) || defined(_WIN32) || defined(WIN64) || defined(_WIN64)
+extern tcuWGLGetDevice                 *cuWGLGetDevice;
+#endif
+
+extern tcuGLCtxCreate                  *cuGLCtxCreate;
+extern tcuGLCtxCreate                  *cuGLCtxCreate_v2;
+extern tcuGLMapBufferObject            *cuGLMapBufferObject;
+extern tcuGLMapBufferObject            *cuGLMapBufferObject_v2;
+extern tcuGLMapBufferObjectAsync       *cuGLMapBufferObjectAsync;
+
+#if __CUDA_API_VERSION >= 6050
+extern tcuGLGetDevices                 *cuGLGetDevices;
+#endif
+
+extern tcuGraphicsGLRegisterBuffer     *cuGraphicsGLRegisterBuffer;
+extern tcuGraphicsGLRegisterImage      *cuGraphicsGLRegisterImage;
+extern tcuGLSetBufferObjectMapFlags    *cuGLSetBufferObjectMapFlags;
+extern tcuGLRegisterBufferObject       *cuGLRegisterBufferObject;
+
+extern tcuGLUnmapBufferObject          *cuGLUnmapBufferObject;
+extern tcuGLUnmapBufferObjectAsync     *cuGLUnmapBufferObjectAsync;
+
+extern tcuGLUnregisterBufferObject     *cuGLUnregisterBufferObject;
+extern tcuGLGetDevices                 *cuGLGetDevices; // CUDA 6.5 only
+
+
+#if defined(WIN32) || defined(_WIN32) || defined(WIN64) || defined(_WIN64)
+#include <Windows.h>
+typedef HMODULE CUDADRIVER;
+#else
+typedef void *CUDADRIVER;
+#endif
+
+
+
+/************************************/
+CUresult CUDAAPI cuInitGL(unsigned int, int cudaVersion, CUDADRIVER &CudaDrvLib);
+/************************************/
+
+#ifdef __cplusplus
+};
+#endif
+
+#endif
+
+#endif
diff --git a/webrtc/modules/video_coding/codecs/h264/include/dynlink_cuda_cuda.h b/webrtc/modules/video_coding/codecs/h264/include/dynlink_cuda_cuda.h
new file mode 100644
index 0000000..3331e4d
--- /dev/null
+++ b/webrtc/modules/video_coding/codecs/h264/include/dynlink_cuda_cuda.h
@@ -0,0 +1,1685 @@
+/*
+ * Copyright 1993-2015 NVIDIA Corporation.  All rights reserved.
+ *
+ * Please refer to the NVIDIA end user license agreement (EULA) associated
+ * with this source code for terms and conditions that govern your use of
+ * this software. Any use, reproduction, disclosure, or distribution of
+ * this software and related documentation outside the terms of the EULA
+ * is strictly prohibited.
+ *
+ */
+
+#ifndef __cuda_cuda_h__
+#define __cuda_cuda_h__
+
+#include <stdlib.h>
+
+#ifndef __CUDA_API_VERSION
+#define __CUDA_API_VERSION 4000
+#endif
+
+/**
+ * \defgroup CUDA_DRIVER CUDA Driver API
+ *
+ * This section describes the low-level CUDA driver application programming
+ * interface.
+ *
+ * @{
+ */
+
+/**
+ * \defgroup CUDA_TYPES Data types used by CUDA driver
+ * @{
+ */
+
+/**
+ * CUDA API version number
+ */
+#define CUDA_VERSION 4000 /* 4.0 */
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/**
+ * CUDA device pointer
+ */
+#if __CUDA_API_VERSION >= 3020
+
+#if defined(__x86_64) || defined(AMD64) || defined(_M_AMD64) || defined(__aarch64__)
+    typedef unsigned long long CUdeviceptr;
+#else
+    typedef unsigned int CUdeviceptr;
+#endif
+
+#endif /* __CUDA_API_VERSION >= 3020 */
+
+typedef int CUdevice;                                     /**< CUDA device */
+typedef struct CUctx_st *CUcontext;                       /**< CUDA context */
+typedef struct CUmod_st *CUmodule;                        /**< CUDA module */
+typedef struct CUfunc_st *CUfunction;                     /**< CUDA function */
+typedef struct CUarray_st *CUarray;                       /**< CUDA array */
+typedef struct CUtexref_st *CUtexref;                     /**< CUDA texture reference */
+typedef struct CUsurfref_st *CUsurfref;                   /**< CUDA surface reference */
+typedef struct CUevent_st *CUevent;                       /**< CUDA event */
+typedef struct CUstream_st *CUstream;                     /**< CUDA stream */
+typedef struct CUgraphicsResource_st *CUgraphicsResource; /**< CUDA graphics interop resource */
+
+typedef struct CUuuid_st                                  /**< CUDA definition of UUID */
+{
+    char bytes[16];
+} CUuuid;
+
+/**
+ * Context creation flags
+ */
+typedef enum CUctx_flags_enum
+{
+    CU_CTX_SCHED_AUTO          = 0x00, /**< Automatic scheduling */
+    CU_CTX_SCHED_SPIN          = 0x01, /**< Set spin as default scheduling */
+    CU_CTX_SCHED_YIELD         = 0x02, /**< Set yield as default scheduling */
+    CU_CTX_SCHED_BLOCKING_SYNC = 0x04, /**< Set blocking synchronization as default scheduling */
+    CU_CTX_BLOCKING_SYNC       = 0x04, /**< Set blocking synchronization as default scheduling \deprecated */
+    CU_CTX_MAP_HOST            = 0x08, /**< Support mapped pinned allocations */
+    CU_CTX_LMEM_RESIZE_TO_MAX  = 0x10, /**< Keep local memory allocation after launch */
+#if __CUDA_API_VERSION < 4000
+    CU_CTX_SCHED_MASK          = 0x03,
+    CU_CTX_FLAGS_MASK          = 0x1f
+#else
+    CU_CTX_SCHED_MASK          = 0x07,
+    CU_CTX_PRIMARY             = 0x20, /**< Initialize and return the primary context */
+    CU_CTX_FLAGS_MASK          = 0x3f
+#endif
+} CUctx_flags;
+
+/**
+ * Event creation flags
+ */
+typedef enum CUevent_flags_enum
+{
+    CU_EVENT_DEFAULT        = 0, /**< Default event flag */
+    CU_EVENT_BLOCKING_SYNC  = 1, /**< Event uses blocking synchronization */
+    CU_EVENT_DISABLE_TIMING = 2  /**< Event will not record timing data */
+} CUevent_flags;
+
+/**
+ * Array formats
+ */
+typedef enum CUarray_format_enum
+{
+    CU_AD_FORMAT_UNSIGNED_INT8  = 0x01, /**< Unsigned 8-bit integers */
+    CU_AD_FORMAT_UNSIGNED_INT16 = 0x02, /**< Unsigned 16-bit integers */
+    CU_AD_FORMAT_UNSIGNED_INT32 = 0x03, /**< Unsigned 32-bit integers */
+    CU_AD_FORMAT_SIGNED_INT8    = 0x08, /**< Signed 8-bit integers */
+    CU_AD_FORMAT_SIGNED_INT16   = 0x09, /**< Signed 16-bit integers */
+    CU_AD_FORMAT_SIGNED_INT32   = 0x0a, /**< Signed 32-bit integers */
+    CU_AD_FORMAT_HALF           = 0x10, /**< 16-bit floating point */
+    CU_AD_FORMAT_FLOAT          = 0x20  /**< 32-bit floating point */
+} CUarray_format;
+
+/**
+ * Texture reference addressing modes
+ */
+typedef enum CUaddress_mode_enum
+{
+    CU_TR_ADDRESS_MODE_WRAP   = 0, /**< Wrapping address mode */
+    CU_TR_ADDRESS_MODE_CLAMP  = 1, /**< Clamp to edge address mode */
+    CU_TR_ADDRESS_MODE_MIRROR = 2, /**< Mirror address mode */
+    CU_TR_ADDRESS_MODE_BORDER = 3  /**< Border address mode */
+} CUaddress_mode;
+
+/**
+ * Texture reference filtering modes
+ */
+typedef enum CUfilter_mode_enum
+{
+    CU_TR_FILTER_MODE_POINT  = 0, /**< Point filter mode */
+    CU_TR_FILTER_MODE_LINEAR = 1  /**< Linear filter mode */
+} CUfilter_mode;
+
+/**
+ * Device properties
+ */
+typedef enum CUdevice_attribute_enum
+{
+    CU_DEVICE_ATTRIBUTE_MAX_THREADS_PER_BLOCK = 1,              /**< Maximum number of threads per block */
+    CU_DEVICE_ATTRIBUTE_MAX_BLOCK_DIM_X = 2,                    /**< Maximum block dimension X */
+    CU_DEVICE_ATTRIBUTE_MAX_BLOCK_DIM_Y = 3,                    /**< Maximum block dimension Y */
+    CU_DEVICE_ATTRIBUTE_MAX_BLOCK_DIM_Z = 4,                    /**< Maximum block dimension Z */
+    CU_DEVICE_ATTRIBUTE_MAX_GRID_DIM_X = 5,                     /**< Maximum grid dimension X */
+    CU_DEVICE_ATTRIBUTE_MAX_GRID_DIM_Y = 6,                     /**< Maximum grid dimension Y */
+    CU_DEVICE_ATTRIBUTE_MAX_GRID_DIM_Z = 7,                     /**< Maximum grid dimension Z */
+    CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK = 8,        /**< Maximum shared memory available per block in bytes */
+    CU_DEVICE_ATTRIBUTE_SHARED_MEMORY_PER_BLOCK = 8,            /**< Deprecated, use CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK */
+    CU_DEVICE_ATTRIBUTE_TOTAL_CONSTANT_MEMORY = 9,              /**< Memory available on device for __constant__ variables in a CUDA C kernel in bytes */
+    CU_DEVICE_ATTRIBUTE_WARP_SIZE = 10,                         /**< Warp size in threads */
+    CU_DEVICE_ATTRIBUTE_MAX_PITCH = 11,                         /**< Maximum pitch in bytes allowed by memory copies */
+    CU_DEVICE_ATTRIBUTE_MAX_REGISTERS_PER_BLOCK = 12,           /**< Maximum number of 32-bit registers available per block */
+    CU_DEVICE_ATTRIBUTE_REGISTERS_PER_BLOCK = 12,               /**< Deprecated, use CU_DEVICE_ATTRIBUTE_MAX_REGISTERS_PER_BLOCK */
+    CU_DEVICE_ATTRIBUTE_CLOCK_RATE = 13,                        /**< Peak clock frequency in kilohertz */
+    CU_DEVICE_ATTRIBUTE_TEXTURE_ALIGNMENT = 14,                 /**< Alignment requirement for textures */
+    CU_DEVICE_ATTRIBUTE_GPU_OVERLAP = 15,                       /**< Device can possibly copy memory and execute a kernel concurrently */
+    CU_DEVICE_ATTRIBUTE_MULTIPROCESSOR_COUNT = 16,              /**< Number of multiprocessors on device */
+    CU_DEVICE_ATTRIBUTE_KERNEL_EXEC_TIMEOUT = 17,               /**< Specifies whether there is a run time limit on kernels */
+    CU_DEVICE_ATTRIBUTE_INTEGRATED = 18,                        /**< Device is integrated with host memory */
+    CU_DEVICE_ATTRIBUTE_CAN_MAP_HOST_MEMORY = 19,               /**< Device can map host memory into CUDA address space */
+    CU_DEVICE_ATTRIBUTE_COMPUTE_MODE = 20,                      /**< Compute mode (See ::CUcomputemode for details) */
+    CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_WIDTH = 21,           /**< Maximum 1D texture width */
+    CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_WIDTH = 22,           /**< Maximum 2D texture width */
+    CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_HEIGHT = 23,          /**< Maximum 2D texture height */
+    CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_WIDTH = 24,           /**< Maximum 3D texture width */
+    CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_HEIGHT = 25,          /**< Maximum 3D texture height */
+    CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_DEPTH = 26,           /**< Maximum 3D texture depth */
+    CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_ARRAY_WIDTH = 27,     /**< Maximum texture array width */
+    CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_ARRAY_HEIGHT = 28,    /**< Maximum texture array height */
+    CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_ARRAY_NUMSLICES = 29, /**< Maximum slices in a texture array */
+    CU_DEVICE_ATTRIBUTE_SURFACE_ALIGNMENT = 30,                 /**< Alignment requirement for surfaces */
+    CU_DEVICE_ATTRIBUTE_CONCURRENT_KERNELS = 31,                /**< Device can possibly execute multiple kernels concurrently */
+    CU_DEVICE_ATTRIBUTE_ECC_ENABLED = 32,                       /**< Device has ECC support enabled */
+    CU_DEVICE_ATTRIBUTE_PCI_BUS_ID = 33,                        /**< PCI bus ID of the device */
+    CU_DEVICE_ATTRIBUTE_PCI_DEVICE_ID = 34,                     /**< PCI device ID of the device */
+    CU_DEVICE_ATTRIBUTE_TCC_DRIVER = 35                         /**< Device is using TCC driver model */
+#if __CUDA_API_VERSION >= 4000
+  , CU_DEVICE_ATTRIBUTE_MEMORY_CLOCK_RATE = 36,                 /**< Peak memory clock frequency in kilohertz */
+    CU_DEVICE_ATTRIBUTE_GLOBAL_MEMORY_BUS_WIDTH = 37,           /**< Global memory bus width in bits */
+    CU_DEVICE_ATTRIBUTE_L2_CACHE_SIZE = 38,                     /**< Size of L2 cache in bytes */
+    CU_DEVICE_ATTRIBUTE_MAX_THREADS_PER_MULTIPROCESSOR = 39,    /**< Maximum resident threads per multiprocessor */
+    CU_DEVICE_ATTRIBUTE_ASYNC_ENGINE_COUNT = 40,                /**< Number of asynchronous engines */
+    CU_DEVICE_ATTRIBUTE_UNIFIED_ADDRESSING = 41,                /**< Device uses shares a unified address space with the host */
+    CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_LAYERED_WIDTH = 42,   /**< Maximum 1D layered texture width */
+    CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_LAYERED_LAYERS = 43   /**< Maximum layers in a 1D layered texture */
+#endif
+} CUdevice_attribute;
+
+/**
+ * Legacy device properties
+ */
+typedef struct CUdevprop_st
+{
+    int maxThreadsPerBlock;     /**< Maximum number of threads per block */
+    int maxThreadsDim[3];       /**< Maximum size of each dimension of a block */
+    int maxGridSize[3];         /**< Maximum size of each dimension of a grid */
+    int sharedMemPerBlock;      /**< Shared memory available per block in bytes */
+    int totalConstantMemory;    /**< Constant memory available on device in bytes */
+    int SIMDWidth;              /**< Warp size in threads */
+    int memPitch;               /**< Maximum pitch in bytes allowed by memory copies */
+    int regsPerBlock;           /**< 32-bit registers available per block */
+    int clockRate;              /**< Clock frequency in kilohertz */
+    int textureAlign;           /**< Alignment requirement for textures */
+} CUdevprop;
+
+/**
+ * Function properties
+ */
+typedef enum CUfunction_attribute_enum
+{
+    /**
+     * The maximum number of threads per block, beyond which a launch of the
+     * function would fail. This number depends on both the function and the
+     * device on which the function is currently loaded.
+     */
+    CU_FUNC_ATTRIBUTE_MAX_THREADS_PER_BLOCK = 0,
+
+    /**
+     * The size in bytes of statically-allocated shared memory required by
+     * this function. This does not include dynamically-allocated shared
+     * memory requested by the user at runtime.
+     */
+    CU_FUNC_ATTRIBUTE_SHARED_SIZE_BYTES = 1,
+
+    /**
+     * The size in bytes of user-allocated constant memory required by this
+     * function.
+     */
+    CU_FUNC_ATTRIBUTE_CONST_SIZE_BYTES = 2,
+
+    /**
+     * The size in bytes of local memory used by each thread of this function.
+     */
+    CU_FUNC_ATTRIBUTE_LOCAL_SIZE_BYTES = 3,
+
+    /**
+     * The number of registers used by each thread of this function.
+     */
+    CU_FUNC_ATTRIBUTE_NUM_REGS = 4,
+
+    /**
+     * The PTX virtual architecture version for which the function was
+     * compiled. This value is the major PTX version * 10 + the minor PTX
+     * version, so a PTX version 1.3 function would return the value 13.
+     * Note that this may return the undefined value of 0 for cubins
+     * compiled prior to CUDA 3.0.
+     */
+    CU_FUNC_ATTRIBUTE_PTX_VERSION = 5,
+
+    /**
+     * The binary architecture version for which the function was compiled.
+     * This value is the major binary version * 10 + the minor binary version,
+     * so a binary version 1.3 function would return the value 13. Note that
+     * this will return a value of 10 for legacy cubins that do not have a
+     * properly-encoded binary architecture version.
+     */
+    CU_FUNC_ATTRIBUTE_BINARY_VERSION = 6,
+
+    CU_FUNC_ATTRIBUTE_MAX
+} CUfunction_attribute;
+
+/**
+ * Function cache configurations
+ */
+typedef enum CUfunc_cache_enum
+{
+    CU_FUNC_CACHE_PREFER_NONE    = 0x00, /**< no preference for shared memory or L1 (default) */
+    CU_FUNC_CACHE_PREFER_SHARED  = 0x01, /**< prefer larger shared memory and smaller L1 cache */
+    CU_FUNC_CACHE_PREFER_L1      = 0x02  /**< prefer larger L1 cache and smaller shared memory */
+} CUfunc_cache;
+
+/**
+ * Memory types
+ */
+typedef enum CUmemorytype_enum
+{
+    CU_MEMORYTYPE_HOST    = 0x01,    /**< Host memory */
+    CU_MEMORYTYPE_DEVICE  = 0x02,    /**< Device memory */
+    CU_MEMORYTYPE_ARRAY   = 0x03     /**< Array memory */
+#if __CUDA_API_VERSION >= 4000
+  , CU_MEMORYTYPE_UNIFIED = 0x04     /**< Unified device or host memory */
+#endif
+} CUmemorytype;
+
+/**
+ * Compute Modes
+ */
+typedef enum CUcomputemode_enum
+{
+    CU_COMPUTEMODE_DEFAULT           = 0,  /**< Default compute mode (Multiple contexts allowed per device) */
+    CU_COMPUTEMODE_EXCLUSIVE         = 1, /**< Compute-exclusive-thread mode (Only one context used by a single thread can be present on this device at a time) */
+    CU_COMPUTEMODE_PROHIBITED        = 2  /**< Compute-prohibited mode (No contexts can be created on this device at this time) */
+#if __CUDA_API_VERSION >= 4000
+  , CU_COMPUTEMODE_EXCLUSIVE_PROCESS = 3  /**< Compute-exclusive-process mode (Only one context used by a single process can be present on this device at a time) */
+#endif
+} CUcomputemode;
+
+/**
+ * Online compiler options
+ */
+typedef enum CUjit_option_enum
+{
+    /**
+     * Max number of registers that a thread may use.\n
+     * Option type: unsigned int
+     */
+    CU_JIT_MAX_REGISTERS = 0,
+
+    /**
+     * IN: Specifies minimum number of threads per block to target compilation
+     * for\n
+     * OUT: Returns the number of threads the compiler actually targeted.
+     * This restricts the resource utilization fo the compiler (e.g. max
+     * registers) such that a block with the given number of threads should be
+     * able to launch based on register limitations. Note, this option does not
+     * currently take into account any other resource limitations, such as
+     * shared memory utilization.\n
+     * Option type: unsigned int
+     */
+    CU_JIT_THREADS_PER_BLOCK,
+
+    /**
+     * Returns a float value in the option of the wall clock time, in
+     * milliseconds, spent creating the cubin\n
+     * Option type: float
+     */
+    CU_JIT_WALL_TIME,
+
+    /**
+     * Pointer to a buffer in which to print any log messsages from PTXAS
+     * that are informational in nature (the buffer size is specified via
+     * option ::CU_JIT_INFO_LOG_BUFFER_SIZE_BYTES) \n
+     * Option type: char*
+     */
+    CU_JIT_INFO_LOG_BUFFER,
+
+    /**
+     * IN: Log buffer size in bytes.  Log messages will be capped at this size
+     * (including null terminator)\n
+     * OUT: Amount of log buffer filled with messages\n
+     * Option type: unsigned int
+     */
+    CU_JIT_INFO_LOG_BUFFER_SIZE_BYTES,
+
+    /**
+     * Pointer to a buffer in which to print any log messages from PTXAS that
+     * reflect errors (the buffer size is specified via option
+     * ::CU_JIT_ERROR_LOG_BUFFER_SIZE_BYTES)\n
+     * Option type: char*
+     */
+    CU_JIT_ERROR_LOG_BUFFER,
+
+    /**
+     * IN: Log buffer size in bytes.  Log messages will be capped at this size
+     * (including null terminator)\n
+     * OUT: Amount of log buffer filled with messages\n
+     * Option type: unsigned int
+     */
+    CU_JIT_ERROR_LOG_BUFFER_SIZE_BYTES,
+
+    /**
+     * Level of optimizations to apply to generated code (0 - 4), with 4
+     * being the default and highest level of optimizations.\n
+     * Option type: unsigned int
+     */
+    CU_JIT_OPTIMIZATION_LEVEL,
+
+    /**
+     * No option value required. Determines the target based on the current
+     * attached context (default)\n
+     * Option type: No option value needed
+     */
+    CU_JIT_TARGET_FROM_CUCONTEXT,
+
+    /**
+     * Target is chosen based on supplied ::CUjit_target_enum.\n
+     * Option type: unsigned int for enumerated type ::CUjit_target_enum
+     */
+    CU_JIT_TARGET,
+
+    /**
+     * Specifies choice of fallback strategy if matching cubin is not found.
+     * Choice is based on supplied ::CUjit_fallback_enum.\n
+     * Option type: unsigned int for enumerated type ::CUjit_fallback_enum
+     */
+    CU_JIT_FALLBACK_STRATEGY
+
+} CUjit_option;
+
+/**
+ * Online compilation targets
+ */
+typedef enum CUjit_target_enum
+{
+    CU_TARGET_COMPUTE_10 = 0,   /**< Compute device class 1.0 */
+    CU_TARGET_COMPUTE_11,       /**< Compute device class 1.1 */
+    CU_TARGET_COMPUTE_12,       /**< Compute device class 1.2 */
+    CU_TARGET_COMPUTE_13,       /**< Compute device class 1.3 */
+    CU_TARGET_COMPUTE_20,       /**< Compute device class 2.0 */
+    CU_TARGET_COMPUTE_21        /**< Compute device class 2.1 */
+} CUjit_target;
+
+/**
+ * Cubin matching fallback strategies
+ */
+typedef enum CUjit_fallback_enum
+{
+    CU_PREFER_PTX = 0,  /**< Prefer to compile ptx */
+
+    CU_PREFER_BINARY    /**< Prefer to fall back to compatible binary code */
+
+} CUjit_fallback;
+
+/**
+ * Flags to register a graphics resource
+ */
+typedef enum CUgraphicsRegisterFlags_enum
+{
+    CU_GRAPHICS_REGISTER_FLAGS_NONE          = 0x00,
+    CU_GRAPHICS_REGISTER_FLAGS_READ_ONLY     = 0x01,
+    CU_GRAPHICS_REGISTER_FLAGS_WRITE_DISCARD = 0x02,
+    CU_GRAPHICS_REGISTER_FLAGS_SURFACE_LDST  = 0x04
+} CUgraphicsRegisterFlags;
+
+/**
+ * Flags for mapping and unmapping interop resources
+ */
+typedef enum CUgraphicsMapResourceFlags_enum
+{
+    CU_GRAPHICS_MAP_RESOURCE_FLAGS_NONE          = 0x00,
+    CU_GRAPHICS_MAP_RESOURCE_FLAGS_READ_ONLY     = 0x01,
+    CU_GRAPHICS_MAP_RESOURCE_FLAGS_WRITE_DISCARD = 0x02
+} CUgraphicsMapResourceFlags;
+
+/**
+ * Array indices for cube faces
+ */
+typedef enum CUarray_cubemap_face_enum
+{
+    CU_CUBEMAP_FACE_POSITIVE_X  = 0x00, /**< Positive X face of cubemap */
+    CU_CUBEMAP_FACE_NEGATIVE_X  = 0x01, /**< Negative X face of cubemap */
+    CU_CUBEMAP_FACE_POSITIVE_Y  = 0x02, /**< Positive Y face of cubemap */
+    CU_CUBEMAP_FACE_NEGATIVE_Y  = 0x03, /**< Negative Y face of cubemap */
+    CU_CUBEMAP_FACE_POSITIVE_Z  = 0x04, /**< Positive Z face of cubemap */
+    CU_CUBEMAP_FACE_NEGATIVE_Z  = 0x05  /**< Negative Z face of cubemap */
+} CUarray_cubemap_face;
+
+/**
+ * Limits
+ */
+typedef enum CUlimit_enum
+{
+    CU_LIMIT_STACK_SIZE        = 0x00, /**< GPU thread stack size */
+    CU_LIMIT_PRINTF_FIFO_SIZE  = 0x01, /**< GPU printf FIFO size */
+    CU_LIMIT_MALLOC_HEAP_SIZE  = 0x02  /**< GPU malloc heap size */
+} CUlimit;
+
+/**
+ * Error codes
+ */
+typedef enum cudaError_enum
+{
+    /**
+     * The API call returned with no errors. In the case of query calls, this
+     * can also mean that the operation being queried is complete (see
+     * ::cuEventQuery() and ::cuStreamQuery()).
+     */
+    CUDA_SUCCESS                              = 0,
+
+    /**
+     * This indicates that one or more of the parameters passed to the API call
+     * is not within an acceptable range of values.
+     */
+    CUDA_ERROR_INVALID_VALUE                  = 1,
+
+    /**
+     * The API call failed because it was unable to allocate enough memory to
+     * perform the requested operation.
+     */
+    CUDA_ERROR_OUT_OF_MEMORY                  = 2,
+
+    /**
+     * This indicates that the CUDA driver has not been initialized with
+     * ::cuInit() or that initialization has failed.
+     */
+    CUDA_ERROR_NOT_INITIALIZED                = 3,
+
+    /**
+     * This indicates that the CUDA driver is in the process of shutting down.
+     */
+    CUDA_ERROR_DEINITIALIZED                  = 4,
+
+    /**
+     * This indicates profiling APIs are called while application is running
+     * in visual profiler mode.
+    */
+    CUDA_ERROR_PROFILER_DISABLED           = 5,
+    /**
+     * This indicates profiling has not been initialized for this context.
+     * Call cuProfilerInitialize() to resolve this.
+    */
+    CUDA_ERROR_PROFILER_NOT_INITIALIZED       = 6,
+    /**
+     * This indicates profiler has already been started and probably
+     * cuProfilerStart() is incorrectly called.
+    */
+    CUDA_ERROR_PROFILER_ALREADY_STARTED       = 7,
+    /**
+     * This indicates profiler has already been stopped and probably
+     * cuProfilerStop() is incorrectly called.
+    */
+    CUDA_ERROR_PROFILER_ALREADY_STOPPED       = 8,
+    /**
+     * This indicates that no CUDA-capable devices were detected by the installed
+     * CUDA driver.
+     */
+    CUDA_ERROR_NO_DEVICE                      = 100,
+
+    /**
+     * This indicates that the device ordinal supplied by the user does not
+     * correspond to a valid CUDA device.
+     */
+    CUDA_ERROR_INVALID_DEVICE                 = 101,
+
+
+    /**
+     * This indicates that the device kernel image is invalid. This can also
+     * indicate an invalid CUDA module.
+     */
+    CUDA_ERROR_INVALID_IMAGE                  = 200,
+
+    /**
+     * This most frequently indicates that there is no context bound to the
+     * current thread. This can also be returned if the context passed to an
+     * API call is not a valid handle (such as a context that has had
+     * ::cuCtxDestroy() invoked on it). This can also be returned if a user
+     * mixes different API versions (i.e. 3010 context with 3020 API calls).
+     * See ::cuCtxGetApiVersion() for more details.
+     */
+    CUDA_ERROR_INVALID_CONTEXT                = 201,
+
+    /**
+     * This indicated that the context being supplied as a parameter to the
+     * API call was already the active context.
+     * \deprecated
+     * This error return is deprecated as of CUDA 3.2. It is no longer an
+     * error to attempt to push the active context via ::cuCtxPushCurrent().
+     */
+    CUDA_ERROR_CONTEXT_ALREADY_CURRENT        = 202,
+
+    /**
+     * This indicates that a map or register operation has failed.
+     */
+    CUDA_ERROR_MAP_FAILED                     = 205,
+
+    /**
+     * This indicates that an unmap or unregister operation has failed.
+     */
+    CUDA_ERROR_UNMAP_FAILED                   = 206,
+
+    /**
+     * This indicates that the specified array is currently mapped and thus
+     * cannot be destroyed.
+     */
+    CUDA_ERROR_ARRAY_IS_MAPPED                = 207,
+
+    /**
+     * This indicates that the resource is already mapped.
+     */
+    CUDA_ERROR_ALREADY_MAPPED                 = 208,
+
+    /**
+     * This indicates that there is no kernel image available that is suitable
+     * for the device. This can occur when a user specifies code generation
+     * options for a particular CUDA source file that do not include the
+     * corresponding device configuration.
+     */
+    CUDA_ERROR_NO_BINARY_FOR_GPU              = 209,
+
+    /**
+     * This indicates that a resource has already been acquired.
+     */
+    CUDA_ERROR_ALREADY_ACQUIRED               = 210,
+
+    /**
+     * This indicates that a resource is not mapped.
+     */
+    CUDA_ERROR_NOT_MAPPED                     = 211,
+
+    /**
+     * This indicates that a mapped resource is not available for access as an
+     * array.
+     */
+    CUDA_ERROR_NOT_MAPPED_AS_ARRAY            = 212,
+
+    /**
+     * This indicates that a mapped resource is not available for access as a
+     * pointer.
+     */
+    CUDA_ERROR_NOT_MAPPED_AS_POINTER          = 213,
+
+    /**
+     * This indicates that an uncorrectable ECC error was detected during
+     * execution.
+     */
+    CUDA_ERROR_ECC_UNCORRECTABLE              = 214,
+
+    /**
+     * This indicates that the ::CUlimit passed to the API call is not
+     * supported by the active device.
+     */
+    CUDA_ERROR_UNSUPPORTED_LIMIT              = 215,
+
+    /**
+     * This indicates that the ::CUcontext passed to the API call can
+     * only be bound to a single CPU thread at a time but is already
+     * bound to a CPU thread.
+     */
+    CUDA_ERROR_CONTEXT_ALREADY_IN_USE         = 216,
+
+    /**
+     * This indicates that the device kernel source is invalid.
+     */
+    CUDA_ERROR_INVALID_SOURCE                 = 300,
+
+    /**
+     * This indicates that the file specified was not found.
+     */
+    CUDA_ERROR_FILE_NOT_FOUND                 = 301,
+
+    /**
+     * This indicates that a link to a shared object failed to resolve.
+     */
+    CUDA_ERROR_SHARED_OBJECT_SYMBOL_NOT_FOUND = 302,
+
+    /**
+     * This indicates that initialization of a shared object failed.
+     */
+    CUDA_ERROR_SHARED_OBJECT_INIT_FAILED      = 303,
+
+    /**
+     * This indicates that an OS call failed.
+     */
+    CUDA_ERROR_OPERATING_SYSTEM               = 304,
+
+
+    /**
+     * This indicates that a resource handle passed to the API call was not
+     * valid. Resource handles are opaque types like ::CUstream and ::CUevent.
+     */
+    CUDA_ERROR_INVALID_HANDLE                 = 400,
+
+
+    /**
+     * This indicates that a named symbol was not found. Examples of symbols
+     * are global/constant variable names, texture names, and surface names.
+     */
+    CUDA_ERROR_NOT_FOUND                      = 500,
+
+
+    /**
+     * This indicates that asynchronous operations issued previously have not
+     * completed yet. This result is not actually an error, but must be indicated
+     * differently than ::CUDA_SUCCESS (which indicates completion). Calls that
+     * may return this value include ::cuEventQuery() and ::cuStreamQuery().
+     */
+    CUDA_ERROR_NOT_READY                      = 600,
+
+
+    /**
+     * An exception occurred on the device while executing a kernel. Common
+     * causes include dereferencing an invalid device pointer and accessing
+     * out of bounds shared memory. The context cannot be used, so it must
+     * be destroyed (and a new one should be created). All existing device
+     * memory allocations from this context are invalid and must be
+     * reconstructed if the program is to continue using CUDA.
+     */
+    CUDA_ERROR_LAUNCH_FAILED                  = 700,
+
+    /**
+     * This indicates that a launch did not occur because it did not have
+     * appropriate resources. This error usually indicates that the user has
+     * attempted to pass too many arguments to the device kernel, or the
+     * kernel launch specifies too many threads for the kernel's register
+     * count. Passing arguments of the wrong size (i.e. a 64-bit pointer
+     * when a 32-bit int is expected) is equivalent to passing too many
+     * arguments and can also result in this error.
+     */
+    CUDA_ERROR_LAUNCH_OUT_OF_RESOURCES        = 701,
+
+    /**
+     * This indicates that the device kernel took too long to execute. This can
+     * only occur if timeouts are enabled - see the device attribute
+     * ::CU_DEVICE_ATTRIBUTE_KERNEL_EXEC_TIMEOUT for more information. The
+     * context cannot be used (and must be destroyed similar to
+     * ::CUDA_ERROR_LAUNCH_FAILED). All existing device memory allocations from
+     * this context are invalid and must be reconstructed if the program is to
+     * continue using CUDA.
+     */
+    CUDA_ERROR_LAUNCH_TIMEOUT                 = 702,
+
+    /**
+     * This error indicates a kernel launch that uses an incompatible texturing
+     * mode.
+     */
+    CUDA_ERROR_LAUNCH_INCOMPATIBLE_TEXTURING  = 703,
+
+    /**
+     * This error indicates that a call to ::cuCtxEnablePeerAccess() is
+     * trying to re-enable peer access to a context which has already
+     * had peer access to it enabled.
+     */
+    CUDA_ERROR_PEER_ACCESS_ALREADY_ENABLED = 704,
+
+    /**
+     * This error indicates that a call to ::cuMemPeerRegister is trying to
+     * register memory from a context which has not had peer access
+     * enabled yet via ::cuCtxEnablePeerAccess(), or that
+     * ::cuCtxDisablePeerAccess() is trying to disable peer access
+     * which has not been enabled yet.
+     */
+    CUDA_ERROR_PEER_ACCESS_NOT_ENABLED    = 705,
+
+    /**
+     * This error indicates that a call to ::cuMemPeerRegister is trying to
+     * register already-registered memory.
+     */
+    CUDA_ERROR_PEER_MEMORY_ALREADY_REGISTERED = 706,
+
+    /**
+     * This error indicates that a call to ::cuMemPeerUnregister is trying to
+     * unregister memory that has not been registered.
+     */
+    CUDA_ERROR_PEER_MEMORY_NOT_REGISTERED     = 707,
+
+    /**
+     * This error indicates that ::cuCtxCreate was called with the flag
+     * ::CU_CTX_PRIMARY on a device which already has initialized its
+     * primary context.
+     */
+    CUDA_ERROR_PRIMARY_CONTEXT_ACTIVE         = 708,
+
+    /**
+     * This error indicates that the context current to the calling thread
+     * has been destroyed using ::cuCtxDestroy, or is a primary context which
+     * has not yet been initialized.
+     */
+    CUDA_ERROR_CONTEXT_IS_DESTROYED           = 709,
+
+    /**
+     * This indicates that an unknown internal error has occurred.
+     */
+    CUDA_ERROR_UNKNOWN                        = 999
+} CUresult;
+
+#if __CUDA_API_VERSION >= 4000
+/**
+ * If set, host memory is portable between CUDA contexts.
+ * Flag for ::cuMemHostAlloc()
+ */
+#define CU_MEMHOSTALLOC_PORTABLE        0x01
+
+/**
+ * If set, host memory is mapped into CUDA address space and
+ * ::cuMemHostGetDevicePointer() may be called on the host pointer.
+ * Flag for ::cuMemHostAlloc()
+ */
+#define CU_MEMHOSTALLOC_DEVICEMAP       0x02
+
+/**
+ * If set, host memory is allocated as write-combined - fast to write,
+ * faster to DMA, slow to read except via SSE4 streaming load instruction
+ * (MOVNTDQA).
+ * Flag for ::cuMemHostAlloc()
+ */
+#define CU_MEMHOSTALLOC_WRITECOMBINED   0x04
+
+/**
+ * If set, host memory is portable between CUDA contexts.
+ * Flag for ::cuMemHostRegister()
+ */
+#define CU_MEMHOSTREGISTER_PORTABLE     0x01
+
+/**
+ * If set, host memory is mapped into CUDA address space and
+ * ::cuMemHostGetDevicePointer() may be called on the host pointer.
+ * Flag for ::cuMemHostRegister()
+ */
+#define CU_MEMHOSTREGISTER_DEVICEMAP    0x02
+
+/**
+ * If set, peer memory is mapped into CUDA address space and
+ * ::cuMemPeerGetDevicePointer() may be called on the host pointer.
+ * Flag for ::cuMemPeerRegister()
+ */
+#define CU_MEMPEERREGISTER_DEVICEMAP    0x02
+#endif
+
+#if __CUDA_API_VERSION >= 3020
+/**
+ * 2D memory copy parameters
+ */
+typedef struct CUDA_MEMCPY2D_st
+{
+    size_t srcXInBytes;         /**< Source X in bytes */
+    size_t srcY;                /**< Source Y */
+
+    CUmemorytype srcMemoryType; /**< Source memory type (host, device, array) */
+    const void *srcHost;        /**< Source host pointer */
+    CUdeviceptr srcDevice;      /**< Source device pointer */
+    CUarray srcArray;           /**< Source array reference */
+    size_t srcPitch;            /**< Source pitch (ignored when src is array) */
+
+    size_t dstXInBytes;         /**< Destination X in bytes */
+    size_t dstY;                /**< Destination Y */
+
+    CUmemorytype dstMemoryType; /**< Destination memory type (host, device, array) */
+    void *dstHost;              /**< Destination host pointer */
+    CUdeviceptr dstDevice;      /**< Destination device pointer */
+    CUarray dstArray;           /**< Destination array reference */
+    size_t dstPitch;            /**< Destination pitch (ignored when dst is array) */
+
+    size_t WidthInBytes;        /**< Width of 2D memory copy in bytes */
+    size_t Height;              /**< Height of 2D memory copy */
+} CUDA_MEMCPY2D;
+
+/**
+ * 3D memory copy parameters
+ */
+typedef struct CUDA_MEMCPY3D_st
+{
+    size_t srcXInBytes;         /**< Source X in bytes */
+    size_t srcY;                /**< Source Y */
+    size_t srcZ;                /**< Source Z */
+    size_t srcLOD;              /**< Source LOD */
+    CUmemorytype srcMemoryType; /**< Source memory type (host, device, array) */
+    const void *srcHost;        /**< Source host pointer */
+    CUdeviceptr srcDevice;      /**< Source device pointer */
+    CUarray srcArray;           /**< Source array reference */
+    void *reserved0;            /**< Must be NULL */
+    size_t srcPitch;            /**< Source pitch (ignored when src is array) */
+    size_t srcHeight;           /**< Source height (ignored when src is array; may be 0 if Depth==1) */
+
+    size_t dstXInBytes;         /**< Destination X in bytes */
+    size_t dstY;                /**< Destination Y */
+    size_t dstZ;                /**< Destination Z */
+    size_t dstLOD;              /**< Destination LOD */
+    CUmemorytype dstMemoryType; /**< Destination memory type (host, device, array) */
+    void *dstHost;              /**< Destination host pointer */
+    CUdeviceptr dstDevice;      /**< Destination device pointer */
+    CUarray dstArray;           /**< Destination array reference */
+    void *reserved1;            /**< Must be NULL */
+    size_t dstPitch;            /**< Destination pitch (ignored when dst is array) */
+    size_t dstHeight;           /**< Destination height (ignored when dst is array; may be 0 if Depth==1) */
+
+    size_t WidthInBytes;        /**< Width of 3D memory copy in bytes */
+    size_t Height;              /**< Height of 3D memory copy */
+    size_t Depth;               /**< Depth of 3D memory copy */
+} CUDA_MEMCPY3D;
+
+/**
+ * 3D memory cross-context copy parameters
+ */
+typedef struct CUDA_MEMCPY3D_PEER_st
+{
+    size_t srcXInBytes;         /**< Source X in bytes */
+    size_t srcY;                /**< Source Y */
+    size_t srcZ;                /**< Source Z */
+    size_t srcLOD;              /**< Source LOD */
+    CUmemorytype srcMemoryType; /**< Source memory type (host, device, array) */
+    const void *srcHost;        /**< Source host pointer */
+    CUdeviceptr srcDevice;      /**< Source device pointer */
+    CUarray srcArray;           /**< Source array reference */
+    CUcontext srcContext;       /**< Source context (ignored with srcMemoryType is ::CU_MEMORYTYPE_ARRAY) */
+    size_t srcPitch;            /**< Source pitch (ignored when src is array) */
+    size_t srcHeight;           /**< Source height (ignored when src is array; may be 0 if Depth==1) */
+
+    size_t dstXInBytes;         /**< Destination X in bytes */
+    size_t dstY;                /**< Destination Y */
+    size_t dstZ;                /**< Destination Z */
+    size_t dstLOD;              /**< Destination LOD */
+    CUmemorytype dstMemoryType; /**< Destination memory type (host, device, array) */
+    void *dstHost;              /**< Destination host pointer */
+    CUdeviceptr dstDevice;      /**< Destination device pointer */
+    CUarray dstArray;           /**< Destination array reference */
+    CUcontext dstContext;       /**< Destination context (ignored with dstMemoryType is ::CU_MEMORYTYPE_ARRAY) */
+    size_t dstPitch;            /**< Destination pitch (ignored when dst is array) */
+    size_t dstHeight;           /**< Destination height (ignored when dst is array; may be 0 if Depth==1) */
+
+    size_t WidthInBytes;        /**< Width of 3D memory copy in bytes */
+    size_t Height;              /**< Height of 3D memory copy */
+    size_t Depth;               /**< Depth of 3D memory copy */
+} CUDA_MEMCPY3D_PEER;
+
+/**
+ * Array descriptor
+ */
+typedef struct CUDA_ARRAY_DESCRIPTOR_st
+{
+    size_t Width;             /**< Width of array */
+    size_t Height;            /**< Height of array */
+
+    CUarray_format Format;    /**< Array format */
+    unsigned int NumChannels; /**< Channels per array element */
+} CUDA_ARRAY_DESCRIPTOR;
+
+/**
+ * 3D array descriptor
+ */
+typedef struct CUDA_ARRAY3D_DESCRIPTOR_st
+{
+    size_t Width;             /**< Width of 3D array */
+    size_t Height;            /**< Height of 3D array */
+    size_t Depth;             /**< Depth of 3D array */
+
+    CUarray_format Format;    /**< Array format */
+    unsigned int NumChannels; /**< Channels per array element */
+    unsigned int Flags;       /**< Flags */
+} CUDA_ARRAY3D_DESCRIPTOR;
+
+#endif /* __CUDA_API_VERSION >= 3020 */
+
+/**
+ * If set, the CUDA array is a collection of layers, where each layer is either a 1D
+ * or a 2D array and the Depth member of CUDA_ARRAY3D_DESCRIPTOR specifies the number
+ * of layers, not the depth of a 3D array.
+ */
+#define CUDA_ARRAY3D_LAYERED        0x01
+
+/**
+ * Deprecated, use CUDA_ARRAY3D_LAYERED
+ */
+#define CUDA_ARRAY3D_2DARRAY        0x01
+
+/**
+ * This flag must be set in order to bind a surface reference
+ * to the CUDA array
+ */
+#define CUDA_ARRAY3D_SURFACE_LDST   0x02
+
+/**
+ * Override the texref format with a format inferred from the array.
+ * Flag for ::cuTexRefSetArray()
+ */
+#define CU_TRSA_OVERRIDE_FORMAT 0x01
+
+/**
+ * Read the texture as integers rather than promoting the values to floats
+ * in the range [0,1].
+ * Flag for ::cuTexRefSetFlags()
+ */
+#define CU_TRSF_READ_AS_INTEGER         0x01
+
+/**
+ * Use normalized texture coordinates in the range [0,1) instead of [0,dim).
+ * Flag for ::cuTexRefSetFlags()
+ */
+#define CU_TRSF_NORMALIZED_COORDINATES  0x02
+
+/**
+ * Perform sRGB->linear conversion during texture read.
+ * Flag for ::cuTexRefSetFlags()
+ */
+#define CU_TRSF_SRGB  0x10
+
+/**
+ * End of array terminator for the \p extra parameter to
+ * ::cuLaunchKernel
+ */
+#define CU_LAUNCH_PARAM_END            ((void*)0x00)
+
+/**
+ * Indicator that the next value in the \p extra parameter to
+ * ::cuLaunchKernel will be a pointer to a buffer containing all kernel
+ * parameters used for launching kernel \p f.  This buffer needs to
+ * honor all alignment/padding requirements of the individual parameters.
+ * If ::CU_LAUNCH_PARAM_BUFFER_SIZE is not also specified in the
+ * \p extra array, then ::CU_LAUNCH_PARAM_BUFFER_POINTER will have no
+ * effect.
+ */
+#define CU_LAUNCH_PARAM_BUFFER_POINTER ((void*)0x01)
+
+/**
+ * Indicator that the next value in the \p extra parameter to
+ * ::cuLaunchKernel will be a pointer to a size_t which contains the
+ * size of the buffer specified with ::CU_LAUNCH_PARAM_BUFFER_POINTER.
+ * It is required that ::CU_LAUNCH_PARAM_BUFFER_POINTER also be specified
+ * in the \p extra array if the value associated with
+ * ::CU_LAUNCH_PARAM_BUFFER_SIZE is not zero.
+ */
+#define CU_LAUNCH_PARAM_BUFFER_SIZE    ((void*)0x02)
+
+/**
+ * For texture references loaded into the module, use default texunit from
+ * texture reference.
+ */
+#define CU_PARAM_TR_DEFAULT -1
+
+/**
+ * CUDA API made obselete at API version 3020
+ */
+#if defined(__CUDA_API_VERSION_INTERNAL)
+    #define CUdeviceptr                  CUdeviceptr_v1
+    #define CUDA_MEMCPY2D_st             CUDA_MEMCPY2D_v1_st
+    #define CUDA_MEMCPY2D                CUDA_MEMCPY2D_v1
+    #define CUDA_MEMCPY3D_st             CUDA_MEMCPY3D_v1_st
+    #define CUDA_MEMCPY3D                CUDA_MEMCPY3D_v1
+    #define CUDA_ARRAY_DESCRIPTOR_st     CUDA_ARRAY_DESCRIPTOR_v1_st
+    #define CUDA_ARRAY_DESCRIPTOR        CUDA_ARRAY_DESCRIPTOR_v1
+    #define CUDA_ARRAY3D_DESCRIPTOR_st   CUDA_ARRAY3D_DESCRIPTOR_v1_st
+    #define CUDA_ARRAY3D_DESCRIPTOR      CUDA_ARRAY3D_DESCRIPTOR_v1
+#endif /* CUDA_FORCE_LEGACY32_INTERNAL */
+
+#if defined(__CUDA_API_VERSION_INTERNAL) || __CUDA_API_VERSION < 3020
+typedef unsigned int CUdeviceptr;
+
+typedef struct CUDA_MEMCPY2D_st
+{
+    unsigned int srcXInBytes;   /**< Source X in bytes */
+    unsigned int srcY;          /**< Source Y */
+    CUmemorytype srcMemoryType; /**< Source memory type (host, device, array) */
+    const void *srcHost;        /**< Source host pointer */
+    CUdeviceptr srcDevice;      /**< Source device pointer */
+    CUarray srcArray;           /**< Source array reference */
+    unsigned int srcPitch;      /**< Source pitch (ignored when src is array) */
+
+    unsigned int dstXInBytes;   /**< Destination X in bytes */
+    unsigned int dstY;          /**< Destination Y */
+    CUmemorytype dstMemoryType; /**< Destination memory type (host, device, array) */
+    void *dstHost;              /**< Destination host pointer */
+    CUdeviceptr dstDevice;      /**< Destination device pointer */
+    CUarray dstArray;           /**< Destination array reference */
+    unsigned int dstPitch;      /**< Destination pitch (ignored when dst is array) */
+
+    unsigned int WidthInBytes;  /**< Width of 2D memory copy in bytes */
+    unsigned int Height;        /**< Height of 2D memory copy */
+} CUDA_MEMCPY2D;
+
+typedef struct CUDA_MEMCPY3D_st
+{
+    unsigned int srcXInBytes;   /**< Source X in bytes */
+    unsigned int srcY;          /**< Source Y */
+    unsigned int srcZ;          /**< Source Z */
+    unsigned int srcLOD;        /**< Source LOD */
+    CUmemorytype srcMemoryType; /**< Source memory type (host, device, array) */
+    const void *srcHost;        /**< Source host pointer */
+    CUdeviceptr srcDevice;      /**< Source device pointer */
+    CUarray srcArray;           /**< Source array reference */
+    void *reserved0;            /**< Must be NULL */
+    unsigned int srcPitch;      /**< Source pitch (ignored when src is array) */
+    unsigned int srcHeight;     /**< Source height (ignored when src is array; may be 0 if Depth==1) */
+
+    unsigned int dstXInBytes;   /**< Destination X in bytes */
+    unsigned int dstY;          /**< Destination Y */
+    unsigned int dstZ;          /**< Destination Z */
+    unsigned int dstLOD;        /**< Destination LOD */
+    CUmemorytype dstMemoryType; /**< Destination memory type (host, device, array) */
+    void *dstHost;              /**< Destination host pointer */
+    CUdeviceptr dstDevice;      /**< Destination device pointer */
+    CUarray dstArray;           /**< Destination array reference */
+    void *reserved1;            /**< Must be NULL */
+    unsigned int dstPitch;      /**< Destination pitch (ignored when dst is array) */
+    unsigned int dstHeight;     /**< Destination height (ignored when dst is array; may be 0 if Depth==1) */
+
+    unsigned int WidthInBytes;  /**< Width of 3D memory copy in bytes */
+    unsigned int Height;        /**< Height of 3D memory copy */
+    unsigned int Depth;         /**< Depth of 3D memory copy */
+} CUDA_MEMCPY3D;
+
+typedef struct CUDA_ARRAY_DESCRIPTOR_st
+{
+    unsigned int Width;         /**< Width of array */
+    unsigned int Height;        /**< Height of array */
+
+    CUarray_format Format;      /**< Array format */
+    unsigned int NumChannels;   /**< Channels per array element */
+} CUDA_ARRAY_DESCRIPTOR;
+
+typedef struct CUDA_ARRAY3D_DESCRIPTOR_st
+{
+    unsigned int Width;         /**< Width of 3D array */
+    unsigned int Height;        /**< Height of 3D array */
+    unsigned int Depth;         /**< Depth of 3D array */
+
+    CUarray_format Format;      /**< Array format */
+    unsigned int NumChannels;   /**< Channels per array element */
+    unsigned int Flags;         /**< Flags */
+} CUDA_ARRAY3D_DESCRIPTOR;
+
+#endif /* (__CUDA_API_VERSION_INTERNAL) || __CUDA_API_VERSION < 3020 */
+
+/*
+ * If set, the CUDA array contains an array of 2D slices
+ * and the Depth member of CUDA_ARRAY3D_DESCRIPTOR specifies
+ * the number of slices, not the depth of a 3D array.
+ */
+#define CUDA_ARRAY3D_2DARRAY        0x01
+
+/**
+ * This flag must be set in order to bind a surface reference
+ * to the CUDA array
+ */
+#define CUDA_ARRAY3D_SURFACE_LDST   0x02
+
+/**
+ * Override the texref format with a format inferred from the array.
+ * Flag for ::cuTexRefSetArray()
+ */
+#define CU_TRSA_OVERRIDE_FORMAT 0x01
+
+/**
+ * Read the texture as integers rather than promoting the values to floats
+ * in the range [0,1].
+ * Flag for ::cuTexRefSetFlags()
+ */
+#define CU_TRSF_READ_AS_INTEGER         0x01
+
+/**
+ * Use normalized texture coordinates in the range [0,1) instead of [0,dim).
+ * Flag for ::cuTexRefSetFlags()
+ */
+#define CU_TRSF_NORMALIZED_COORDINATES  0x02
+
+/**
+ * Perform sRGB->linear conversion during texture read.
+ * Flag for ::cuTexRefSetFlags()
+ */
+#define CU_TRSF_SRGB  0x10
+
+/**
+ * For texture references loaded into the module, use default texunit from
+ * texture reference.
+ */
+#define CU_PARAM_TR_DEFAULT -1
+
+/** @} */ /* END CUDA_TYPES */
+
+#if defined(WIN32) || defined(_WIN32) || defined(WIN64) || defined(_WIN64)
+    #define CUDAAPI __stdcall
+#else
+    #define CUDAAPI
+#endif
+
+/**
+ * \defgroup CUDA_INITIALIZE Initialization
+ *
+ * This section describes the initialization functions of the low-level CUDA
+ * driver application programming interface.
+ *
+ * @{
+ */
+
+/*********************************
+ ** Initialization
+ *********************************/
+typedef CUresult  CUDAAPI tcuInit(unsigned int Flags);
+
+/*********************************
+ ** Driver Version Query
+ *********************************/
+typedef CUresult  CUDAAPI tcuDriverGetVersion(int *driverVersion);
+
+/************************************
+ **
+ **    Device management
+ **
+ ***********************************/
+
+typedef CUresult  CUDAAPI tcuDeviceGet(CUdevice *device, int ordinal);
+typedef CUresult  CUDAAPI tcuDeviceGetCount(int *count);
+typedef CUresult  CUDAAPI tcuDeviceGetName(char *name, int len, CUdevice dev);
+typedef CUresult  CUDAAPI tcuDeviceComputeCapability(int *major, int *minor, CUdevice dev);
+#if __CUDA_API_VERSION >= 3020
+    typedef CUresult  CUDAAPI tcuDeviceTotalMem(size_t *bytes, CUdevice dev);
+#else
+    typedef CUresult  CUDAAPI tcuDeviceTotalMem(unsigned int *bytes, CUdevice dev);
+#endif
+
+typedef CUresult  CUDAAPI tcuDeviceGetProperties(CUdevprop *prop, CUdevice dev);
+typedef CUresult  CUDAAPI tcuDeviceGetAttribute(int *pi, CUdevice_attribute attrib, CUdevice dev);
+
+/************************************
+ **
+ **    Context management
+ **
+ ***********************************/
+typedef CUresult  CUDAAPI tcuCtxCreate(CUcontext *pctx, unsigned int flags, CUdevice dev);
+typedef CUresult  CUDAAPI tcuCtxDestroy(CUcontext ctx);
+typedef CUresult  CUDAAPI tcuCtxAttach(CUcontext *pctx, unsigned int flags);
+typedef CUresult  CUDAAPI tcuCtxDetach(CUcontext ctx);
+typedef CUresult  CUDAAPI tcuCtxPushCurrent(CUcontext ctx);
+typedef CUresult  CUDAAPI tcuCtxPopCurrent(CUcontext *pctx);
+
+typedef CUresult  CUDAAPI tcuCtxSetCurrent(CUcontext ctx);
+typedef CUresult  CUDAAPI tcuCtxGetCurrent(CUcontext *pctx);
+
+typedef CUresult  CUDAAPI tcuCtxGetDevice(CUdevice *device);
+typedef CUresult  CUDAAPI tcuCtxSynchronize(void);
+
+
+/************************************
+ **
+ **    Module management
+ **
+ ***********************************/
+typedef CUresult  CUDAAPI tcuModuleLoad(CUmodule *module, const char *fname);
+typedef CUresult  CUDAAPI tcuModuleLoadData(CUmodule *module, const void *image);
+typedef CUresult  CUDAAPI tcuModuleLoadDataEx(CUmodule *module, const void *image, unsigned int numOptions, CUjit_option *options, void **optionValues);
+typedef CUresult  CUDAAPI tcuModuleLoadFatBinary(CUmodule *module, const void *fatCubin);
+typedef CUresult  CUDAAPI tcuModuleUnload(CUmodule hmod);
+typedef CUresult  CUDAAPI tcuModuleGetFunction(CUfunction *hfunc, CUmodule hmod, const char *name);
+
+#if __CUDA_API_VERSION >= 3020
+    typedef CUresult  CUDAAPI tcuModuleGetGlobal(CUdeviceptr *dptr, size_t *bytes, CUmodule hmod, const char *name);
+#else
+    typedef CUresult  CUDAAPI tcuModuleGetGlobal(CUdeviceptr *dptr, unsigned int *bytes, CUmodule hmod, const char *name);
+#endif
+
+typedef CUresult  CUDAAPI tcuModuleGetTexRef(CUtexref *pTexRef, CUmodule hmod, const char *name);
+typedef CUresult  CUDAAPI tcuModuleGetSurfRef(CUsurfref *pSurfRef, CUmodule hmod, const char *name);
+
+/************************************
+ **
+ **    Memory management
+ **
+ ***********************************/
+#if __CUDA_API_VERSION >= 3020
+    typedef CUresult CUDAAPI tcuMemGetInfo(size_t *free, size_t *total);
+    typedef CUresult CUDAAPI tcuMemAlloc(CUdeviceptr *dptr, size_t bytesize);
+    typedef CUresult CUDAAPI tcuMemGetAddressRange(CUdeviceptr *pbase, size_t *psize, CUdeviceptr dptr);
+    typedef CUresult CUDAAPI tcuMemAllocPitch(CUdeviceptr *dptr,
+                                              size_t *pPitch,
+                                              size_t WidthInBytes,
+                                              size_t Height,
+                                              // size of biggest r/w to be performed by kernels on this memory
+                                              // 4, 8 or 16 bytes
+                                              unsigned int ElementSizeBytes
+                                             );
+#else
+    typedef CUresult CUDAAPI tcuMemGetInfo(unsigned int *free, unsigned int *total);
+    typedef CUresult CUDAAPI tcuMemAlloc(CUdeviceptr *dptr, unsigned int bytesize);
+    typedef CUresult CUDAAPI tcuMemGetAddressRange(CUdeviceptr *pbase, unsigned int *psize, CUdeviceptr dptr);
+    typedef CUresult CUDAAPI tcuMemAllocPitch(CUdeviceptr *dptr,
+                                              unsigned int *pPitch,
+                                              unsigned int WidthInBytes,
+                                              unsigned int Height,
+                                              // size of biggest r/w to be performed by kernels on this memory
+                                              // 4, 8 or 16 bytes
+                                              unsigned int ElementSizeBytes
+                                             );
+#endif
+
+typedef CUresult CUDAAPI tcuMemFree(CUdeviceptr dptr);
+
+#if __CUDA_API_VERSION >= 3020
+    typedef CUresult CUDAAPI tcuMemAllocHost(void **pp, size_t bytesize);
+#else
+    typedef CUresult CUDAAPI tcuMemAllocHost(void **pp, unsigned int bytesize);
+#endif
+
+typedef CUresult CUDAAPI tcuMemFreeHost(void *p);
+typedef CUresult CUDAAPI tcuMemHostAlloc(void **pp, size_t bytesize, unsigned int Flags);
+
+typedef CUresult CUDAAPI tcuMemHostGetDevicePointer(CUdeviceptr *pdptr, void *p, unsigned int Flags);
+typedef CUresult CUDAAPI tcuMemHostGetFlags(unsigned int *pFlags, void *p);
+
+typedef CUresult CUDAAPI tcuMemHostRegister(void *p, size_t bytesize, unsigned int Flags);
+typedef CUresult CUDAAPI tcuMemHostUnregister(void *p);;
+typedef CUresult CUDAAPI tcuMemcpy(CUdeviceptr dst, CUdeviceptr src, size_t ByteCount);
+typedef CUresult CUDAAPI tcuMemcpyPeer(CUdeviceptr dstDevice, CUcontext dstContext, CUdeviceptr srcDevice, CUcontext srcContext, size_t ByteCount);
+
+/************************************
+ **
+ **    Synchronous Memcpy
+ **
+ ** Intra-device memcpy's done with these functions may execute in parallel with the CPU,
+ ** but if host memory is involved, they wait until the copy is done before returning.
+ **
+ ***********************************/
+// 1D functions
+#if __CUDA_API_VERSION >= 3020
+    // system <-> device memory
+    typedef CUresult  CUDAAPI tcuMemcpyHtoD(CUdeviceptr dstDevice, const void *srcHost, size_t ByteCount);
+    typedef CUresult  CUDAAPI tcuMemcpyDtoH(void *dstHost, CUdeviceptr srcDevice, size_t ByteCount);
+
+    // device <-> device memory
+    typedef CUresult  CUDAAPI tcuMemcpyDtoD(CUdeviceptr dstDevice, CUdeviceptr srcDevice, size_t ByteCount);
+
+    // device <-> array memory
+    typedef CUresult  CUDAAPI tcuMemcpyDtoA(CUarray dstArray, size_t dstOffset, CUdeviceptr srcDevice, size_t ByteCount);
+    typedef CUresult  CUDAAPI tcuMemcpyAtoD(CUdeviceptr dstDevice, CUarray srcArray, size_t srcOffset, size_t ByteCount);
+
+    // system <-> array memory
+    typedef CUresult  CUDAAPI tcuMemcpyHtoA(CUarray dstArray, size_t dstOffset, const void *srcHost, size_t ByteCount);
+    typedef CUresult  CUDAAPI tcuMemcpyAtoH(void *dstHost, CUarray srcArray, size_t srcOffset, size_t ByteCount);
+
+    // array <-> array memory
+    typedef CUresult  CUDAAPI tcuMemcpyAtoA(CUarray dstArray, size_t dstOffset, CUarray srcArray, size_t srcOffset, size_t ByteCount);
+#else
+    // system <-> device memory
+    typedef CUresult  CUDAAPI tcuMemcpyHtoD(CUdeviceptr dstDevice, const void *srcHost, unsigned int ByteCount);
+    typedef CUresult  CUDAAPI tcuMemcpyDtoH(void *dstHost, CUdeviceptr srcDevice, unsigned int ByteCount);
+
+    // device <-> device memory
+    typedef CUresult  CUDAAPI tcuMemcpyDtoD(CUdeviceptr dstDevice, CUdeviceptr srcDevice, unsigned int ByteCount);
+
+    // device <-> array memory
+    typedef CUresult  CUDAAPI tcuMemcpyDtoA(CUarray dstArray, unsigned int dstOffset, CUdeviceptr srcDevice, unsigned int ByteCount);
+    typedef CUresult  CUDAAPI tcuMemcpyAtoD(CUdeviceptr dstDevice, CUarray srcArray, unsigned int srcOffset, unsigned int ByteCount);
+
+    // system <-> array memory
+    typedef CUresult  CUDAAPI tcuMemcpyHtoA(CUarray dstArray, unsigned int dstOffset, const void *srcHost, unsigned int ByteCount);
+    typedef CUresult  CUDAAPI tcuMemcpyAtoH(void *dstHost, CUarray srcArray, unsigned int srcOffset, unsigned int ByteCount);
+
+    // array <-> array memory
+    typedef CUresult  CUDAAPI tcuMemcpyAtoA(CUarray dstArray, unsigned int dstOffset, CUarray srcArray, unsigned int srcOffset, unsigned int ByteCount);
+#endif
+
+// 2D memcpy
+typedef CUresult  CUDAAPI tcuMemcpy2D(const CUDA_MEMCPY2D *pCopy);
+typedef CUresult  CUDAAPI tcuMemcpy2DUnaligned(const CUDA_MEMCPY2D *pCopy);
+
+// 3D memcpy
+typedef CUresult  CUDAAPI tcuMemcpy3D(const CUDA_MEMCPY3D *pCopy);
+
+/************************************
+ **
+ **    Asynchronous Memcpy
+ **
+ ** Any host memory involved must be DMA'able (e.g., allocated with cuMemAllocHost).
+ ** memcpy's done with these functions execute in parallel with the CPU and, if
+ ** the hardware is available, may execute in parallel with the GPU.
+ ** Asynchronous memcpy must be accompanied by appropriate stream synchronization.
+ **
+ ***********************************/
+
+// 1D functions
+#if __CUDA_API_VERSION >= 3020
+    // system <-> device memory
+    typedef CUresult  CUDAAPI tcuMemcpyHtoDAsync(CUdeviceptr dstDevice,
+                                                 const void *srcHost, size_t ByteCount, CUstream hStream);
+    typedef CUresult  CUDAAPI tcuMemcpyDtoHAsync(void *dstHost,
+                                                 CUdeviceptr srcDevice, size_t ByteCount, CUstream hStream);
+
+    // device <-> device memory
+    typedef CUresult  CUDAAPI tcuMemcpyDtoDAsync(CUdeviceptr dstDevice,
+                                                 CUdeviceptr srcDevice, size_t ByteCount, CUstream hStream);
+
+    // system <-> array memory
+    typedef CUresult  CUDAAPI tcuMemcpyHtoAAsync(CUarray dstArray, size_t dstOffset,
+                                                 const void *srcHost, size_t ByteCount, CUstream hStream);
+    typedef CUresult  CUDAAPI tcuMemcpyAtoHAsync(void *dstHost, CUarray srcArray, size_t srcOffset,
+                                                 size_t ByteCount, CUstream hStream);
+#else
+    // system <-> device memory
+    typedef CUresult  CUDAAPI tcuMemcpyHtoDAsync(CUdeviceptr dstDevice,
+                                                 const void *srcHost, unsigned int ByteCount, CUstream hStream);
+    typedef CUresult  CUDAAPI tcuMemcpyDtoHAsync(void *dstHost,
+                                                 CUdeviceptr srcDevice, unsigned int ByteCount, CUstream hStream);
+
+    // device <-> device memory
+    typedef CUresult  CUDAAPI tcuMemcpyDtoDAsync(CUdeviceptr dstDevice,
+                                                 CUdeviceptr srcDevice, unsigned int ByteCount, CUstream hStream);
+
+    // system <-> array memory
+    typedef CUresult  CUDAAPI tcuMemcpyHtoAAsync(CUarray dstArray, unsigned int dstOffset,
+                                                 const void *srcHost, unsigned int ByteCount, CUstream hStream);
+    typedef CUresult  CUDAAPI tcuMemcpyAtoHAsync(void *dstHost, CUarray srcArray, unsigned int srcOffset,
+                                                 unsigned int ByteCount, CUstream hStream);
+#endif
+
+// 2D memcpy
+typedef CUresult  CUDAAPI tcuMemcpy2DAsync(const CUDA_MEMCPY2D *pCopy, CUstream hStream);
+
+// 3D memcpy
+typedef CUresult  CUDAAPI tcuMemcpy3DAsync(const CUDA_MEMCPY3D *pCopy, CUstream hStream);
+
+/************************************
+ **
+ **    Memset
+ **
+ ***********************************/
+typedef CUresult  CUDAAPI tcuMemsetD8(CUdeviceptr dstDevice, unsigned char uc, unsigned int N);
+typedef CUresult  CUDAAPI tcuMemsetD16(CUdeviceptr dstDevice, unsigned short us, unsigned int N);
+typedef CUresult  CUDAAPI tcuMemsetD32(CUdeviceptr dstDevice, unsigned int ui, unsigned int N);
+
+#if __CUDA_API_VERSION >= 3020
+    typedef CUresult  CUDAAPI tcuMemsetD2D8(CUdeviceptr dstDevice, unsigned int dstPitch, unsigned char uc, size_t Width, size_t Height);
+    typedef CUresult  CUDAAPI tcuMemsetD2D16(CUdeviceptr dstDevice, unsigned int dstPitch, unsigned short us, size_t Width, size_t Height);
+    typedef CUresult  CUDAAPI tcuMemsetD2D32(CUdeviceptr dstDevice, unsigned int dstPitch, unsigned int ui, size_t Width, size_t Height);
+#else
+    typedef CUresult  CUDAAPI tcuMemsetD2D8(CUdeviceptr dstDevice, unsigned int dstPitch, unsigned char uc, unsigned int Width, unsigned int Height);
+    typedef CUresult  CUDAAPI tcuMemsetD2D16(CUdeviceptr dstDevice, unsigned int dstPitch, unsigned short us, unsigned int Width, unsigned int Height);
+    typedef CUresult  CUDAAPI tcuMemsetD2D32(CUdeviceptr dstDevice, unsigned int dstPitch, unsigned int ui, unsigned int Width, unsigned int Height);
+#endif
+
+/************************************
+ **
+ **    Function management
+ **
+ ***********************************/
+
+
+typedef CUresult CUDAAPI tcuFuncSetBlockShape(CUfunction hfunc, int x, int y, int z);
+typedef CUresult CUDAAPI tcuFuncSetSharedSize(CUfunction hfunc, unsigned int bytes);
+typedef CUresult CUDAAPI tcuFuncGetAttribute(int *pi, CUfunction_attribute attrib, CUfunction hfunc);
+typedef CUresult CUDAAPI tcuFuncSetCacheConfig(CUfunction hfunc, CUfunc_cache config);
+typedef CUresult CUDAAPI tcuLaunchKernel(CUfunction f,
+                                         unsigned int gridDimX,  unsigned int gridDimY,  unsigned int gridDimZ,
+                                         unsigned int blockDimX, unsigned int blockDimY, unsigned int blockDimZ,
+                                         unsigned int sharedMemBytes,
+                                         CUstream hStream, void **kernelParams, void **extra);
+
+/************************************
+ **
+ **    Array management
+ **
+ ***********************************/
+
+typedef CUresult  CUDAAPI tcuArrayCreate(CUarray *pHandle, const CUDA_ARRAY_DESCRIPTOR *pAllocateArray);
+typedef CUresult  CUDAAPI tcuArrayGetDescriptor(CUDA_ARRAY_DESCRIPTOR *pArrayDescriptor, CUarray hArray);
+typedef CUresult  CUDAAPI tcuArrayDestroy(CUarray hArray);
+
+typedef CUresult  CUDAAPI tcuArray3DCreate(CUarray *pHandle, const CUDA_ARRAY3D_DESCRIPTOR *pAllocateArray);
+typedef CUresult  CUDAAPI tcuArray3DGetDescriptor(CUDA_ARRAY3D_DESCRIPTOR *pArrayDescriptor, CUarray hArray);
+
+
+/************************************
+ **
+ **    Texture reference management
+ **
+ ***********************************/
+typedef CUresult  CUDAAPI tcuTexRefCreate(CUtexref *pTexRef);
+typedef CUresult  CUDAAPI tcuTexRefDestroy(CUtexref hTexRef);
+
+typedef CUresult  CUDAAPI tcuTexRefSetArray(CUtexref hTexRef, CUarray hArray, unsigned int Flags);
+
+#if __CUDA_API_VERSION >= 3020
+    typedef CUresult  CUDAAPI tcuTexRefSetAddress(size_t *ByteOffset, CUtexref hTexRef, CUdeviceptr dptr, size_t bytes);
+    typedef CUresult  CUDAAPI tcuTexRefSetAddress2D(CUtexref hTexRef, const CUDA_ARRAY_DESCRIPTOR *desc, CUdeviceptr dptr, size_t Pitch);
+#else
+    typedef CUresult  CUDAAPI tcuTexRefSetAddress(unsigned int *ByteOffset, CUtexref hTexRef, CUdeviceptr dptr, unsigned int bytes);
+    typedef CUresult  CUDAAPI tcuTexRefSetAddress2D(CUtexref hTexRef, const CUDA_ARRAY_DESCRIPTOR *desc, CUdeviceptr dptr, unsigned int Pitch);
+#endif
+
+typedef CUresult  CUDAAPI tcuTexRefSetFormat(CUtexref hTexRef, CUarray_format fmt, int NumPackedComponents);
+typedef CUresult  CUDAAPI tcuTexRefSetAddressMode(CUtexref hTexRef, int dim, CUaddress_mode am);
+typedef CUresult  CUDAAPI tcuTexRefSetFilterMode(CUtexref hTexRef, CUfilter_mode fm);
+typedef CUresult  CUDAAPI tcuTexRefSetFlags(CUtexref hTexRef, unsigned int Flags);
+
+typedef CUresult  CUDAAPI tcuTexRefGetAddress(CUdeviceptr *pdptr, CUtexref hTexRef);
+typedef CUresult  CUDAAPI tcuTexRefGetArray(CUarray *phArray, CUtexref hTexRef);
+typedef CUresult  CUDAAPI tcuTexRefGetAddressMode(CUaddress_mode *pam, CUtexref hTexRef, int dim);
+typedef CUresult  CUDAAPI tcuTexRefGetFilterMode(CUfilter_mode *pfm, CUtexref hTexRef);
+typedef CUresult  CUDAAPI tcuTexRefGetFormat(CUarray_format *pFormat, int *pNumChannels, CUtexref hTexRef);
+typedef CUresult  CUDAAPI tcuTexRefGetFlags(unsigned int *pFlags, CUtexref hTexRef);
+
+/************************************
+ **
+ **    Surface reference management
+ **
+ ***********************************/
+typedef CUresult  CUDAAPI tcuSurfRefSetArray(CUsurfref hSurfRef, CUarray hArray, unsigned int Flags);
+typedef CUresult  CUDAAPI tcuSurfRefGetArray(CUarray *phArray, CUsurfref hSurfRef);
+
+/************************************
+ **
+ **    Parameter management
+ **
+ ***********************************/
+
+typedef CUresult  CUDAAPI tcuParamSetSize(CUfunction hfunc, unsigned int numbytes);
+typedef CUresult  CUDAAPI tcuParamSeti(CUfunction hfunc, int offset, unsigned int value);
+typedef CUresult  CUDAAPI tcuParamSetf(CUfunction hfunc, int offset, float value);
+typedef CUresult  CUDAAPI tcuParamSetv(CUfunction hfunc, int offset, void *ptr, unsigned int numbytes);
+typedef CUresult  CUDAAPI tcuParamSetTexRef(CUfunction hfunc, int texunit, CUtexref hTexRef);
+
+
+/************************************
+ **
+ **    Launch functions
+ **
+ ***********************************/
+
+typedef CUresult CUDAAPI tcuLaunch(CUfunction f);
+typedef CUresult CUDAAPI tcuLaunchGrid(CUfunction f, int grid_width, int grid_height);
+typedef CUresult CUDAAPI tcuLaunchGridAsync(CUfunction f, int grid_width, int grid_height, CUstream hStream);
+
+/************************************
+ **
+ **    Events
+ **
+ ***********************************/
+typedef CUresult CUDAAPI tcuEventCreate(CUevent *phEvent, unsigned int Flags);
+typedef CUresult CUDAAPI tcuEventRecord(CUevent hEvent, CUstream hStream);
+typedef CUresult CUDAAPI tcuEventQuery(CUevent hEvent);
+typedef CUresult CUDAAPI tcuEventSynchronize(CUevent hEvent);
+typedef CUresult CUDAAPI tcuEventDestroy(CUevent hEvent);
+typedef CUresult CUDAAPI tcuEventElapsedTime(float *pMilliseconds, CUevent hStart, CUevent hEnd);
+
+/************************************
+ **
+ **    Streams
+ **
+ ***********************************/
+typedef CUresult CUDAAPI  tcuStreamCreate(CUstream *phStream, unsigned int Flags);
+typedef CUresult CUDAAPI  tcuStreamQuery(CUstream hStream);
+typedef CUresult CUDAAPI  tcuStreamSynchronize(CUstream hStream);
+typedef CUresult CUDAAPI  tcuStreamDestroy(CUstream hStream);
+
+/************************************
+ **
+ **    Graphics interop
+ **
+ ***********************************/
+typedef CUresult CUDAAPI tcuGraphicsUnregisterResource(CUgraphicsResource resource);
+typedef CUresult CUDAAPI tcuGraphicsSubResourceGetMappedArray(CUarray *pArray, CUgraphicsResource resource, unsigned int arrayIndex, unsigned int mipLevel);
+
+#if __CUDA_API_VERSION >= 3020
+    typedef CUresult CUDAAPI tcuGraphicsResourceGetMappedPointer(CUdeviceptr *pDevPtr, size_t *pSize, CUgraphicsResource resource);
+#else
+    typedef CUresult CUDAAPI tcuGraphicsResourceGetMappedPointer(CUdeviceptr *pDevPtr, unsigned int *pSize, CUgraphicsResource resource);
+#endif
+
+typedef CUresult CUDAAPI tcuGraphicsResourceSetMapFlags(CUgraphicsResource resource, unsigned int flags);
+typedef CUresult CUDAAPI tcuGraphicsMapResources(unsigned int count, CUgraphicsResource *resources, CUstream hStream);
+typedef CUresult CUDAAPI tcuGraphicsUnmapResources(unsigned int count, CUgraphicsResource *resources, CUstream hStream);
+
+/************************************
+ **
+ **    Export tables
+ **
+ ***********************************/
+typedef CUresult CUDAAPI tcuGetExportTable(const void **ppExportTable, const CUuuid *pExportTableId);
+
+/************************************
+ **
+ **    Limits
+ **
+ ***********************************/
+
+typedef CUresult CUDAAPI tcuCtxSetLimit(CUlimit limit, size_t value);
+typedef CUresult CUDAAPI tcuCtxGetLimit(size_t *pvalue, CUlimit limit);
+
+
+extern tcuDriverGetVersion             *cuDriverGetVersion;
+extern tcuDeviceGet                    *cuDeviceGet;
+extern tcuDeviceGetCount               *cuDeviceGetCount;
+extern tcuDeviceGetName                *cuDeviceGetName;
+extern tcuDeviceComputeCapability      *cuDeviceComputeCapability;
+extern tcuDeviceGetProperties          *cuDeviceGetProperties;
+extern tcuDeviceGetAttribute           *cuDeviceGetAttribute;
+extern tcuCtxDestroy                   *cuCtxDestroy;
+extern tcuCtxAttach                    *cuCtxAttach;
+extern tcuCtxDetach                    *cuCtxDetach;
+extern tcuCtxPushCurrent               *cuCtxPushCurrent;
+extern tcuCtxPopCurrent                *cuCtxPopCurrent;
+
+extern tcuCtxSetCurrent                *cuCtxSetCurrent;
+extern tcuCtxGetCurrent                *cuCtxGetCurrent;
+
+extern tcuCtxGetDevice                 *cuCtxGetDevice;
+extern tcuCtxSynchronize               *cuCtxSynchronize;
+extern tcuModuleLoad                   *cuModuleLoad;
+extern tcuModuleLoadData               *cuModuleLoadData;
+extern tcuModuleLoadDataEx             *cuModuleLoadDataEx;
+extern tcuModuleLoadFatBinary          *cuModuleLoadFatBinary;
+extern tcuModuleUnload                 *cuModuleUnload;
+extern tcuModuleGetFunction            *cuModuleGetFunction;
+extern tcuModuleGetTexRef              *cuModuleGetTexRef;
+extern tcuModuleGetSurfRef             *cuModuleGetSurfRef;
+extern tcuMemFreeHost                  *cuMemFreeHost;
+extern tcuMemHostAlloc                 *cuMemHostAlloc;
+extern tcuMemHostGetFlags              *cuMemHostGetFlags;
+
+extern tcuMemHostRegister              *cuMemHostRegister;
+extern tcuMemHostUnregister            *cuMemHostUnregister;
+extern tcuMemcpy                       *cuMemcpy;
+extern tcuMemcpyPeer                   *cuMemcpyPeer;
+
+extern tcuDeviceTotalMem               *cuDeviceTotalMem;
+extern tcuCtxCreate                    *cuCtxCreate;
+extern tcuModuleGetGlobal              *cuModuleGetGlobal;
+extern tcuMemGetInfo                   *cuMemGetInfo;
+extern tcuMemAlloc                     *cuMemAlloc;
+extern tcuMemAllocPitch                *cuMemAllocPitch;
+extern tcuMemFree                      *cuMemFree;
+extern tcuMemGetAddressRange           *cuMemGetAddressRange;
+extern tcuMemAllocHost                 *cuMemAllocHost;
+extern tcuMemHostGetDevicePointer      *cuMemHostGetDevicePointer;
+extern tcuFuncSetBlockShape            *cuFuncSetBlockShape;
+extern tcuFuncSetSharedSize            *cuFuncSetSharedSize;
+extern tcuFuncGetAttribute             *cuFuncGetAttribute;
+extern tcuFuncSetCacheConfig           *cuFuncSetCacheConfig;
+extern tcuLaunchKernel                 *cuLaunchKernel;
+extern tcuArrayDestroy                 *cuArrayDestroy;
+extern tcuTexRefCreate                 *cuTexRefCreate;
+extern tcuTexRefDestroy                *cuTexRefDestroy;
+extern tcuTexRefSetArray               *cuTexRefSetArray;
+extern tcuTexRefSetFormat              *cuTexRefSetFormat;
+extern tcuTexRefSetAddressMode         *cuTexRefSetAddressMode;
+extern tcuTexRefSetFilterMode          *cuTexRefSetFilterMode;
+extern tcuTexRefSetFlags               *cuTexRefSetFlags;
+extern tcuTexRefGetArray               *cuTexRefGetArray;
+extern tcuTexRefGetAddressMode         *cuTexRefGetAddressMode;
+extern tcuTexRefGetFilterMode          *cuTexRefGetFilterMode;
+extern tcuTexRefGetFormat              *cuTexRefGetFormat;
+extern tcuTexRefGetFlags               *cuTexRefGetFlags;
+extern tcuSurfRefSetArray              *cuSurfRefSetArray;
+extern tcuSurfRefGetArray              *cuSurfRefGetArray;
+extern tcuParamSetSize                 *cuParamSetSize;
+extern tcuParamSeti                    *cuParamSeti;
+extern tcuParamSetf                    *cuParamSetf;
+extern tcuParamSetv                    *cuParamSetv;
+extern tcuParamSetTexRef               *cuParamSetTexRef;
+extern tcuLaunch                       *cuLaunch;
+extern tcuLaunchGrid                   *cuLaunchGrid;
+extern tcuLaunchGridAsync              *cuLaunchGridAsync;
+extern tcuEventCreate                  *cuEventCreate;
+extern tcuEventRecord                  *cuEventRecord;
+extern tcuEventQuery                   *cuEventQuery;
+extern tcuEventSynchronize             *cuEventSynchronize;
+extern tcuEventDestroy                 *cuEventDestroy;
+extern tcuEventElapsedTime             *cuEventElapsedTime;
+extern tcuStreamCreate                 *cuStreamCreate;
+extern tcuStreamQuery                  *cuStreamQuery;
+extern tcuStreamSynchronize            *cuStreamSynchronize;
+extern tcuStreamDestroy                *cuStreamDestroy;
+extern tcuGraphicsUnregisterResource   *cuGraphicsUnregisterResource;
+extern tcuGraphicsSubResourceGetMappedArray  *cuGraphicsSubResourceGetMappedArray;
+extern tcuGraphicsResourceSetMapFlags  *cuGraphicsResourceSetMapFlags;
+extern tcuGraphicsMapResources         *cuGraphicsMapResources;
+extern tcuGraphicsUnmapResources       *cuGraphicsUnmapResources;
+extern tcuGetExportTable               *cuGetExportTable;
+extern tcuCtxSetLimit                  *cuCtxSetLimit;
+extern tcuCtxGetLimit                  *cuCtxGetLimit;
+
+// These functions could be using the CUDA 3.2 interface (_v2)
+extern tcuMemcpyHtoD                   *cuMemcpyHtoD;
+extern tcuMemcpyDtoH                   *cuMemcpyDtoH;
+extern tcuMemcpyDtoD                   *cuMemcpyDtoD;
+extern tcuMemcpyDtoA                   *cuMemcpyDtoA;
+extern tcuMemcpyAtoD                   *cuMemcpyAtoD;
+extern tcuMemcpyHtoA                   *cuMemcpyHtoA;
+extern tcuMemcpyAtoH                   *cuMemcpyAtoH;
+extern tcuMemcpyAtoA                   *cuMemcpyAtoA;
+extern tcuMemcpy2D                     *cuMemcpy2D;
+extern tcuMemcpy2DUnaligned            *cuMemcpy2DUnaligned;
+extern tcuMemcpy3D                     *cuMemcpy3D;
+extern tcuMemcpyHtoDAsync              *cuMemcpyHtoDAsync;
+extern tcuMemcpyDtoHAsync              *cuMemcpyDtoHAsync;
+extern tcuMemcpyDtoDAsync              *cuMemcpyDtoDAsync;
+extern tcuMemcpyHtoAAsync              *cuMemcpyHtoAAsync;
+extern tcuMemcpyAtoHAsync              *cuMemcpyAtoHAsync;
+extern tcuMemcpy2DAsync                *cuMemcpy2DAsync;
+extern tcuMemcpy3DAsync                *cuMemcpy3DAsync;
+extern tcuMemsetD8                     *cuMemsetD8;
+extern tcuMemsetD16                    *cuMemsetD16;
+extern tcuMemsetD32                    *cuMemsetD32;
+extern tcuMemsetD2D8                   *cuMemsetD2D8;
+extern tcuMemsetD2D16                  *cuMemsetD2D16;
+extern tcuMemsetD2D32                  *cuMemsetD2D32;
+extern tcuArrayCreate                  *cuArrayCreate;
+extern tcuArrayGetDescriptor           *cuArrayGetDescriptor;
+extern tcuArray3DCreate                *cuArray3DCreate;
+extern tcuArray3DGetDescriptor         *cuArray3DGetDescriptor;
+extern tcuTexRefSetAddress             *cuTexRefSetAddress;
+extern tcuTexRefSetAddress2D           *cuTexRefSetAddress2D;
+extern tcuTexRefGetAddress             *cuTexRefGetAddress;
+extern tcuGraphicsResourceGetMappedPointer   *cuGraphicsResourceGetMappedPointer;
+
+/************************************/
+CUresult CUDAAPI cuInit   (unsigned int, int cudaVersion, void *hHandleDriver);
+/************************************/
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif //__cuda_cuda_h__
diff --git a/webrtc/modules/video_coding/codecs/h264/include/dynlink_cuda_d3d.h b/webrtc/modules/video_coding/codecs/h264/include/dynlink_cuda_d3d.h
new file mode 100644
index 0000000..6aface6
--- /dev/null
+++ b/webrtc/modules/video_coding/codecs/h264/include/dynlink_cuda_d3d.h
@@ -0,0 +1,110 @@
+/*
+ * Copyright 1993-2015 NVIDIA Corporation.  All rights reserved.
+ *
+ * Please refer to the NVIDIA end user license agreement (EULA) associated
+ * with this source code for terms and conditions that govern your use of
+ * this software. Any use, reproduction, disclosure, or distribution of
+ * this software and related documentation outside the terms of the EULA
+ * is strictly prohibited.
+ *
+ */
+
+#ifndef __cuda_d3d_h__
+#define __cuda_d3d_h__
+
+#if defined(WIN32) || defined(_WIN32) || defined(WIN64) || defined(_WIN64)
+#pragma warning(disable: 4312)
+
+#if defined (CUDA_INIT_D3D9) || defined(CUDA_INIT_D3D10) || defined(CUDA_INIT_D3D11)
+#include <Windows.h>
+#include <mmsystem.h>
+#endif
+
+#ifdef CUDA_INIT_D3D9
+#include <d3dx9.h>
+#pragma warning( disable : 4996 ) // disable deprecated warning 
+#include <strsafe.h>
+#pragma warning( default : 4996 )
+
+/**
+ * CUDA 2.x compatibility - Flags to register a D3D9 graphics resource
+ */
+typedef enum CUd3d9register_flags_enum
+{
+    CU_D3D9_REGISTER_FLAGS_NONE  = 0x00,
+    CU_D3D9_REGISTER_FLAGS_ARRAY = 0x01,
+} CUd3d9register_flags;
+
+/**
+ * CUDA 2.x compatibility - Flags for D3D9 mapping and unmapping interop resources
+ */
+typedef enum CUd3d9map_flags_enum
+{
+    CU_D3D9_MAPRESOURCE_FLAGS_NONE         = 0x00,
+    CU_D3D9_MAPRESOURCE_FLAGS_READONLY     = 0x01,
+    CU_D3D9_MAPRESOURCE_FLAGS_WRITEDISCARD = 0x02,
+} CUd3d9map_flags;
+
+// D3D9/CUDA interop (CUDA 1.x compatible API). These functions are deprecated, please use the ones below
+typedef CUresult CUDAAPI tcuD3D9Begin(IDirect3DDevice9 *pDevice);
+typedef CUresult CUDAAPI tcuD3D9End(void);
+typedef CUresult CUDAAPI tcuD3D9RegisterVertexBuffer(IDirect3DVertexBuffer9 *pVB);
+typedef CUresult CUDAAPI tcuD3D9MapVertexBuffer(CUdeviceptr *pDevPtr, unsigned int *pSize, IDirect3DVertexBuffer9 *pVB);
+typedef CUresult CUDAAPI tcuD3D9UnmapVertexBuffer(IDirect3DVertexBuffer9 *pVB);
+typedef CUresult CUDAAPI tcuD3D9UnregisterVertexBuffer(IDirect3DVertexBuffer9 *pVB);
+
+// D3D9/CUDA interop (CUDA 2.x compatible)
+typedef CUresult CUDAAPI tcuD3D9GetDirect3DDevice(IDirect3DDevice9 **ppD3DDevice);
+typedef CUresult CUDAAPI tcuD3D9RegisterResource(IDirect3DResource9 *pResource, unsigned int Flags);
+typedef CUresult CUDAAPI tcuD3D9UnregisterResource(IDirect3DResource9 *pResource);
+
+typedef CUresult CUDAAPI tcuD3D9MapResources(unsigned int count, IDirect3DResource9 **ppResource);
+typedef CUresult CUDAAPI tcuD3D9UnmapResources(unsigned int count, IDirect3DResource9 **ppResource);
+typedef CUresult CUDAAPI tcuD3D9ResourceSetMapFlags(IDirect3DResource9 *pResource, unsigned int Flags);
+
+typedef CUresult CUDAAPI tcuD3D9ResourceGetSurfaceDimensions(unsigned int *pWidth, unsigned int *pHeight, unsigned int *pDepth, IDirect3DResource9 *pResource, unsigned int Face, unsigned int Level);
+typedef CUresult CUDAAPI tcuD3D9ResourceGetMappedArray(CUarray *pArray, IDirect3DResource9 *pResource, unsigned int Face, unsigned int Level);
+typedef CUresult CUDAAPI tcuD3D9ResourceGetMappedPointer(CUdeviceptr *pDevPtr, IDirect3DResource9 *pResource, unsigned int Face, unsigned int Level);
+typedef CUresult CUDAAPI tcuD3D9ResourceGetMappedSize(unsigned int *pSize, IDirect3DResource9 *pResource, unsigned int Face, unsigned int Level);
+typedef CUresult CUDAAPI tcuD3D9ResourceGetMappedPitch(unsigned int *pPitch, unsigned int *pPitchSlice, IDirect3DResource9 *pResource, unsigned int Face, unsigned int Level);
+
+// D3D9/CUDA interop (CUDA 2.0+)
+typedef CUresult CUDAAPI tcuD3D9GetDevice(CUdevice *pCudaDevice, const char *pszAdapterName);
+typedef CUresult CUDAAPI tcuD3D9CtxCreate(CUcontext *pCtx, CUdevice *pCudaDevice, unsigned int Flags, IDirect3DDevice9 *pD3DDevice);
+typedef CUresult CUDAAPI tcuGraphicsD3D9RegisterResource(CUgraphicsResource *pCudaResource, IDirect3DResource9 *pD3DResource, unsigned int Flags);
+#endif
+
+#ifdef CUDA_INIT_D3D10
+#include <dxgi.h>
+#include <d3d10_1.h>
+#include <d3d10.h>
+#include <d3dx10.h>
+
+#pragma warning( disable : 4996 ) // disable deprecated warning 
+#include <strsafe.h>
+#pragma warning( default : 4996 )
+
+// D3D11/CUDA interop (CUDA 3.0)
+typedef CUresult CUDAAPI tcuD3D10GetDevice(CUdevice *pCudaDevice, IDXGIAdapter *pAdapter);
+typedef CUresult CUDAAPI tcuD3D10CtxCreate(CUcontext *pCtx, CUdevice *pCudaDevice, unsigned int Flags, ID3D10Device *pD3DDevice);
+typedef CUresult CUDAAPI tcuGraphicsD3D10RegisterResource(CUgraphicsResource *pCudaResource, ID3D10Resource *pD3DResource, unsigned int Flags);
+#endif // CUDA_INIT_D3D10
+
+#ifdef CUDA_INIT_D3D11
+#include <dxgi.h>
+#include <d3d11.h>
+#include <d3dx11.h>
+
+#pragma warning( disable : 4996 ) // disable deprecated warning 
+#include <strsafe.h>
+#pragma warning( default : 4996 )
+
+// D3D11/CUDA interop (CUDA 3.0)
+typedef CUresult CUDAAPI tcuD3D11GetDevice(CUdevice *pCudaDevice, IDXGIAdapter *pAdapter);
+typedef CUresult CUDAAPI tcuD3D11CtxCreate(CUcontext *pCtx, CUdevice *pCudaDevice, unsigned int Flags, ID3D11Device *pD3DDevice);
+typedef CUresult CUDAAPI tcuGraphicsD3D11RegisterResource(CUgraphicsResource *pCudaResource, ID3D11Resource *pD3DResource, unsigned int Flags);
+#endif // CUDA_INIT_D3D11
+
+#endif // WIN32
+
+#endif // __cuda_d3d_h__
diff --git a/webrtc/modules/video_coding/codecs/h264/include/dynlink_cuda_gl.h b/webrtc/modules/video_coding/codecs/h264/include/dynlink_cuda_gl.h
new file mode 100644
index 0000000..3408bd8
--- /dev/null
+++ b/webrtc/modules/video_coding/codecs/h264/include/dynlink_cuda_gl.h
@@ -0,0 +1,54 @@
+/*
+ * Copyright 1993-2015 NVIDIA Corporation.  All rights reserved.
+ *
+ * Please refer to the NVIDIA end user license agreement (EULA) associated
+ * with this source code for terms and conditions that govern your use of
+ * this software. Any use, reproduction, disclosure, or distribution of
+ * this software and related documentation outside the terms of the EULA
+ * is strictly prohibited.
+ *
+ */
+
+#ifndef __dynlink_cuda_gl_h__
+#define __dynlink_cuda_gl_h__
+
+#if defined(WIN32) || defined(_WIN32) || defined(WIN64) || defined(_WIN64)
+#  define WINDOWS_LEAN_AND_MEAN
+#  define NOMINMAX
+#  include <windows.h>
+#endif
+
+// includes, system
+#include <stdlib.h>
+#include <stdio.h>
+#include <string.h>
+#include <math.h>
+
+// includes, GL
+#include <GL/glew.h>
+
+#if defined (__APPLE__) || defined(MACOSX)
+#include <GLUT/glut.h>
+#else
+#include <GL/freeglut.h>
+#endif
+
+/************************************
+ **
+ **    OpenGL Graphics/Interop
+ **
+ ***********************************/
+
+// OpenGL/CUDA interop (CUDA 2.0+)
+typedef CUresult CUDAAPI tcuGLCtxCreate(CUcontext *pCtx, unsigned int Flags, CUdevice device);
+typedef CUresult CUDAAPI tcuGraphicsGLRegisterBuffer(CUgraphicsResource *pCudaResource, GLuint buffer, unsigned int Flags);
+typedef CUresult CUDAAPI tcuGraphicsGLRegisterImage(CUgraphicsResource *pCudaResource, GLuint image, GLenum target, unsigned int Flags);
+
+#if defined(WIN32) || defined(_WIN32) || defined(WIN64) || defined(_WIN64)
+#include <GL/wglext.h>
+// WIN32
+typedef CUresult CUDAAPI tcuWGLGetDevice(CUdevice *pDevice, HGPUNV hGpu);
+#endif
+
+#endif // __dynlink_cuda_gl_h__
+
diff --git a/webrtc/modules/video_coding/codecs/h264/include/dynlink_cuviddec.h b/webrtc/modules/video_coding/codecs/h264/include/dynlink_cuviddec.h
new file mode 100644
index 0000000..6ef95bd
--- /dev/null
+++ b/webrtc/modules/video_coding/codecs/h264/include/dynlink_cuviddec.h
@@ -0,0 +1,844 @@
+/*
+ * This copyright notice applies to this header file only:
+ *
+ * Copyright (c) 2010-2016 NVIDIA Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person
+ * obtaining a copy of this software and associated documentation
+ * files (the "Software"), to deal in the Software without
+ * restriction, including without limitation the rights to use,
+ * copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the software, and to permit persons to whom the
+ * software is furnished to do so, subject to the following
+ * conditions:
+ *
+ * The above copyright notice and this permission notice shall be
+ * included in all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES
+ * OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT
+ * HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
+ * WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ */
+
+/**
+ * \file cuviddec.h
+ * NvCuvid API provides Video Decoding interface to NVIDIA GPU devices.
+ * \date 2015-2016
+ * This file contains constants, structure definitions and function prototypes used for decoding.
+ */
+
+#if !defined(__CUDA_VIDEO_H__)
+#define __CUDA_VIDEO_H__
+
+#ifndef __dynlink_cuda_h__
+#include "dynlink_cuda.h"
+#endif // __dynlink_cuda_h__
+
+#if defined(__x86_64) || defined(AMD64) || defined(_M_AMD64)
+#if (CUDA_VERSION >= 3020) && (!defined(CUDA_FORCE_API_VERSION) || (CUDA_FORCE_API_VERSION >= 3020))
+#define __CUVID_DEVPTR64
+#endif
+#endif
+
+#if defined(__cplusplus)
+extern "C" {
+#endif /* __cplusplus */
+
+typedef void *CUvideodecoder;
+typedef struct _CUcontextlock_st *CUvideoctxlock;
+
+/**
+ * \addtogroup VIDEO_DECODER Video Decoder
+ * @{
+ */
+
+/*!
+ * \enum cudaVideoCodec
+ * Video Codec Enums
+ */
+typedef enum cudaVideoCodec_enum {
+    cudaVideoCodec_MPEG1=0,                 /**<  MPEG1   */
+    cudaVideoCodec_MPEG2,                   /**<  MPEG2  */
+    cudaVideoCodec_MPEG4,                   /**<  MPEG4   */
+    cudaVideoCodec_VC1,                     /**<  VC1   */
+    cudaVideoCodec_H264,                    /**<  H264   */
+    cudaVideoCodec_JPEG,                    /**<  JPEG   */
+    cudaVideoCodec_H264_SVC,                /**<  H264-SVC   */
+    cudaVideoCodec_H264_MVC,                /**<  H264-MVC   */
+    cudaVideoCodec_HEVC,                    /**<  HEVC   */
+    cudaVideoCodec_VP8,                     /**<  VP8   */
+    cudaVideoCodec_VP9,                     /**<  VP9   */
+    cudaVideoCodec_NumCodecs,               /**<  Max COdecs   */
+    // Uncompressed YUV
+    cudaVideoCodec_YUV420 = (('I'<<24)|('Y'<<16)|('U'<<8)|('V')),   /**< Y,U,V (4:2:0)  */
+    cudaVideoCodec_YV12   = (('Y'<<24)|('V'<<16)|('1'<<8)|('2')),   /**< Y,V,U (4:2:0)  */
+    cudaVideoCodec_NV12   = (('N'<<24)|('V'<<16)|('1'<<8)|('2')),   /**< Y,UV  (4:2:0)  */
+    cudaVideoCodec_YUYV   = (('Y'<<24)|('U'<<16)|('Y'<<8)|('V')),   /**< YUYV/YUY2 (4:2:2)  */
+    cudaVideoCodec_UYVY   = (('U'<<24)|('Y'<<16)|('V'<<8)|('Y'))    /**< UYVY (4:2:2)  */
+} cudaVideoCodec;
+
+/*!
+ * \enum cudaVideoSurfaceFormat
+ * Video Surface Formats Enums
+ */
+typedef enum cudaVideoSurfaceFormat_enum {
+    cudaVideoSurfaceFormat_NV12=0       /**< NV12 (currently the only supported output format)  */
+} cudaVideoSurfaceFormat;
+
+/*!
+ * \enum cudaVideoDeinterlaceMode
+ * Deinterlacing Modes Enums
+ */
+typedef enum cudaVideoDeinterlaceMode_enum {
+    cudaVideoDeinterlaceMode_Weave=0,   /**< Weave both fields (no deinterlacing) */
+    cudaVideoDeinterlaceMode_Bob,       /**< Drop one field  */
+    cudaVideoDeinterlaceMode_Adaptive   /**< Adaptive deinterlacing  */
+} cudaVideoDeinterlaceMode;
+
+/*!
+ * \enum cudaVideoChromaFormat
+ * Chroma Formats Enums
+ */
+typedef enum cudaVideoChromaFormat_enum {
+    cudaVideoChromaFormat_Monochrome=0,  /**< MonoChrome */
+    cudaVideoChromaFormat_420,           /**< 4:2:0 */
+    cudaVideoChromaFormat_422,           /**< 4:2:2 */
+    cudaVideoChromaFormat_444            /**< 4:4:4 */
+} cudaVideoChromaFormat;
+
+/*!
+ * \enum cudaVideoCreateFlags
+ * Decoder Flags Enums
+ */
+typedef enum cudaVideoCreateFlags_enum {
+    cudaVideoCreate_Default = 0x00,     /**< Default operation mode: use dedicated video engines */
+    cudaVideoCreate_PreferCUDA = 0x01,  /**< Use a CUDA-based decoder if faster than dedicated engines (requires a valid vidLock object for multi-threading) */
+    cudaVideoCreate_PreferDXVA = 0x02,  /**< Go through DXVA internally if possible (requires D3D9 interop) */
+    cudaVideoCreate_PreferCUVID = 0x04  /**< Use dedicated video engines directly */
+} cudaVideoCreateFlags;
+
+/*!
+ * \struct CUVIDDECODECREATEINFO
+ * Struct used in create decoder
+ */
+typedef struct _CUVIDDECODECREATEINFO
+{
+    unsigned long ulWidth;              /**< Coded Sequence Width */
+    unsigned long ulHeight;             /**< Coded Sequence Height */
+    unsigned long ulNumDecodeSurfaces;  /**< Maximum number of internal decode surfaces */
+    cudaVideoCodec CodecType;           /**< cudaVideoCodec_XXX */
+    cudaVideoChromaFormat ChromaFormat; /**< cudaVideoChromaFormat_XXX (only 4:2:0 is currently supported) */
+    unsigned long ulCreationFlags;      /**< Decoder creation flags (cudaVideoCreateFlags_XXX) */
+    unsigned long bitDepthMinus8;
+    unsigned long Reserved1[4];         /**< Reserved for future use - set to zero */
+    /**
+    * area of the frame that should be displayed
+    */
+    struct {
+        short left;
+        short top;
+        short right;
+        short bottom;
+    } display_area;
+
+    cudaVideoSurfaceFormat OutputFormat;       /**< cudaVideoSurfaceFormat_XXX */
+    cudaVideoDeinterlaceMode DeinterlaceMode;  /**< cudaVideoDeinterlaceMode_XXX */
+    unsigned long ulTargetWidth;               /**< Post-processed Output Width (Should be aligned to 2) */
+    unsigned long ulTargetHeight;              /**< Post-processed Output Height (Should be aligbed to 2) */
+    unsigned long ulNumOutputSurfaces;         /**< Maximum number of output surfaces simultaneously mapped */
+    CUvideoctxlock vidLock;                    /**< If non-NULL, context lock used for synchronizing ownership of the cuda context */
+    /**
+    * target rectangle in the output frame (for aspect ratio conversion)
+    * if a null rectangle is specified, {0,0,ulTargetWidth,ulTargetHeight} will be used
+    */
+    struct {
+        short left;
+        short top;
+        short right;
+        short bottom;
+    } target_rect;
+    unsigned long Reserved2[5];                /**< Reserved for future use - set to zero */
+} CUVIDDECODECREATEINFO;
+
+/*!
+ * \struct CUVIDH264DPBENTRY
+ * H.264 DPB Entry
+ */
+typedef struct _CUVIDH264DPBENTRY
+{
+    int PicIdx;                 /**< picture index of reference frame */
+    int FrameIdx;               /**< frame_num(short-term) or LongTermFrameIdx(long-term) */
+    int is_long_term;           /**< 0=short term reference, 1=long term reference */
+    int not_existing;           /**< non-existing reference frame (corresponding PicIdx should be set to -1) */
+    int used_for_reference;     /**< 0=unused, 1=top_field, 2=bottom_field, 3=both_fields */
+    int FieldOrderCnt[2];       /**< field order count of top and bottom fields */
+} CUVIDH264DPBENTRY;
+
+/*!
+ * \struct CUVIDH264MVCEXT
+ * H.264 MVC Picture Parameters Ext
+ */
+typedef struct _CUVIDH264MVCEXT
+{
+    int num_views_minus1;
+    int view_id;
+    unsigned char inter_view_flag;
+    unsigned char num_inter_view_refs_l0;
+    unsigned char num_inter_view_refs_l1;
+    unsigned char MVCReserved8Bits;
+    int InterViewRefsL0[16];
+    int InterViewRefsL1[16];
+} CUVIDH264MVCEXT;
+
+/*!
+ * \struct CUVIDH264SVCEXT
+ * H.264 SVC Picture Parameters Ext
+ */
+typedef struct _CUVIDH264SVCEXT
+{
+    unsigned char profile_idc;
+    unsigned char level_idc;
+    unsigned char DQId;
+    unsigned char DQIdMax;
+    unsigned char disable_inter_layer_deblocking_filter_idc;
+    unsigned char ref_layer_chroma_phase_y_plus1;
+    signed char   inter_layer_slice_alpha_c0_offset_div2;
+    signed char   inter_layer_slice_beta_offset_div2;
+
+    unsigned short DPBEntryValidFlag;
+    unsigned char inter_layer_deblocking_filter_control_present_flag;
+    unsigned char extended_spatial_scalability_idc;
+    unsigned char adaptive_tcoeff_level_prediction_flag;
+    unsigned char slice_header_restriction_flag;
+    unsigned char chroma_phase_x_plus1_flag;
+    unsigned char chroma_phase_y_plus1;
+
+    unsigned char tcoeff_level_prediction_flag;
+    unsigned char constrained_intra_resampling_flag;
+    unsigned char ref_layer_chroma_phase_x_plus1_flag;
+    unsigned char store_ref_base_pic_flag;
+    unsigned char Reserved8BitsA;
+    unsigned char Reserved8BitsB;
+    // For the 4 scaled_ref_layer_XX fields below,
+    // if (extended_spatial_scalability_idc == 1), SPS field, G.7.3.2.1.4, add prefix "seq_"
+    // if (extended_spatial_scalability_idc == 2), SLH field, G.7.3.3.4,
+    short scaled_ref_layer_left_offset;
+    short scaled_ref_layer_top_offset;
+    short scaled_ref_layer_right_offset;
+    short scaled_ref_layer_bottom_offset;
+    unsigned short Reserved16Bits;
+    struct _CUVIDPICPARAMS *pNextLayer; /**< Points to the picparams for the next layer to be decoded. Linked list ends at the target layer. */
+    int bRefBaseLayer;                  /**< whether to store ref base pic */
+} CUVIDH264SVCEXT;
+
+/*!
+ * \struct CUVIDH264PICPARAMS
+ * H.264 Picture Parameters
+ */
+typedef struct _CUVIDH264PICPARAMS
+{
+    // SPS
+    int log2_max_frame_num_minus4;
+    int pic_order_cnt_type;
+    int log2_max_pic_order_cnt_lsb_minus4;
+    int delta_pic_order_always_zero_flag;
+    int frame_mbs_only_flag;
+    int direct_8x8_inference_flag;
+    int num_ref_frames;             // NOTE: shall meet level 4.1 restrictions
+    unsigned char residual_colour_transform_flag;
+    unsigned char bit_depth_luma_minus8;    // Must be 0 (only 8-bit supported)
+    unsigned char bit_depth_chroma_minus8;  // Must be 0 (only 8-bit supported)
+    unsigned char qpprime_y_zero_transform_bypass_flag;
+    // PPS
+    int entropy_coding_mode_flag;
+    int pic_order_present_flag;
+    int num_ref_idx_l0_active_minus1;
+    int num_ref_idx_l1_active_minus1;
+    int weighted_pred_flag;
+    int weighted_bipred_idc;
+    int pic_init_qp_minus26;
+    int deblocking_filter_control_present_flag;
+    int redundant_pic_cnt_present_flag;
+    int transform_8x8_mode_flag;
+    int MbaffFrameFlag;
+    int constrained_intra_pred_flag;
+    int chroma_qp_index_offset;
+    int second_chroma_qp_index_offset;
+    int ref_pic_flag;
+    int frame_num;
+    int CurrFieldOrderCnt[2];
+    // DPB
+    CUVIDH264DPBENTRY dpb[16];          // List of reference frames within the DPB
+    // Quantization Matrices (raster-order)
+    unsigned char WeightScale4x4[6][16];
+    unsigned char WeightScale8x8[2][64];
+    // FMO/ASO
+    unsigned char fmo_aso_enable;
+    unsigned char num_slice_groups_minus1;
+    unsigned char slice_group_map_type;
+    signed char pic_init_qs_minus26;
+    unsigned int slice_group_change_rate_minus1;
+    union
+    {
+        unsigned long long slice_group_map_addr;
+        const unsigned char *pMb2SliceGroupMap;
+    } fmo;
+    unsigned int  Reserved[12];
+    // SVC/MVC
+    union
+    {
+        CUVIDH264MVCEXT mvcext;
+        CUVIDH264SVCEXT svcext;
+    };
+} CUVIDH264PICPARAMS;
+
+
+/*!
+ * \struct CUVIDMPEG2PICPARAMS
+ * MPEG-2 Picture Parameters
+ */
+typedef struct _CUVIDMPEG2PICPARAMS
+{
+    int ForwardRefIdx;          // Picture index of forward reference (P/B-frames)
+    int BackwardRefIdx;         // Picture index of backward reference (B-frames)
+    int picture_coding_type;
+    int full_pel_forward_vector;
+    int full_pel_backward_vector;
+    int f_code[2][2];
+    int intra_dc_precision;
+    int frame_pred_frame_dct;
+    int concealment_motion_vectors;
+    int q_scale_type;
+    int intra_vlc_format;
+    int alternate_scan;
+    int top_field_first;
+    // Quantization matrices (raster order)
+    unsigned char QuantMatrixIntra[64];
+    unsigned char QuantMatrixInter[64];
+} CUVIDMPEG2PICPARAMS;
+
+////////////////////////////////////////////////////////////////////////////////////////////////
+//
+// MPEG-4 Picture Parameters
+//
+
+// MPEG-4 has VOP types instead of Picture types
+#define I_VOP 0
+#define P_VOP 1
+#define B_VOP 2
+#define S_VOP 3
+
+/*!
+ * \struct CUVIDMPEG4PICPARAMS
+ * MPEG-4 Picture Parameters
+ */
+typedef struct _CUVIDMPEG4PICPARAMS
+{
+    int ForwardRefIdx;          // Picture index of forward reference (P/B-frames)
+    int BackwardRefIdx;         // Picture index of backward reference (B-frames)
+    // VOL
+    int video_object_layer_width;
+    int video_object_layer_height;
+    int vop_time_increment_bitcount;
+    int top_field_first;
+    int resync_marker_disable;
+    int quant_type;
+    int quarter_sample;
+    int short_video_header;
+    int divx_flags;
+    // VOP
+    int vop_coding_type;
+    int vop_coded;
+    int vop_rounding_type;
+    int alternate_vertical_scan_flag;
+    int interlaced;
+    int vop_fcode_forward;
+    int vop_fcode_backward;
+    int trd[2];
+    int trb[2];
+    // Quantization matrices (raster order)
+    unsigned char QuantMatrixIntra[64];
+    unsigned char QuantMatrixInter[64];
+    int gmc_enabled;
+} CUVIDMPEG4PICPARAMS;
+
+/*!
+ * \struct CUVIDVC1PICPARAMS
+ * VC1 Picture Parameters
+ */
+typedef struct _CUVIDVC1PICPARAMS
+{
+    int ForwardRefIdx;      /**< Picture index of forward reference (P/B-frames) */
+    int BackwardRefIdx;     /**< Picture index of backward reference (B-frames) */
+    int FrameWidth;         /**< Actual frame width */
+    int FrameHeight;        /**< Actual frame height */
+    // PICTURE
+    int intra_pic_flag;     /**< Set to 1 for I,BI frames */
+    int ref_pic_flag;       /**< Set to 1 for I,P frames */
+    int progressive_fcm;    /**< Progressive frame */
+    // SEQUENCE
+    int profile;
+    int postprocflag;
+    int pulldown;
+    int interlace;
+    int tfcntrflag;
+    int finterpflag;
+    int psf;
+    int multires;
+    int syncmarker;
+    int rangered;
+    int maxbframes;
+    // ENTRYPOINT
+    int panscan_flag;
+    int refdist_flag;
+    int extended_mv;
+    int dquant;
+    int vstransform;
+    int loopfilter;
+    int fastuvmc;
+    int overlap;
+    int quantizer;
+    int extended_dmv;
+    int range_mapy_flag;
+    int range_mapy;
+    int range_mapuv_flag;
+    int range_mapuv;
+    int rangeredfrm;    // range reduction state
+} CUVIDVC1PICPARAMS;
+
+/*!
+ * \struct CUVIDJPEGPICPARAMS
+ * JPEG Picture Parameters
+ */
+typedef struct _CUVIDJPEGPICPARAMS
+{
+    int Reserved;
+} CUVIDJPEGPICPARAMS;
+
+
+ /*!
+ * \struct CUVIDHEVCPICPARAMS
+ * HEVC Picture Parameters
+ */
+typedef struct _CUVIDHEVCPICPARAMS
+{
+    // sps
+    int pic_width_in_luma_samples;
+    int pic_height_in_luma_samples;
+    unsigned char log2_min_luma_coding_block_size_minus3;
+    unsigned char log2_diff_max_min_luma_coding_block_size;
+    unsigned char log2_min_transform_block_size_minus2;
+    unsigned char log2_diff_max_min_transform_block_size;
+    unsigned char pcm_enabled_flag;
+    unsigned char log2_min_pcm_luma_coding_block_size_minus3;
+    unsigned char log2_diff_max_min_pcm_luma_coding_block_size;
+    unsigned char pcm_sample_bit_depth_luma_minus1;
+
+    unsigned char pcm_sample_bit_depth_chroma_minus1;
+    unsigned char pcm_loop_filter_disabled_flag;
+    unsigned char strong_intra_smoothing_enabled_flag;
+    unsigned char max_transform_hierarchy_depth_intra;
+    unsigned char max_transform_hierarchy_depth_inter;
+    unsigned char amp_enabled_flag;
+    unsigned char separate_colour_plane_flag;
+    unsigned char log2_max_pic_order_cnt_lsb_minus4;
+
+    unsigned char num_short_term_ref_pic_sets;
+    unsigned char long_term_ref_pics_present_flag;
+    unsigned char num_long_term_ref_pics_sps;
+    unsigned char sps_temporal_mvp_enabled_flag;
+    unsigned char sample_adaptive_offset_enabled_flag;
+    unsigned char scaling_list_enable_flag;
+    unsigned char IrapPicFlag;
+    unsigned char IdrPicFlag;
+
+    unsigned char bit_depth_luma_minus8;
+    unsigned char bit_depth_chroma_minus8;
+    unsigned char reserved1[14];
+
+    // pps
+    unsigned char dependent_slice_segments_enabled_flag;
+    unsigned char slice_segment_header_extension_present_flag;
+    unsigned char sign_data_hiding_enabled_flag;
+    unsigned char cu_qp_delta_enabled_flag;
+    unsigned char diff_cu_qp_delta_depth;
+    signed char init_qp_minus26;
+    signed char pps_cb_qp_offset;
+    signed char pps_cr_qp_offset;
+
+    unsigned char constrained_intra_pred_flag;
+    unsigned char weighted_pred_flag;
+    unsigned char weighted_bipred_flag;
+    unsigned char transform_skip_enabled_flag;
+    unsigned char transquant_bypass_enabled_flag;
+    unsigned char entropy_coding_sync_enabled_flag;
+    unsigned char log2_parallel_merge_level_minus2;
+    unsigned char num_extra_slice_header_bits;
+
+    unsigned char loop_filter_across_tiles_enabled_flag;
+    unsigned char loop_filter_across_slices_enabled_flag;
+    unsigned char output_flag_present_flag;
+    unsigned char num_ref_idx_l0_default_active_minus1;
+    unsigned char num_ref_idx_l1_default_active_minus1;
+    unsigned char lists_modification_present_flag;
+    unsigned char cabac_init_present_flag;
+    unsigned char pps_slice_chroma_qp_offsets_present_flag;
+
+    unsigned char deblocking_filter_override_enabled_flag;
+    unsigned char pps_deblocking_filter_disabled_flag;
+    signed char pps_beta_offset_div2;
+    signed char pps_tc_offset_div2;
+    unsigned char tiles_enabled_flag;
+    unsigned char uniform_spacing_flag;
+    unsigned char num_tile_columns_minus1;
+    unsigned char num_tile_rows_minus1;
+
+    unsigned short column_width_minus1[21];
+    unsigned short row_height_minus1[21];
+    unsigned int reserved3[15];
+
+    // RefPicSets
+    int NumBitsForShortTermRPSInSlice;
+    int NumDeltaPocsOfRefRpsIdx;
+    int NumPocTotalCurr;
+    int NumPocStCurrBefore;
+    int NumPocStCurrAfter;
+    int NumPocLtCurr;
+    int CurrPicOrderCntVal;
+    int RefPicIdx[16];                  // [refpic] Indices of valid reference pictures (-1 if unused for reference)
+    int PicOrderCntVal[16];             // [refpic]
+    unsigned char IsLongTerm[16];       // [refpic] 0=not a long-term reference, 1=long-term reference
+    unsigned char RefPicSetStCurrBefore[8]; // [0..NumPocStCurrBefore-1] -> refpic (0..15)
+    unsigned char RefPicSetStCurrAfter[8];  // [0..NumPocStCurrAfter-1] -> refpic (0..15)
+    unsigned char RefPicSetLtCurr[8];       // [0..NumPocLtCurr-1] -> refpic (0..15)
+    unsigned char RefPicSetInterLayer0[8];
+    unsigned char RefPicSetInterLayer1[8];
+    unsigned int reserved4[12];
+
+    // scaling lists (diag order)
+    unsigned char ScalingList4x4[6][16];       // [matrixId][i]
+    unsigned char ScalingList8x8[6][64];       // [matrixId][i]
+    unsigned char ScalingList16x16[6][64];     // [matrixId][i]
+    unsigned char ScalingList32x32[2][64];     // [matrixId][i]
+    unsigned char ScalingListDCCoeff16x16[6];  // [matrixId]
+    unsigned char ScalingListDCCoeff32x32[2];  // [matrixId]
+} CUVIDHEVCPICPARAMS;
+
+
+/*!
+ * \struct CUVIDVP8PICPARAMS
+ * VP8 Picture Parameters
+ */
+typedef struct _CUVIDVP8PICPARAMS
+{
+    int width;
+    int height;
+    unsigned int first_partition_size;
+    //Frame Indexes
+    unsigned char LastRefIdx;
+    unsigned char GoldenRefIdx;
+    unsigned char AltRefIdx;
+    union {
+        struct {
+            unsigned char frame_type : 1;    /**< 0 = KEYFRAME, 1 = INTERFRAME  */
+            unsigned char version : 3;
+            unsigned char show_frame : 1;
+            unsigned char update_mb_segmentation_data : 1;    /**< Must be 0 if segmentation is not enabled */
+            unsigned char Reserved2Bits : 2;
+        };
+        unsigned char wFrameTagFlags;
+    };
+    unsigned char Reserved1[4];
+    unsigned int  Reserved2[3];
+} CUVIDVP8PICPARAMS;
+
+/*!
+ * \struct CUVIDVP9PICPARAMS
+ * VP9 Picture Parameters
+ */
+typedef struct _CUVIDVP9PICPARAMS
+{
+    unsigned int width;
+    unsigned int height;
+
+    //Frame Indices
+    unsigned char LastRefIdx;
+    unsigned char GoldenRefIdx;
+    unsigned char AltRefIdx;
+    unsigned char colorSpace;
+
+    unsigned short profile : 3;
+    unsigned short frameContextIdx : 2;
+    unsigned short frameType : 1;
+    unsigned short showFrame : 1;
+    unsigned short errorResilient : 1;
+    unsigned short frameParallelDecoding : 1;
+    unsigned short subSamplingX : 1;
+    unsigned short subSamplingY : 1;
+    unsigned short intraOnly : 1;
+    unsigned short allow_high_precision_mv : 1;
+    unsigned short refreshEntropyProbs : 1;
+    unsigned short reserved2Bits : 2;
+
+    unsigned short reserved16Bits;
+
+    unsigned char  refFrameSignBias[4];
+
+    unsigned char bitDepthMinus8Luma;
+    unsigned char bitDepthMinus8Chroma;
+    unsigned char loopFilterLevel;
+    unsigned char loopFilterSharpness;
+
+    unsigned char modeRefLfEnabled;
+    unsigned char log2_tile_columns;
+    unsigned char log2_tile_rows;
+
+    unsigned char segmentEnabled : 1;
+    unsigned char segmentMapUpdate : 1;
+    unsigned char segmentMapTemporalUpdate : 1;
+    unsigned char segmentFeatureMode : 1;
+    unsigned char reserved4Bits : 4;
+
+
+    unsigned char segmentFeatureEnable[8][4];
+    short segmentFeatureData[8][4];
+    unsigned char mb_segment_tree_probs[7];
+    unsigned char segment_pred_probs[3];
+    unsigned char reservedSegment16Bits[2];
+
+    int qpYAc;
+    int qpYDc;
+    int qpChDc;
+    int qpChAc;
+
+    unsigned int activeRefIdx[3];
+    unsigned int resetFrameContext;
+    unsigned int mcomp_filter_type;
+    unsigned int mbRefLfDelta[4];
+    unsigned int mbModeLfDelta[2];
+    unsigned int frameTagSize;
+    unsigned int offsetToDctParts;
+    unsigned int reserved128Bits[4];
+
+} CUVIDVP9PICPARAMS;
+
+
+/*!
+ * \struct CUVIDPICPARAMS
+ * Picture Parameters for Decoding
+ */
+typedef struct _CUVIDPICPARAMS
+{
+    int PicWidthInMbs;                    /**< Coded Frame Size */
+    int FrameHeightInMbs;                 /**< Coded Frame Height */
+    int CurrPicIdx;                       /**< Output index of the current picture */
+    int field_pic_flag;                   /**< 0=frame picture, 1=field picture */
+    int bottom_field_flag;                /**< 0=top field, 1=bottom field (ignored if field_pic_flag=0) */
+    int second_field;                     /**< Second field of a complementary field pair */
+    // Bitstream data
+    unsigned int nBitstreamDataLen;        /**< Number of bytes in bitstream data buffer */
+    const unsigned char *pBitstreamData;   /**< Ptr to bitstream data for this picture (slice-layer) */
+    unsigned int nNumSlices;               /**< Number of slices in this picture */
+    const unsigned int *pSliceDataOffsets; /**< nNumSlices entries, contains offset of each slice within the bitstream data buffer */
+    int ref_pic_flag;                      /**< This picture is a reference picture */
+    int intra_pic_flag;                    /**< This picture is entirely intra coded */
+    unsigned int Reserved[30];             /**< Reserved for future use */
+    // Codec-specific data
+    union {
+        CUVIDMPEG2PICPARAMS mpeg2;         /**< Also used for MPEG-1 */
+        CUVIDH264PICPARAMS h264;
+        CUVIDVC1PICPARAMS vc1;
+        CUVIDMPEG4PICPARAMS mpeg4;
+        CUVIDJPEGPICPARAMS jpeg;
+        CUVIDHEVCPICPARAMS hevc;
+        CUVIDVP8PICPARAMS vp8;
+        CUVIDVP9PICPARAMS vp9;
+        unsigned int CodecReserved[1024];
+    } CodecSpecific;
+} CUVIDPICPARAMS;
+
+
+/*!
+ * \struct CUVIDPROCPARAMS
+ * Picture Parameters for Postprocessing
+ */
+typedef struct _CUVIDPROCPARAMS
+{
+    int progressive_frame;  /**< Input is progressive (deinterlace_mode will be ignored)  */
+    int second_field;       /**< Output the second field (ignored if deinterlace mode is Weave) */
+    int top_field_first;    /**< Input frame is top field first (1st field is top, 2nd field is bottom) */
+    int unpaired_field;     /**< Input only contains one field (2nd field is invalid) */
+    // The fields below are used for raw YUV input
+    unsigned int reserved_flags;        /**< Reserved for future use (set to zero) */
+    unsigned int reserved_zero;         /**< Reserved (set to zero) */
+    unsigned long long raw_input_dptr;  /**< Input CUdeviceptr for raw YUV extensions */
+    unsigned int raw_input_pitch;       /**< pitch in bytes of raw YUV input (should be aligned appropriately) */
+    unsigned int raw_input_format;      /**< Reserved for future use (set to zero) */
+    unsigned long long raw_output_dptr; /**< Reserved for future use (set to zero) */
+    unsigned int raw_output_pitch;      /**< Reserved for future use (set to zero) */
+    unsigned int Reserved[48];
+    void *Reserved3[3];
+} CUVIDPROCPARAMS;
+
+
+/**
+ *
+ * In order to minimize decode latencies, there should be always at least 2 pictures in the decode
+ * queue at any time, in order to make sure that all decode engines are always busy.
+ *
+ * Overall data flow:
+ *  - cuvidCreateDecoder(...)
+ *  For each picture:
+ *  - cuvidDecodePicture(N)
+ *  - cuvidMapVideoFrame(N-4)
+ *  - do some processing in cuda
+ *  - cuvidUnmapVideoFrame(N-4)
+ *  - cuvidDecodePicture(N+1)
+ *  - cuvidMapVideoFrame(N-3)
+ *    ...
+ *  - cuvidDestroyDecoder(...)
+ *
+ * NOTE:
+ * - When the cuda context is created from a D3D device, the D3D device must also be created
+ *   with the D3DCREATE_MULTITHREADED flag.
+ * - There is a limit to how many pictures can be mapped simultaneously (ulNumOutputSurfaces)
+ * - cuVidDecodePicture may block the calling thread if there are too many pictures pending
+ *   in the decode queue
+ */
+
+/**
+ * \fn CUresult CUDAAPI cuvidCreateDecoder(CUvideodecoder *phDecoder, CUVIDDECODECREATEINFO *pdci)
+ * Create the decoder object
+ */
+typedef CUresult CUDAAPI tcuvidCreateDecoder(CUvideodecoder *phDecoder, CUVIDDECODECREATEINFO *pdci);
+
+/**
+ * \fn CUresult CUDAAPI cuvidDestroyDecoder(CUvideodecoder hDecoder)
+ * Destroy the decoder object
+ */
+typedef CUresult CUDAAPI tcuvidDestroyDecoder(CUvideodecoder hDecoder);
+
+/**
+ * \fn CUresult CUDAAPI cuvidDecodePicture(CUvideodecoder hDecoder, CUVIDPICPARAMS *pPicParams)
+ * Decode a single picture (field or frame)
+ */
+typedef CUresult CUDAAPI tcuvidDecodePicture(CUvideodecoder hDecoder, CUVIDPICPARAMS *pPicParams);
+
+
+#if !defined(__CUVID_DEVPTR64) || defined(__CUVID_INTERNAL)
+/**
+ * \fn CUresult CUDAAPI cuvidMapVideoFrame(CUvideodecoder hDecoder, int nPicIdx, unsigned int *pDevPtr, unsigned int *pPitch, CUVIDPROCPARAMS *pVPP);
+ * Post-process and map a video frame for use in cuda
+ */
+typedef CUresult CUDAAPI tcuvidMapVideoFrame(CUvideodecoder hDecoder, int nPicIdx,
+                                           unsigned int *pDevPtr, unsigned int *pPitch,
+                                           CUVIDPROCPARAMS *pVPP);
+
+/**
+ * \fn CUresult CUDAAPI cuvidUnmapVideoFrame(CUvideodecoder hDecoder, unsigned int DevPtr)
+ * Unmap a previously mapped video frame
+ */
+typedef CUresult CUDAAPI tcuvidUnmapVideoFrame(CUvideodecoder hDecoder, unsigned int DevPtr);
+#endif
+
+#if defined(WIN64) || defined(_WIN64) || defined(__x86_64) || defined(AMD64) || defined(_M_AMD64)
+/**
+ * \fn CUresult CUDAAPI cuvidMapVideoFrame64(CUvideodecoder hDecoder, int nPicIdx, unsigned long long *pDevPtr, unsigned int *pPitch, CUVIDPROCPARAMS *pVPP);
+ * map a video frame
+ */
+typedef CUresult CUDAAPI tcuvidMapVideoFrame64(CUvideodecoder hDecoder, int nPicIdx, unsigned long long *pDevPtr,
+                                             unsigned int *pPitch, CUVIDPROCPARAMS *pVPP);
+
+/**
+ * \fn CUresult CUDAAPI cuvidUnmapVideoFrame64(CUvideodecoder hDecoder, unsigned long long DevPtr);
+ * Unmap a previously mapped video frame
+ */
+typedef CUresult CUDAAPI tcuvidUnmapVideoFrame64(CUvideodecoder hDecoder, unsigned long long DevPtr);
+
+#if defined(__CUVID_DEVPTR64) && !defined(__CUVID_INTERNAL)
+#define tcuvidMapVideoFrame      tcuvidMapVideoFrame64
+#define tcuvidUnmapVideoFrame    tcuvidUnmapVideoFrame64
+#endif
+#endif
+
+
+
+/**
+ *
+ * Context-locking: to facilitate multi-threaded implementations, the following 4 functions
+ * provide a simple mutex-style host synchronization. If a non-NULL context is specified
+ * in CUVIDDECODECREATEINFO, the codec library will acquire the mutex associated with the given
+ * context before making any cuda calls.
+ * A multi-threaded application could create a lock associated with a context handle so that
+ * multiple threads can safely share the same cuda context:
+ *  - use cuCtxPopCurrent immediately after context creation in order to create a 'floating' context
+ *    that can be passed to cuvidCtxLockCreate.
+ *  - When using a floating context, all cuda calls should only be made within a cuvidCtxLock/cuvidCtxUnlock section.
+ *
+ * NOTE: This is a safer alternative to cuCtxPushCurrent and cuCtxPopCurrent, and is not related to video
+ * decoder in any way (implemented as a critical section associated with cuCtx{Push|Pop}Current calls).
+*/
+
+/**
+ * \fn CUresult CUDAAPI cuvidCtxLockCreate(CUvideoctxlock *pLock, CUcontext ctx)
+ */
+typedef CUresult CUDAAPI tcuvidCtxLockCreate(CUvideoctxlock *pLock, CUcontext ctx);
+
+/**
+ * \fn CUresult CUDAAPI cuvidCtxLockDestroy(CUvideoctxlock lck)
+ */
+typedef CUresult CUDAAPI tcuvidCtxLockDestroy(CUvideoctxlock lck);
+
+/**
+ * \fn CUresult CUDAAPI cuvidCtxLock(CUvideoctxlock lck, unsigned int reserved_flags)
+ */
+typedef CUresult CUDAAPI tcuvidCtxLock(CUvideoctxlock lck, unsigned int reserved_flags);
+
+/**
+ * \fn CUresult CUDAAPI cuvidCtxUnlock(CUvideoctxlock lck, unsigned int reserved_flags)
+ */
+typedef CUresult CUDAAPI tcuvidCtxUnlock(CUvideoctxlock lck, unsigned int reserved_flags);
+
+/** @} */  /* End VIDEO_DECODER */
+////////////////////////////////////////////////////////////////////////////////////////////////
+
+// Auto-lock helper for C++ applications
+class CCtxAutoLock
+{
+private:
+    CUvideoctxlock m_ctx;
+public:
+    CCtxAutoLock(CUvideoctxlock ctx);
+    ~CCtxAutoLock();
+};
+
+extern tcuvidCreateDecoder        *cuvidCreateDecoder;
+extern tcuvidDestroyDecoder       *cuvidDestroyDecoder;
+extern tcuvidDecodePicture        *cuvidDecodePicture;
+extern tcuvidMapVideoFrame        *cuvidMapVideoFrame;
+extern tcuvidUnmapVideoFrame      *cuvidUnmapVideoFrame;
+
+#if defined(__x86_64) || defined(AMD64) || defined(_M_AMD64)
+extern tcuvidMapVideoFrame64      *cuvidMapVideoFrame64;
+extern tcuvidUnmapVideoFrame64    *cuvidUnmapVideoFrame64;
+#endif
+
+//extern tcuvidGetVideoFrameSurface *cuvidGetVideoFrameSurface;
+
+extern tcuvidCtxLockCreate        *cuvidCtxLockCreate;
+extern tcuvidCtxLockDestroy       *cuvidCtxLockDestroy;
+extern tcuvidCtxLock              *cuvidCtxLock;
+extern tcuvidCtxUnlock            *cuvidCtxUnlock;
+
+#if defined(__cplusplus)
+}
+
+#endif /* __cplusplus */
+
+#endif // __CUDA_VIDEO_H__
+
diff --git a/webrtc/modules/video_coding/codecs/h264/include/dynlink_nvcuvid.h b/webrtc/modules/video_coding/codecs/h264/include/dynlink_nvcuvid.h
new file mode 100644
index 0000000..228da53
--- /dev/null
+++ b/webrtc/modules/video_coding/codecs/h264/include/dynlink_nvcuvid.h
@@ -0,0 +1,333 @@
+/*
+ * This copyright notice applies to this header file only:
+ *
+ * Copyright (c) 2010-2016 NVIDIA Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person
+ * obtaining a copy of this software and associated documentation
+ * files (the "Software"), to deal in the Software without
+ * restriction, including without limitation the rights to use,
+ * copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the software, and to permit persons to whom the
+ * software is furnished to do so, subject to the following
+ * conditions:
+ *
+ * The above copyright notice and this permission notice shall be
+ * included in all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES
+ * OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT
+ * HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
+ * WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ */
+
+/**
+ * \file nvcuvid.h
+ *   NvCuvid API provides Video Decoding interface to NVIDIA GPU devices.
+ * \date 2015-2015
+ *  This file contains the interface constants, structure definitions and function prototypes.
+ */
+
+#if !defined(__NVCUVID_H__)
+#define __NVCUVID_H__
+
+#include "dynlink_cuviddec.h"
+
+#if defined(__cplusplus)
+extern "C" {
+#endif /* __cplusplus */
+
+/*********************************
+** Initialization
+*********************************/
+CUresult  CUDAAPI cuvidInit(unsigned int Flags);
+    
+////////////////////////////////////////////////////////////////////////////////////////////////
+//
+// High-level helper APIs for video sources
+//
+
+typedef void *CUvideosource;
+typedef void *CUvideoparser;
+typedef long long CUvideotimestamp;
+
+/**
+ * \addtogroup VIDEO_PARSER Video Parser
+ * @{
+ */
+
+/*!
+ * \enum cudaVideoState
+ * Video Source State
+ */
+typedef enum {
+    cudaVideoState_Error   = -1,    /**< Error state (invalid source)  */
+    cudaVideoState_Stopped = 0,     /**< Source is stopped (or reached end-of-stream)  */
+    cudaVideoState_Started = 1      /**< Source is running and delivering data  */
+} cudaVideoState;
+
+/*!
+ * \enum cudaAudioCodec
+ * Audio compression
+ */
+typedef enum {
+    cudaAudioCodec_MPEG1=0,         /**< MPEG-1 Audio  */
+    cudaAudioCodec_MPEG2,           /**< MPEG-2 Audio  */
+    cudaAudioCodec_MP3,             /**< MPEG-1 Layer III Audio  */
+    cudaAudioCodec_AC3,             /**< Dolby Digital (AC3) Audio  */
+    cudaAudioCodec_LPCM             /**< PCM Audio  */
+} cudaAudioCodec;
+
+/*!
+ * \struct CUVIDEOFORMAT
+ * Video format
+ */
+typedef struct
+{
+    cudaVideoCodec codec;                   /**< Compression format  */
+   /**
+    * frame rate = numerator / denominator (for example: 30000/1001)
+    */
+    struct {
+        unsigned int numerator;             /**< frame rate numerator   (0 = unspecified or variable frame rate) */
+        unsigned int denominator;           /**< frame rate denominator (0 = unspecified or variable frame rate) */
+    } frame_rate;
+    unsigned char progressive_sequence;     /**< 0=interlaced, 1=progressive */
+    unsigned char bit_depth_luma_minus8;    /**< high bit depth Luma */
+    unsigned char bit_depth_chroma_minus8;  /**< high bit depth Chroma */
+    unsigned char reserved1;                /**< Reserved for future use */
+    unsigned int coded_width;               /**< coded frame width */
+    unsigned int coded_height;              /**< coded frame height  */
+   /**
+    *   area of the frame that should be displayed
+    * typical example:
+    *   coded_width = 1920, coded_height = 1088
+    *   display_area = { 0,0,1920,1080 }
+    */
+    struct {
+        int left;                           /**< left position of display rect  */
+        int top;                            /**< top position of display rect  */
+        int right;                          /**< right position of display rect  */
+        int bottom;                         /**< bottom position of display rect  */
+    } display_area;
+    cudaVideoChromaFormat chroma_format;    /**<  Chroma format */
+    unsigned int bitrate;                   /**< video bitrate (bps, 0=unknown) */
+   /**
+    * Display Aspect Ratio = x:y (4:3, 16:9, etc)
+    */
+    struct {
+        int x;
+        int y;
+    } display_aspect_ratio;
+    /**
+    * Video Signal Description
+    */
+    struct {
+        unsigned char video_format          : 3;
+        unsigned char video_full_range_flag : 1;
+        unsigned char reserved_zero_bits    : 4;
+        unsigned char color_primaries;
+        unsigned char transfer_characteristics;
+        unsigned char matrix_coefficients;
+    } video_signal_description;
+    unsigned int seqhdr_data_length;          /**< Additional bytes following (CUVIDEOFORMATEX)  */
+} CUVIDEOFORMAT;
+
+/*!
+ * \struct CUVIDEOFORMATEX
+ * Video format including raw sequence header information
+ */
+typedef struct
+{
+    CUVIDEOFORMAT format;
+    unsigned char raw_seqhdr_data[1024];
+} CUVIDEOFORMATEX;
+
+/*!
+ * \struct CUAUDIOFORMAT
+ * Audio Formats
+ */
+typedef struct
+{
+    cudaAudioCodec codec;       /**< Compression format  */
+    unsigned int channels;      /**< number of audio channels */
+    unsigned int samplespersec; /**< sampling frequency */
+    unsigned int bitrate;       /**< For uncompressed, can also be used to determine bits per sample */
+    unsigned int reserved1;     /**< Reserved for future use */
+    unsigned int reserved2;     /**< Reserved for future use */
+} CUAUDIOFORMAT;
+
+
+/*!
+ * \enum CUvideopacketflags
+ * Data packet flags
+ */
+typedef enum {
+    CUVID_PKT_ENDOFSTREAM   = 0x01,   /**< Set when this is the last packet for this stream  */
+    CUVID_PKT_TIMESTAMP     = 0x02,   /**< Timestamp is valid  */
+    CUVID_PKT_DISCONTINUITY = 0x04    /**< Set when a discontinuity has to be signalled  */
+} CUvideopacketflags;
+
+/*!
+ * \struct CUVIDSOURCEDATAPACKET
+ * Data Packet
+ */
+typedef struct _CUVIDSOURCEDATAPACKET
+{
+    unsigned long flags;            /**< Combination of CUVID_PKT_XXX flags */
+    unsigned long payload_size;     /**< number of bytes in the payload (may be zero if EOS flag is set) */
+    const unsigned char *payload;   /**< Pointer to packet payload data (may be NULL if EOS flag is set) */
+    CUvideotimestamp timestamp;     /**< Presentation timestamp (10MHz clock), only valid if CUVID_PKT_TIMESTAMP flag is set */
+} CUVIDSOURCEDATAPACKET;
+
+// Callback for packet delivery
+typedef int (CUDAAPI *PFNVIDSOURCECALLBACK)(void *, CUVIDSOURCEDATAPACKET *);
+
+/*!
+ * \struct CUVIDSOURCEPARAMS
+ * Source Params
+ */
+typedef struct _CUVIDSOURCEPARAMS
+{
+    unsigned int ulClockRate;                   /**< Timestamp units in Hz (0=default=10000000Hz)  */
+    unsigned int uReserved1[7];                 /**< Reserved for future use - set to zero  */
+    void *pUserData;                            /**< Parameter passed in to the data handlers  */
+    PFNVIDSOURCECALLBACK pfnVideoDataHandler;   /**< Called to deliver audio packets  */
+    PFNVIDSOURCECALLBACK pfnAudioDataHandler;   /**< Called to deliver video packets  */
+    void *pvReserved2[8];                       /**< Reserved for future use - set to NULL */
+} CUVIDSOURCEPARAMS;
+
+/*!
+ * \enum CUvideosourceformat_flags
+ * CUvideosourceformat_flags
+ */
+typedef enum {
+    CUVID_FMT_EXTFORMATINFO = 0x100             /**< Return extended format structure (CUVIDEOFORMATEX) */
+} CUvideosourceformat_flags;
+
+#if !defined(__APPLE__)
+/**
+ * \fn CUresult CUDAAPI cuvidCreateVideoSource(CUvideosource *pObj, const char *pszFileName, CUVIDSOURCEPARAMS *pParams)
+ * Create Video Source
+ */
+typedef CUresult CUDAAPI tcuvidCreateVideoSource(CUvideosource *pObj, const char *pszFileName, CUVIDSOURCEPARAMS *pParams);
+
+/**
+ * \fn CUresult CUDAAPI cuvidCreateVideoSourceW(CUvideosource *pObj, const wchar_t *pwszFileName, CUVIDSOURCEPARAMS *pParams)
+ * Create Video Source
+ */
+typedef CUresult CUDAAPI tcuvidCreateVideoSourceW(CUvideosource *pObj, const wchar_t *pwszFileName, CUVIDSOURCEPARAMS *pParams);
+
+/**
+ * \fn CUresult CUDAAPI cuvidDestroyVideoSource(CUvideosource obj)
+ * Destroy Video Source
+ */
+typedef CUresult CUDAAPI tcuvidDestroyVideoSource(CUvideosource obj);
+
+/**
+ * \fn CUresult CUDAAPI cuvidSetVideoSourceState(CUvideosource obj, cudaVideoState state)
+ * Set Video Source state
+ */
+typedef CUresult CUDAAPI tcuvidSetVideoSourceState(CUvideosource obj, cudaVideoState state);
+
+/**
+ * \fn cudaVideoState CUDAAPI cuvidGetVideoSourceState(CUvideosource obj)
+ * Get Video Source state
+ */
+typedef cudaVideoState CUDAAPI tcuvidGetVideoSourceState(CUvideosource obj);
+
+/**
+ * \fn CUresult CUDAAPI cuvidGetSourceVideoFormat(CUvideosource obj, CUVIDEOFORMAT *pvidfmt, unsigned int flags)
+ * Get Video Source Format
+ */
+typedef CUresult CUDAAPI tcuvidGetSourceVideoFormat(CUvideosource obj, CUVIDEOFORMAT *pvidfmt, unsigned int flags);
+
+/**
+ * \fn CUresult CUDAAPI cuvidGetSourceAudioFormat(CUvideosource obj, CUAUDIOFORMAT *paudfmt, unsigned int flags)
+ * Set Video Source state
+ */
+typedef CUresult CUDAAPI tcuvidGetSourceAudioFormat(CUvideosource obj, CUAUDIOFORMAT *paudfmt, unsigned int flags);
+
+#endif
+
+/**
+ * \struct CUVIDPARSERDISPINFO
+ */
+typedef struct _CUVIDPARSERDISPINFO
+{
+    int picture_index;         /**<                 */
+    int progressive_frame;     /**<                 */
+    int top_field_first;       /**<                 */
+    int repeat_first_field;    /**< Number of additional fields (1=ivtc, 2=frame doubling, 4=frame tripling, -1=unpaired field)  */
+    CUvideotimestamp timestamp; /**<     */
+} CUVIDPARSERDISPINFO;
+
+//
+// Parser callbacks
+// The parser will call these synchronously from within cuvidParseVideoData(), whenever a picture is ready to
+// be decoded and/or displayed.
+//
+typedef int (CUDAAPI *PFNVIDSEQUENCECALLBACK)(void *, CUVIDEOFORMAT *);
+typedef int (CUDAAPI *PFNVIDDECODECALLBACK)(void *, CUVIDPICPARAMS *);
+typedef int (CUDAAPI *PFNVIDDISPLAYCALLBACK)(void *, CUVIDPARSERDISPINFO *);
+
+/**
+ * \struct CUVIDPARSERPARAMS
+ */
+typedef struct _CUVIDPARSERPARAMS
+{
+    cudaVideoCodec CodecType;               /**< cudaVideoCodec_XXX  */
+    unsigned int ulMaxNumDecodeSurfaces;    /**< Max # of decode surfaces (parser will cycle through these) */
+    unsigned int ulClockRate;               /**< Timestamp units in Hz (0=default=10000000Hz) */
+    unsigned int ulErrorThreshold;          /**< % Error threshold (0-100) for calling pfnDecodePicture (100=always call pfnDecodePicture even if picture bitstream is fully corrupted) */
+    unsigned int ulMaxDisplayDelay;         /**< Max display queue delay (improves pipelining of decode with display) - 0=no delay (recommended values: 2..4) */
+    unsigned int uReserved1[5];             /**< Reserved for future use - set to 0 */
+    void *pUserData;                        /**< User data for callbacks */
+    PFNVIDSEQUENCECALLBACK pfnSequenceCallback; /**< Called before decoding frames and/or whenever there is a format change */
+    PFNVIDDECODECALLBACK pfnDecodePicture;      /**< Called when a picture is ready to be decoded (decode order) */
+    PFNVIDDISPLAYCALLBACK pfnDisplayPicture;    /**< Called whenever a picture is ready to be displayed (display order)  */
+    void *pvReserved2[7];                       /**< Reserved for future use - set to NULL */
+    CUVIDEOFORMATEX *pExtVideoInfo;             /**< [Optional] sequence header data from system layer */
+} CUVIDPARSERPARAMS;
+
+/**
+ * \fn CUresult CUDAAPI cuvidCreateVideoParser(CUvideoparser *pObj, CUVIDPARSERPARAMS *pParams)
+ */
+typedef CUresult CUDAAPI tcuvidCreateVideoParser(CUvideoparser *pObj, CUVIDPARSERPARAMS *pParams);
+
+/**
+ * \fn CUresult CUDAAPI cuvidParseVideoData(CUvideoparser obj, CUVIDSOURCEDATAPACKET *pPacket)
+ */
+typedef CUresult CUDAAPI tcuvidParseVideoData(CUvideoparser obj, CUVIDSOURCEDATAPACKET *pPacket);
+
+/**
+ * \fn CUresult CUDAAPI cuvidDestroyVideoParser(CUvideoparser obj)
+ */
+typedef CUresult CUDAAPI tcuvidDestroyVideoParser(CUvideoparser obj);
+
+extern tcuvidCreateVideoSource               *cuvidCreateVideoSource;
+extern tcuvidCreateVideoSourceW              *cuvidCreateVideoSourceW;
+extern tcuvidDestroyVideoSource              *cuvidDestroyVideoSource;
+extern tcuvidSetVideoSourceState             *cuvidSetVideoSourceState;
+extern tcuvidGetVideoSourceState             *cuvidGetVideoSourceState;
+extern tcuvidGetSourceVideoFormat            *cuvidGetSourceVideoFormat;
+extern tcuvidGetSourceAudioFormat            *cuvidGetSourceAudioFormat;
+
+extern tcuvidCreateVideoParser               *cuvidCreateVideoParser;
+extern tcuvidParseVideoData                  *cuvidParseVideoData;
+extern tcuvidDestroyVideoParser              *cuvidDestroyVideoParser;
+
+/** @} */  /* END VIDEO_PARSER */
+////////////////////////////////////////////////////////////////////////////////////////////////
+
+#if defined(__cplusplus)
+}
+#endif /* __cplusplus */
+
+#endif // __NVCUVID_H__
+
+
diff --git a/webrtc/modules/video_coding/codecs/h264/include/exception.h b/webrtc/modules/video_coding/codecs/h264/include/exception.h
new file mode 100644
index 0000000..58c74bf
--- /dev/null
+++ b/webrtc/modules/video_coding/codecs/h264/include/exception.h
@@ -0,0 +1,151 @@
+/*
+ * Copyright 1993-2015 NVIDIA Corporation.  All rights reserved.
+ *
+ * Please refer to the NVIDIA end user license agreement (EULA) associated
+ * with this source code for terms and conditions that govern your use of
+ * this software. Any use, reproduction, disclosure, or distribution of
+ * this software and related documentation outside the terms of the EULA
+ * is strictly prohibited.
+ *
+ */
+
+/* CUda UTility Library */
+#ifndef _EXCEPTION_H_
+#define _EXCEPTION_H_
+
+// includes, system
+#include <exception>
+#include <stdexcept>
+#include <iostream>
+#include <stdlib.h>
+
+//! Exception wrapper.
+//! @param Std_Exception Exception out of namespace std for easy typing.
+template<class Std_Exception>
+class Exception : public Std_Exception
+{
+    public:
+
+        //! @brief Static construction interface
+        //! @return Alwayss throws ( Located_Exception<Exception>)
+        //! @param file file in which the Exception occurs
+        //! @param line line in which the Exception occurs
+        //! @param detailed details on the code fragment causing the Exception
+        static void throw_it(const char *file,
+                             const int line,
+                             const char *detailed = "-");
+
+        //! Static construction interface
+        //! @return Alwayss throws ( Located_Exception<Exception>)
+        //! @param file file in which the Exception occurs
+        //! @param line line in which the Exception occurs
+        //! @param detailed details on the code fragment causing the Exception
+        static void throw_it(const char *file,
+                             const int line,
+                             const std::string &detailed);
+
+        //! Destructor
+        virtual ~Exception() throw();
+
+    private:
+
+        //! Constructor, default (private)
+        Exception();
+
+        //! Constructor, standard
+        //! @param str string returned by what()
+        Exception(const std::string &str);
+
+};
+
+////////////////////////////////////////////////////////////////////////////////
+//! Exception handler function for arbitrary exceptions
+//! @param ex exception to handle
+////////////////////////////////////////////////////////////////////////////////
+template<class Exception_Typ>
+inline void
+handleException(const Exception_Typ &ex)
+{
+    std::cerr << ex.what() << std::endl;
+
+    exit(EXIT_FAILURE);
+}
+
+//! Convenience macros
+
+//! Exception caused by dynamic program behavior, e.g. file does not exist
+#define RUNTIME_EXCEPTION( msg) \
+    Exception<std::runtime_error>::throw_it( __FILE__, __LINE__, msg)
+
+//! Logic exception in program, e.g. an assert failed
+#define LOGIC_EXCEPTION( msg) \
+    Exception<std::logic_error>::throw_it( __FILE__, __LINE__, msg)
+
+//! Out of range exception
+#define RANGE_EXCEPTION( msg) \
+    Exception<std::range_error>::throw_it( __FILE__, __LINE__, msg)
+
+////////////////////////////////////////////////////////////////////////////////
+//! Implementation
+
+// includes, system
+#include <sstream>
+
+////////////////////////////////////////////////////////////////////////////////
+//! Static construction interface.
+//! @param  Exception causing code fragment (file and line) and detailed infos.
+////////////////////////////////////////////////////////////////////////////////
+/*static*/ template<class Std_Exception>
+void
+Exception<Std_Exception>::
+throw_it(const char *file, const int line, const char *detailed)
+{
+    std::stringstream s;
+
+    // Quiet heavy-weight but exceptions are not for
+    // performance / release versions
+    s << "Exception in file '" << file << "' in line " << line << "\n"
+      << "Detailed description: " << detailed << "\n";
+
+    throw Exception(s.str());
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Static construction interface.
+//! @param  Exception causing code fragment (file and line) and detailed infos.
+////////////////////////////////////////////////////////////////////////////////
+/*static*/ template<class Std_Exception>
+void
+Exception<Std_Exception>::
+throw_it(const char *file, const int line, const std::string &msg)
+{
+    throw_it(file, line, msg.c_str());
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Constructor, default (private).
+////////////////////////////////////////////////////////////////////////////////
+template<class Std_Exception>
+Exception<Std_Exception>::Exception() :
+    Exception("Unknown Exception.\n")
+{ }
+
+////////////////////////////////////////////////////////////////////////////////
+//! Constructor, standard (private).
+//! String returned by what().
+////////////////////////////////////////////////////////////////////////////////
+template<class Std_Exception>
+Exception<Std_Exception>::Exception(const std::string &s) :
+    Std_Exception(s)
+{ }
+
+////////////////////////////////////////////////////////////////////////////////
+//! Destructor
+////////////////////////////////////////////////////////////////////////////////
+template<class Std_Exception>
+Exception<Std_Exception>::~Exception() throw() { }
+
+// functions, exported
+
+#endif // #ifndef _EXCEPTION_H_
+
diff --git a/webrtc/modules/video_coding/codecs/h264/include/helper_cuda.h b/webrtc/modules/video_coding/codecs/h264/include/helper_cuda.h
new file mode 100644
index 0000000..71afa1a
--- /dev/null
+++ b/webrtc/modules/video_coding/codecs/h264/include/helper_cuda.h
@@ -0,0 +1,966 @@
+/*
+ * Copyright 1993-2015 NVIDIA Corporation.  All rights reserved.
+ *
+ * Please refer to the NVIDIA end user license agreement (EULA) associated
+ * with this source code for terms and conditions that govern your use of
+ * this software. Any use, reproduction, disclosure, or distribution of
+ * this software and related documentation outside the terms of the EULA
+ * is strictly prohibited.
+ *
+ */
+
+////////////////////////////////////////////////////////////////////////////////
+// These are CUDA Helper functions for initialization and error checking
+
+#ifndef HELPER_CUDA_H
+#define HELPER_CUDA_H
+
+#pragma once
+
+#include <stdlib.h>
+#include <stdio.h>
+#include <string.h>
+
+#include <helper_string.h>
+
+#ifndef EXIT_WAIVED
+#define EXIT_WAIVED 2
+#endif
+
+// Note, it is required that your SDK sample to include the proper header files, please
+// refer the CUDA examples for examples of the needed CUDA headers, which may change depending
+// on which CUDA functions are used.
+
+// CUDA Runtime error messages
+#ifdef __DRIVER_TYPES_H__
+static const char *_cudaGetErrorEnum(cudaError_t error)
+{
+    switch (error)
+    {
+        case cudaSuccess:
+            return "cudaSuccess";
+
+        case cudaErrorMissingConfiguration:
+            return "cudaErrorMissingConfiguration";
+
+        case cudaErrorMemoryAllocation:
+            return "cudaErrorMemoryAllocation";
+
+        case cudaErrorInitializationError:
+            return "cudaErrorInitializationError";
+
+        case cudaErrorLaunchFailure:
+            return "cudaErrorLaunchFailure";
+
+        case cudaErrorPriorLaunchFailure:
+            return "cudaErrorPriorLaunchFailure";
+
+        case cudaErrorLaunchTimeout:
+            return "cudaErrorLaunchTimeout";
+
+        case cudaErrorLaunchOutOfResources:
+            return "cudaErrorLaunchOutOfResources";
+
+        case cudaErrorInvalidDeviceFunction:
+            return "cudaErrorInvalidDeviceFunction";
+
+        case cudaErrorInvalidConfiguration:
+            return "cudaErrorInvalidConfiguration";
+
+        case cudaErrorInvalidDevice:
+            return "cudaErrorInvalidDevice";
+
+        case cudaErrorInvalidValue:
+            return "cudaErrorInvalidValue";
+
+        case cudaErrorInvalidPitchValue:
+            return "cudaErrorInvalidPitchValue";
+
+        case cudaErrorInvalidSymbol:
+            return "cudaErrorInvalidSymbol";
+
+        case cudaErrorMapBufferObjectFailed:
+            return "cudaErrorMapBufferObjectFailed";
+
+        case cudaErrorUnmapBufferObjectFailed:
+            return "cudaErrorUnmapBufferObjectFailed";
+
+        case cudaErrorInvalidHostPointer:
+            return "cudaErrorInvalidHostPointer";
+
+        case cudaErrorInvalidDevicePointer:
+            return "cudaErrorInvalidDevicePointer";
+
+        case cudaErrorInvalidTexture:
+            return "cudaErrorInvalidTexture";
+
+        case cudaErrorInvalidTextureBinding:
+            return "cudaErrorInvalidTextureBinding";
+
+        case cudaErrorInvalidChannelDescriptor:
+            return "cudaErrorInvalidChannelDescriptor";
+
+        case cudaErrorInvalidMemcpyDirection:
+            return "cudaErrorInvalidMemcpyDirection";
+
+        case cudaErrorAddressOfConstant:
+            return "cudaErrorAddressOfConstant";
+
+        case cudaErrorTextureFetchFailed:
+            return "cudaErrorTextureFetchFailed";
+
+        case cudaErrorTextureNotBound:
+            return "cudaErrorTextureNotBound";
+
+        case cudaErrorSynchronizationError:
+            return "cudaErrorSynchronizationError";
+
+        case cudaErrorInvalidFilterSetting:
+            return "cudaErrorInvalidFilterSetting";
+
+        case cudaErrorInvalidNormSetting:
+            return "cudaErrorInvalidNormSetting";
+
+        case cudaErrorMixedDeviceExecution:
+            return "cudaErrorMixedDeviceExecution";
+
+        case cudaErrorCudartUnloading:
+            return "cudaErrorCudartUnloading";
+
+        case cudaErrorUnknown:
+            return "cudaErrorUnknown";
+
+        case cudaErrorNotYetImplemented:
+            return "cudaErrorNotYetImplemented";
+
+        case cudaErrorMemoryValueTooLarge:
+            return "cudaErrorMemoryValueTooLarge";
+
+        case cudaErrorInvalidResourceHandle:
+            return "cudaErrorInvalidResourceHandle";
+
+        case cudaErrorNotReady:
+            return "cudaErrorNotReady";
+
+        case cudaErrorInsufficientDriver:
+            return "cudaErrorInsufficientDriver";
+
+        case cudaErrorSetOnActiveProcess:
+            return "cudaErrorSetOnActiveProcess";
+
+        case cudaErrorInvalidSurface:
+            return "cudaErrorInvalidSurface";
+
+        case cudaErrorNoDevice:
+            return "cudaErrorNoDevice";
+
+        case cudaErrorECCUncorrectable:
+            return "cudaErrorECCUncorrectable";
+
+        case cudaErrorSharedObjectSymbolNotFound:
+            return "cudaErrorSharedObjectSymbolNotFound";
+
+        case cudaErrorSharedObjectInitFailed:
+            return "cudaErrorSharedObjectInitFailed";
+
+        case cudaErrorUnsupportedLimit:
+            return "cudaErrorUnsupportedLimit";
+
+        case cudaErrorDuplicateVariableName:
+            return "cudaErrorDuplicateVariableName";
+
+        case cudaErrorDuplicateTextureName:
+            return "cudaErrorDuplicateTextureName";
+
+        case cudaErrorDuplicateSurfaceName:
+            return "cudaErrorDuplicateSurfaceName";
+
+        case cudaErrorDevicesUnavailable:
+            return "cudaErrorDevicesUnavailable";
+
+        case cudaErrorInvalidKernelImage:
+            return "cudaErrorInvalidKernelImage";
+
+        case cudaErrorNoKernelImageForDevice:
+            return "cudaErrorNoKernelImageForDevice";
+
+        case cudaErrorIncompatibleDriverContext:
+            return "cudaErrorIncompatibleDriverContext";
+
+        case cudaErrorPeerAccessAlreadyEnabled:
+            return "cudaErrorPeerAccessAlreadyEnabled";
+
+        case cudaErrorPeerAccessNotEnabled:
+            return "cudaErrorPeerAccessNotEnabled";
+
+        case cudaErrorDeviceAlreadyInUse:
+            return "cudaErrorDeviceAlreadyInUse";
+
+        case cudaErrorProfilerDisabled:
+            return "cudaErrorProfilerDisabled";
+
+        case cudaErrorProfilerNotInitialized:
+            return "cudaErrorProfilerNotInitialized";
+
+        case cudaErrorProfilerAlreadyStarted:
+            return "cudaErrorProfilerAlreadyStarted";
+
+        case cudaErrorProfilerAlreadyStopped:
+            return "cudaErrorProfilerAlreadyStopped";
+
+#if __CUDA_API_VERSION >= 0x4000
+
+        case cudaErrorAssert:
+            return "cudaErrorAssert";
+
+        case cudaErrorTooManyPeers:
+            return "cudaErrorTooManyPeers";
+
+        case cudaErrorHostMemoryAlreadyRegistered:
+            return "cudaErrorHostMemoryAlreadyRegistered";
+
+        case cudaErrorHostMemoryNotRegistered:
+            return "cudaErrorHostMemoryNotRegistered";
+#endif
+
+        case cudaErrorStartupFailure:
+            return "cudaErrorStartupFailure";
+
+        case cudaErrorApiFailureBase:
+            return "cudaErrorApiFailureBase";
+    }
+
+    return "<unknown>";
+}
+#endif
+
+#ifdef __cuda_cuda_h__
+// CUDA Driver API errors
+static const char *_cudaGetErrorEnum(CUresult error)
+{
+    switch (error)
+    {
+        case CUDA_SUCCESS:
+            return "CUDA_SUCCESS";
+
+        case CUDA_ERROR_INVALID_VALUE:
+            return "CUDA_ERROR_INVALID_VALUE";
+
+        case CUDA_ERROR_OUT_OF_MEMORY:
+            return "CUDA_ERROR_OUT_OF_MEMORY";
+
+        case CUDA_ERROR_NOT_INITIALIZED:
+            return "CUDA_ERROR_NOT_INITIALIZED";
+
+        case CUDA_ERROR_DEINITIALIZED:
+            return "CUDA_ERROR_DEINITIALIZED";
+
+        case CUDA_ERROR_PROFILER_DISABLED:
+            return "CUDA_ERROR_PROFILER_DISABLED";
+
+        case CUDA_ERROR_PROFILER_NOT_INITIALIZED:
+            return "CUDA_ERROR_PROFILER_NOT_INITIALIZED";
+
+        case CUDA_ERROR_PROFILER_ALREADY_STARTED:
+            return "CUDA_ERROR_PROFILER_ALREADY_STARTED";
+
+        case CUDA_ERROR_PROFILER_ALREADY_STOPPED:
+            return "CUDA_ERROR_PROFILER_ALREADY_STOPPED";
+
+        case CUDA_ERROR_NO_DEVICE:
+            return "CUDA_ERROR_NO_DEVICE";
+
+        case CUDA_ERROR_INVALID_DEVICE:
+            return "CUDA_ERROR_INVALID_DEVICE";
+
+        case CUDA_ERROR_INVALID_IMAGE:
+            return "CUDA_ERROR_INVALID_IMAGE";
+
+        case CUDA_ERROR_INVALID_CONTEXT:
+            return "CUDA_ERROR_INVALID_CONTEXT";
+
+        case CUDA_ERROR_CONTEXT_ALREADY_CURRENT:
+            return "CUDA_ERROR_CONTEXT_ALREADY_CURRENT";
+
+        case CUDA_ERROR_MAP_FAILED:
+            return "CUDA_ERROR_MAP_FAILED";
+
+        case CUDA_ERROR_UNMAP_FAILED:
+            return "CUDA_ERROR_UNMAP_FAILED";
+
+        case CUDA_ERROR_ARRAY_IS_MAPPED:
+            return "CUDA_ERROR_ARRAY_IS_MAPPED";
+
+        case CUDA_ERROR_ALREADY_MAPPED:
+            return "CUDA_ERROR_ALREADY_MAPPED";
+
+        case CUDA_ERROR_NO_BINARY_FOR_GPU:
+            return "CUDA_ERROR_NO_BINARY_FOR_GPU";
+
+        case CUDA_ERROR_ALREADY_ACQUIRED:
+            return "CUDA_ERROR_ALREADY_ACQUIRED";
+
+        case CUDA_ERROR_NOT_MAPPED:
+            return "CUDA_ERROR_NOT_MAPPED";
+
+        case CUDA_ERROR_NOT_MAPPED_AS_ARRAY:
+            return "CUDA_ERROR_NOT_MAPPED_AS_ARRAY";
+
+        case CUDA_ERROR_NOT_MAPPED_AS_POINTER:
+            return "CUDA_ERROR_NOT_MAPPED_AS_POINTER";
+
+        case CUDA_ERROR_ECC_UNCORRECTABLE:
+            return "CUDA_ERROR_ECC_UNCORRECTABLE";
+
+        case CUDA_ERROR_UNSUPPORTED_LIMIT:
+            return "CUDA_ERROR_UNSUPPORTED_LIMIT";
+
+        case CUDA_ERROR_CONTEXT_ALREADY_IN_USE:
+            return "CUDA_ERROR_CONTEXT_ALREADY_IN_USE";
+
+        case CUDA_ERROR_INVALID_SOURCE:
+            return "CUDA_ERROR_INVALID_SOURCE";
+
+        case CUDA_ERROR_FILE_NOT_FOUND:
+            return "CUDA_ERROR_FILE_NOT_FOUND";
+
+        case CUDA_ERROR_SHARED_OBJECT_SYMBOL_NOT_FOUND:
+            return "CUDA_ERROR_SHARED_OBJECT_SYMBOL_NOT_FOUND";
+
+        case CUDA_ERROR_SHARED_OBJECT_INIT_FAILED:
+            return "CUDA_ERROR_SHARED_OBJECT_INIT_FAILED";
+
+        case CUDA_ERROR_OPERATING_SYSTEM:
+            return "CUDA_ERROR_OPERATING_SYSTEM";
+
+        case CUDA_ERROR_INVALID_HANDLE:
+            return "CUDA_ERROR_INVALID_HANDLE";
+
+        case CUDA_ERROR_NOT_FOUND:
+            return "CUDA_ERROR_NOT_FOUND";
+
+        case CUDA_ERROR_NOT_READY:
+            return "CUDA_ERROR_NOT_READY";
+
+        case CUDA_ERROR_LAUNCH_FAILED:
+            return "CUDA_ERROR_LAUNCH_FAILED";
+
+        case CUDA_ERROR_LAUNCH_OUT_OF_RESOURCES:
+            return "CUDA_ERROR_LAUNCH_OUT_OF_RESOURCES";
+
+        case CUDA_ERROR_LAUNCH_TIMEOUT:
+            return "CUDA_ERROR_LAUNCH_TIMEOUT";
+
+        case CUDA_ERROR_LAUNCH_INCOMPATIBLE_TEXTURING:
+            return "CUDA_ERROR_LAUNCH_INCOMPATIBLE_TEXTURING";
+
+        case CUDA_ERROR_PEER_ACCESS_ALREADY_ENABLED:
+            return "CUDA_ERROR_PEER_ACCESS_ALREADY_ENABLED";
+
+        case CUDA_ERROR_PEER_ACCESS_NOT_ENABLED:
+            return "CUDA_ERROR_PEER_ACCESS_NOT_ENABLED";
+
+        case CUDA_ERROR_PRIMARY_CONTEXT_ACTIVE:
+            return "CUDA_ERROR_PRIMARY_CONTEXT_ACTIVE";
+
+        case CUDA_ERROR_CONTEXT_IS_DESTROYED:
+            return "CUDA_ERROR_CONTEXT_IS_DESTROYED";
+
+        case CUDA_ERROR_ASSERT:
+            return "CUDA_ERROR_ASSERT";
+
+        case CUDA_ERROR_TOO_MANY_PEERS:
+            return "CUDA_ERROR_TOO_MANY_PEERS";
+
+        case CUDA_ERROR_HOST_MEMORY_ALREADY_REGISTERED:
+            return "CUDA_ERROR_HOST_MEMORY_ALREADY_REGISTERED";
+
+        case CUDA_ERROR_HOST_MEMORY_NOT_REGISTERED:
+            return "CUDA_ERROR_HOST_MEMORY_NOT_REGISTERED";
+
+        case CUDA_ERROR_UNKNOWN:
+            return "CUDA_ERROR_UNKNOWN";
+    }
+
+    return "<unknown>";
+}
+#endif
+
+#ifdef CUBLAS_API_H_
+// cuBLAS API errors
+static const char *_cudaGetErrorEnum(cublasStatus_t error)
+{
+    switch (error)
+    {
+        case CUBLAS_STATUS_SUCCESS:
+            return "CUBLAS_STATUS_SUCCESS";
+
+        case CUBLAS_STATUS_NOT_INITIALIZED:
+            return "CUBLAS_STATUS_NOT_INITIALIZED";
+
+        case CUBLAS_STATUS_ALLOC_FAILED:
+            return "CUBLAS_STATUS_ALLOC_FAILED";
+
+        case CUBLAS_STATUS_INVALID_VALUE:
+            return "CUBLAS_STATUS_INVALID_VALUE";
+
+        case CUBLAS_STATUS_ARCH_MISMATCH:
+            return "CUBLAS_STATUS_ARCH_MISMATCH";
+
+        case CUBLAS_STATUS_MAPPING_ERROR:
+            return "CUBLAS_STATUS_MAPPING_ERROR";
+
+        case CUBLAS_STATUS_EXECUTION_FAILED:
+            return "CUBLAS_STATUS_EXECUTION_FAILED";
+
+        case CUBLAS_STATUS_INTERNAL_ERROR:
+            return "CUBLAS_STATUS_INTERNAL_ERROR";
+    }
+
+    return "<unknown>";
+}
+#endif
+
+#ifdef _CUFFT_H_
+// cuFFT API errors
+static const char *_cudaGetErrorEnum(cufftResult error)
+{
+    switch (error)
+    {
+        case CUFFT_SUCCESS:
+            return "CUFFT_SUCCESS";
+
+        case CUFFT_INVALID_PLAN:
+            return "CUFFT_INVALID_PLAN";
+
+        case CUFFT_ALLOC_FAILED:
+            return "CUFFT_ALLOC_FAILED";
+
+        case CUFFT_INVALID_TYPE:
+            return "CUFFT_INVALID_TYPE";
+
+        case CUFFT_INVALID_VALUE:
+            return "CUFFT_INVALID_VALUE";
+
+        case CUFFT_INTERNAL_ERROR:
+            return "CUFFT_INTERNAL_ERROR";
+
+        case CUFFT_EXEC_FAILED:
+            return "CUFFT_EXEC_FAILED";
+
+        case CUFFT_SETUP_FAILED:
+            return "CUFFT_SETUP_FAILED";
+
+        case CUFFT_INVALID_SIZE:
+            return "CUFFT_INVALID_SIZE";
+
+        case CUFFT_UNALIGNED_DATA:
+            return "CUFFT_UNALIGNED_DATA";
+    }
+
+    return "<unknown>";
+}
+#endif
+
+
+#ifdef CUSPARSEAPI
+// cuSPARSE API errors
+static const char *_cudaGetErrorEnum(cusparseStatus_t error)
+{
+    switch (error)
+    {
+        case CUSPARSE_STATUS_SUCCESS:
+            return "CUSPARSE_STATUS_SUCCESS";
+
+        case CUSPARSE_STATUS_NOT_INITIALIZED:
+            return "CUSPARSE_STATUS_NOT_INITIALIZED";
+
+        case CUSPARSE_STATUS_ALLOC_FAILED:
+            return "CUSPARSE_STATUS_ALLOC_FAILED";
+
+        case CUSPARSE_STATUS_INVALID_VALUE:
+            return "CUSPARSE_STATUS_INVALID_VALUE";
+
+        case CUSPARSE_STATUS_ARCH_MISMATCH:
+            return "CUSPARSE_STATUS_ARCH_MISMATCH";
+
+        case CUSPARSE_STATUS_MAPPING_ERROR:
+            return "CUSPARSE_STATUS_MAPPING_ERROR";
+
+        case CUSPARSE_STATUS_EXECUTION_FAILED:
+            return "CUSPARSE_STATUS_EXECUTION_FAILED";
+
+        case CUSPARSE_STATUS_INTERNAL_ERROR:
+            return "CUSPARSE_STATUS_INTERNAL_ERROR";
+
+        case CUSPARSE_STATUS_MATRIX_TYPE_NOT_SUPPORTED:
+            return "CUSPARSE_STATUS_MATRIX_TYPE_NOT_SUPPORTED";
+    }
+
+    return "<unknown>";
+}
+#endif
+
+#ifdef CURAND_H_
+// cuRAND API errors
+static const char *_cudaGetErrorEnum(curandStatus_t error)
+{
+    switch (error)
+    {
+        case CURAND_STATUS_SUCCESS:
+            return "CURAND_STATUS_SUCCESS";
+
+        case CURAND_STATUS_VERSION_MISMATCH:
+            return "CURAND_STATUS_VERSION_MISMATCH";
+
+        case CURAND_STATUS_NOT_INITIALIZED:
+            return "CURAND_STATUS_NOT_INITIALIZED";
+
+        case CURAND_STATUS_ALLOCATION_FAILED:
+            return "CURAND_STATUS_ALLOCATION_FAILED";
+
+        case CURAND_STATUS_TYPE_ERROR:
+            return "CURAND_STATUS_TYPE_ERROR";
+
+        case CURAND_STATUS_OUT_OF_RANGE:
+            return "CURAND_STATUS_OUT_OF_RANGE";
+
+        case CURAND_STATUS_LENGTH_NOT_MULTIPLE:
+            return "CURAND_STATUS_LENGTH_NOT_MULTIPLE";
+
+        case CURAND_STATUS_DOUBLE_PRECISION_REQUIRED:
+            return "CURAND_STATUS_DOUBLE_PRECISION_REQUIRED";
+
+        case CURAND_STATUS_LAUNCH_FAILURE:
+            return "CURAND_STATUS_LAUNCH_FAILURE";
+
+        case CURAND_STATUS_PREEXISTING_FAILURE:
+            return "CURAND_STATUS_PREEXISTING_FAILURE";
+
+        case CURAND_STATUS_INITIALIZATION_FAILED:
+            return "CURAND_STATUS_INITIALIZATION_FAILED";
+
+        case CURAND_STATUS_ARCH_MISMATCH:
+            return "CURAND_STATUS_ARCH_MISMATCH";
+
+        case CURAND_STATUS_INTERNAL_ERROR:
+            return "CURAND_STATUS_INTERNAL_ERROR";
+    }
+
+    return "<unknown>";
+}
+#endif
+
+#ifdef NV_NPPIDEFS_H
+// NPP API errors
+static const char *_cudaGetErrorEnum(NppStatus error)
+{
+    switch (error)
+    {
+        case NPP_NOT_SUPPORTED_MODE_ERROR:
+            return "NPP_NOT_SUPPORTED_MODE_ERROR";
+
+        case NPP_ROUND_MODE_NOT_SUPPORTED_ERROR:
+            return "NPP_ROUND_MODE_NOT_SUPPORTED_ERROR";
+
+        case NPP_RESIZE_NO_OPERATION_ERROR:
+            return "NPP_RESIZE_NO_OPERATION_ERROR";
+
+        case NPP_NOT_SUFFICIENT_COMPUTE_CAPABILITY:
+            return "NPP_NOT_SUFFICIENT_COMPUTE_CAPABILITY";
+
+#if 0 // __CUDA_API_VERSION <= 0x5000
+        case NPP_BAD_ARG_ERROR:
+            return "NPP_BAD_ARGUMENT_ERROR";
+        case NPP_COEFF_ERROR:
+            return "NPP_COEFFICIENT_ERROR";
+        case NPP_RECT_ERROR:
+            return "NPP_RECTANGLE_ERROR";
+        case NPP_QUAD_ERROR:
+            return "NPP_QUADRANGLE_ERROR";
+        case NPP_MEM_ALLOC_ERR:
+            return "NPP_MEMORY_ALLOCATION_ERROR";
+        case NPP_HISTO_NUMBER_OF_LEVELS_ERROR:
+            return "NPP_HISTOGRAM_NUMBER_OF_LEVELS_ERROR";
+        case NPP_INVALID_INPUT:
+            return "NPP_INVALID_INPUT";
+        case NPP_POINTER_ERROR:
+            return "NPP_POINTER_ERROR";
+#else
+       // These are for CUDA 5.5 or higher
+        case NPP_BAD_ARGUMENT_ERROR:
+            return "NPP_BAD_ARGUMENT_ERROR";
+        case NPP_COEFFICIENT_ERROR:
+            return "NPP_COEFFICIENT_ERROR";
+        case NPP_RECTANGLE_ERROR:
+            return "NPP_RECTANGLE_ERROR";
+        case NPP_QUADRANGLE_ERROR:
+            return "NPP_QUADRANGLE_ERROR";
+        case NPP_MEMORY_ALLOCATION_ERR:
+            return "NPP_MEMORY_ALLOCATION_ERROR";
+        case NPP_HISTOGRAM_NUMBER_OF_LEVELS_ERROR:
+            return "NPP_HISTOGRAM_NUMBER_OF_LEVELS_ERROR";
+        case NPP_INVALID_HOST_POINTER_ERROR:
+            return "NPP_INVALID_HOST_POINTER_ERROR";
+        case NPP_INVALID_DEVICE_POINTER_ERROR:
+            return "NPP_INVALID_DEVICE_POINTER_ERROR";
+
+#endif
+
+        case NPP_LUT_NUMBER_OF_LEVELS_ERROR:
+            return "NPP_LUT_NUMBER_OF_LEVELS_ERROR";
+
+        case NPP_TEXTURE_BIND_ERROR:
+            return "NPP_TEXTURE_BIND_ERROR";
+
+        case NPP_WRONG_INTERSECTION_ROI_ERROR:
+            return "NPP_WRONG_INTERSECTION_ROI_ERROR";
+
+        case NPP_NOT_EVEN_STEP_ERROR:
+            return "NPP_NOT_EVEN_STEP_ERROR";
+
+        case NPP_INTERPOLATION_ERROR:
+            return "NPP_INTERPOLATION_ERROR";
+
+        case NPP_RESIZE_FACTOR_ERROR:
+            return "NPP_RESIZE_FACTOR_ERROR";
+
+        case NPP_HAAR_CLASSIFIER_PIXEL_MATCH_ERROR:
+            return "NPP_HAAR_CLASSIFIER_PIXEL_MATCH_ERROR";
+
+        case NPP_MEMFREE_ERR:
+            return "NPP_MEMFREE_ERR";
+
+        case NPP_MEMSET_ERR:
+            return "NPP_MEMSET_ERR";
+
+        case NPP_MEMCPY_ERROR:
+            return "NPP_MEMCPY_ERROR";
+
+        case NPP_MIRROR_FLIP_ERR:
+            return "NPP_MIRROR_FLIP_ERR";
+
+        case NPP_ALIGNMENT_ERROR:
+            return "NPP_ALIGNMENT_ERROR";
+
+        case NPP_STEP_ERROR:
+            return "NPP_STEP_ERROR";
+
+        case NPP_SIZE_ERROR:
+            return "NPP_SIZE_ERROR";
+
+        case NPP_NULL_POINTER_ERROR:
+            return "NPP_NULL_POINTER_ERROR";
+
+        case NPP_CUDA_KERNEL_EXECUTION_ERROR:
+            return "NPP_CUDA_KERNEL_EXECUTION_ERROR";
+
+        case NPP_NOT_IMPLEMENTED_ERROR:
+            return "NPP_NOT_IMPLEMENTED_ERROR";
+
+        case NPP_ERROR:
+            return "NPP_ERROR";
+
+        case NPP_SUCCESS:
+            return "NPP_SUCCESS";
+
+        case NPP_WARNING:
+            return "NPP_WARNING";
+
+        case NPP_WRONG_INTERSECTION_QUAD_WARNING:
+            return "NPP_WRONG_INTERSECTION_QUAD_WARNING";
+
+        case NPP_MISALIGNED_DST_ROI_WARNING:
+            return "NPP_MISALIGNED_DST_ROI_WARNING";
+
+        case NPP_AFFINE_QUAD_INCORRECT_WARNING:
+            return "NPP_AFFINE_QUAD_INCORRECT_WARNING";
+
+        case NPP_DOUBLE_SIZE_WARNING:
+            return "NPP_DOUBLE_SIZE_WARNING";
+
+        case NPP_ODD_ROI_WARNING:
+            return "NPP_ODD_ROI_WARNING";
+
+        case NPP_WRONG_INTERSECTION_ROI_WARNING:
+            return "NPP_WRONG_INTERSECTION_ROI_WARNING";
+    }
+
+    return "<unknown>";
+}
+#endif
+
+template< typename T >
+void check(T result, char const *const func, const char *const file, int const line)
+{
+    if (result)
+    {
+        fprintf(stderr, "CUDA error at %s:%d code=%d(%s) \"%s\" \n",
+                file, line, static_cast<unsigned int>(result), _cudaGetErrorEnum(result), func);
+        exit(EXIT_FAILURE);
+    }
+}
+
+#ifdef __DRIVER_TYPES_H__
+// This will output the proper CUDA error strings in the event that a CUDA host call returns an error
+#define checkCudaErrors(val)           check ( (val), #val, __FILE__, __LINE__ )
+
+// This will output the proper error string when calling cudaGetLastError
+#define getLastCudaError(msg)      __getLastCudaError (msg, __FILE__, __LINE__)
+
+inline void __getLastCudaError(const char *errorMessage, const char *file, const int line)
+{
+    cudaError_t err = cudaGetLastError();
+
+    if (cudaSuccess != err)
+    {
+        fprintf(stderr, "%s(%i) : getLastCudaError() CUDA error : %s : (%d) %s.\n",
+                file, line, errorMessage, (int)err, cudaGetErrorString(err));
+        exit(EXIT_FAILURE);
+    }
+}
+#endif
+
+#ifndef MAX
+#define MAX(a,b) (a > b ? a : b)
+#endif
+
+// Beginning of GPU Architecture definitions
+inline int _ConvertSMVer2Cores(int major, int minor)
+{
+    // Defines for GPU Architecture types (using the SM version to determine the # of cores per SM
+    typedef struct
+    {
+        int SM; // 0xMm (hexidecimal notation), M = SM Major version, and m = SM minor version
+        int Cores;
+    } sSMtoCores;
+
+    sSMtoCores nGpuArchCoresPerSM[] =
+    {
+        { 0x10,  8 }, // Tesla Generation (SM 1.0) G80 class
+        { 0x11,  8 }, // Tesla Generation (SM 1.1) G8x class
+        { 0x12,  8 }, // Tesla Generation (SM 1.2) G9x class
+        { 0x13,  8 }, // Tesla Generation (SM 1.3) GT200 class
+        { 0x20, 32 }, // Fermi Generation (SM 2.0) GF100 class
+        { 0x21, 48 }, // Fermi Generation (SM 2.1) GF10x class
+        { 0x30, 192}, // Kepler Generation (SM 3.0) GK10x class
+        { 0x35, 192}, // Kepler Generation (SM 3.5) GK11x class
+        {   -1, -1 }
+    };
+
+    int index = 0;
+
+    while (nGpuArchCoresPerSM[index].SM != -1)
+    {
+        if (nGpuArchCoresPerSM[index].SM == ((major << 4) + minor))
+        {
+            return nGpuArchCoresPerSM[index].Cores;
+        }
+
+        index++;
+    }
+
+    // If we don't find the values, we default use the previous one to run properly
+    printf("MapSMtoCores for SM %d.%d is undefined.  Default to use %d Cores/SM\n", major, minor, nGpuArchCoresPerSM[7].Cores);
+    return nGpuArchCoresPerSM[7].Cores;
+}
+// end of GPU Architecture definitions
+
+#ifdef __CUDA_RUNTIME_H__
+// General GPU Device CUDA Initialization
+inline int gpuDeviceInit(int devID)
+{
+    int device_count;
+    checkCudaErrors(cudaGetDeviceCount(&device_count));
+
+    if (device_count == 0)
+    {
+        fprintf(stderr, "gpuDeviceInit() CUDA error: no devices supporting CUDA.\n");
+        exit(EXIT_FAILURE);
+    }
+
+    if (devID < 0)
+    {
+        devID = 0;
+    }
+
+    if (devID > device_count-1)
+    {
+        fprintf(stderr, "\n");
+        fprintf(stderr, ">> %d CUDA capable GPU device(s) detected. <<\n", device_count);
+        fprintf(stderr, ">> gpuDeviceInit (-device=%d) is not a valid GPU device. <<\n", devID);
+        fprintf(stderr, "\n");
+        return -devID;
+    }
+
+    cudaDeviceProp deviceProp;
+    checkCudaErrors(cudaGetDeviceProperties(&deviceProp, devID));
+
+    if (deviceProp.computeMode == cudaComputeModeProhibited)
+    {
+        fprintf(stderr, "Error: device is running in <Compute Mode Prohibited>, no threads can use ::cudaSetDevice().\n");
+        return -1;
+    }
+
+    if (deviceProp.major < 1)
+    {
+        fprintf(stderr, "gpuDeviceInit(): GPU device does not support CUDA.\n");
+        exit(EXIT_FAILURE);
+    }
+
+    checkCudaErrors(cudaSetDevice(devID));
+    printf("gpuDeviceInit() CUDA Device [%d]: \"%s\n", devID, deviceProp.name);
+
+    return devID;
+}
+
+// This function returns the best GPU (with maximum GFLOPS)
+inline int gpuGetMaxGflopsDeviceId()
+{
+    int current_device     = 0, sm_per_multiproc  = 0;
+    int max_compute_perf   = 0, max_perf_device   = 0;
+    int device_count       = 0, best_SM_arch      = 0;
+    cudaDeviceProp deviceProp;
+    cudaGetDeviceCount(&device_count);
+
+    checkCudaErrors(cudaGetDeviceCount(&device_count));
+
+    if (device_count == 0)
+    {
+        fprintf(stderr, "gpuGetMaxGflopsDeviceId() CUDA error: no devices supporting CUDA.\n");
+        exit(EXIT_FAILURE);
+    }
+
+    // Find the best major SM Architecture GPU device
+    while (current_device < device_count)
+    {
+        cudaGetDeviceProperties(&deviceProp, current_device);
+
+        // If this GPU is not running on Compute Mode prohibited, then we can add it to the list
+        if (deviceProp.computeMode != cudaComputeModeProhibited)
+        {
+            if (deviceProp.major > 0 && deviceProp.major < 9999)
+            {
+                best_SM_arch = MAX(best_SM_arch, deviceProp.major);
+            }
+        }
+
+        current_device++;
+    }
+
+    // Find the best CUDA capable GPU device
+    current_device = 0;
+
+    while (current_device < device_count)
+    {
+        cudaGetDeviceProperties(&deviceProp, current_device);
+
+        // If this GPU is not running on Compute Mode prohibited, then we can add it to the list
+        if (deviceProp.computeMode != cudaComputeModeProhibited)
+        {
+            if (deviceProp.major == 9999 && deviceProp.minor == 9999)
+            {
+                sm_per_multiproc = 1;
+            }
+            else
+            {
+                sm_per_multiproc = _ConvertSMVer2Cores(deviceProp.major, deviceProp.minor);
+            }
+
+            int compute_perf  = deviceProp.multiProcessorCount * sm_per_multiproc * deviceProp.clockRate;
+
+            if (compute_perf  > max_compute_perf)
+            {
+                // If we find GPU with SM major > 2, search only these
+                if (best_SM_arch > 2)
+                {
+                    // If our device==dest_SM_arch, choose this, or else pass
+                    if (deviceProp.major == best_SM_arch)
+                    {
+                        max_compute_perf  = compute_perf;
+                        max_perf_device   = current_device;
+                    }
+                }
+                else
+                {
+                    max_compute_perf  = compute_perf;
+                    max_perf_device   = current_device;
+                }
+            }
+        }
+
+        ++current_device;
+    }
+
+    return max_perf_device;
+}
+
+
+// Initialization code to find the best CUDA Device
+inline int findCudaDevice(int argc, const char **argv)
+{
+    cudaDeviceProp deviceProp;
+    int devID = 0;
+
+    // If the command-line has a device number specified, use it
+    if (checkCmdLineFlag(argc, argv, "device"))
+    {
+        devID = getCmdLineArgumentInt(argc, argv, "device=");
+
+        if (devID < 0)
+        {
+            printf("Invalid command line parameter\n ");
+            exit(EXIT_FAILURE);
+        }
+        else
+        {
+            devID = gpuDeviceInit(devID);
+
+            if (devID < 0)
+            {
+                printf("exiting...\n");
+                exit(EXIT_FAILURE);
+            }
+        }
+    }
+    else
+    {
+        // Otherwise pick the device with highest Gflops/s
+        devID = gpuGetMaxGflopsDeviceId();
+        checkCudaErrors(cudaSetDevice(devID));
+        checkCudaErrors(cudaGetDeviceProperties(&deviceProp, devID));
+        printf("GPU Device %d: \"%s\" with compute capability %d.%d\n\n", devID, deviceProp.name, deviceProp.major, deviceProp.minor);
+    }
+
+    return devID;
+}
+
+// General check for CUDA GPU SM Capabilities
+inline bool checkCudaCapabilities(int major_version, int minor_version)
+{
+    cudaDeviceProp deviceProp;
+    deviceProp.major = 0;
+    deviceProp.minor = 0;
+    int dev;
+
+    checkCudaErrors(cudaGetDevice(&dev));
+    checkCudaErrors(cudaGetDeviceProperties(&deviceProp, dev));
+
+    if ((deviceProp.major > major_version) ||
+        (deviceProp.major == major_version && deviceProp.minor >= minor_version))
+    {
+        printf("> Device %d: <%16s >, Compute SM %d.%d detected\n", dev, deviceProp.name, deviceProp.major, deviceProp.minor);
+        return true;
+    }
+    else
+    {
+        printf("No GPU device was found that can support CUDA compute capability %d.%d.\n", major_version, minor_version);
+        return false;
+    }
+}
+#endif
+
+// end of CUDA Helper Functions
+
+
+#endif
diff --git a/webrtc/modules/video_coding/codecs/h264/include/helper_cuda_drvapi.h b/webrtc/modules/video_coding/codecs/h264/include/helper_cuda_drvapi.h
new file mode 100644
index 0000000..923eba6
--- /dev/null
+++ b/webrtc/modules/video_coding/codecs/h264/include/helper_cuda_drvapi.h
@@ -0,0 +1,513 @@
+/*
+ * Copyright 1993-2015 NVIDIA Corporation.  All rights reserved.
+ *
+ * Please refer to the NVIDIA end user license agreement (EULA) associated
+ * with this source code for terms and conditions that govern your use of
+ * this software. Any use, reproduction, disclosure, or distribution of
+ * this software and related documentation outside the terms of the EULA
+ * is strictly prohibited.
+ *
+ */
+
+// Helper functions for CUDA Driver API error handling (make sure that CUDA_H or the DYNLINK API version is included in your projects)
+#ifndef HELPER_CUDA_DRVAPI_H
+#define HELPER_CUDA_DRVAPI_H
+
+#include <stdlib.h>
+#include <stdio.h>
+#include <string.h>
+
+#include "dynlink_cuda_cuda.h"
+#include "drvapi_error_string.h"
+#include "helper_string.h"
+
+#ifndef __CUDA_API_VERSION
+#define __CUDA_API_VERSION 4000
+#endif
+
+#ifndef MAX
+#define MAX(a,b) (a > b ? a : b)
+#endif
+
+#ifndef EXIT_WAIVED
+#define EXIT_WAIVED 2
+#endif
+
+////////////////////////////////////////////////////////////////////////////////
+// These are CUDA Helper functions
+
+// add a level of protection to the CUDA SDK samples, let's force samples to explicitly include CUDA.H
+#ifdef  __cuda_cuda_h__
+// This will output the proper CUDA error strings in the event that a CUDA host call returns an error
+#ifndef checkCudaErrors
+#define checkCudaErrors(err)  __checkCudaErrors (err, __FILE__, __LINE__)
+
+// These are the inline versions for all of the SDK helper functions
+inline void __checkCudaErrors(CUresult err, const char *file, const int line)
+{
+    if (CUDA_SUCCESS != err)
+    {
+        fprintf(stderr, "checkCudaErrors() Driver API error = %04d \"%s\" from file <%s>, line %i.\n",
+                err, getCudaDrvErrorString(err), file, line);
+        exit(EXIT_FAILURE);
+    }
+}
+#endif
+
+#ifdef getLastCudaDrvErrorMsg
+#undef getLastCudaDrvErrorMsg
+#endif
+
+#define getLastCudaDrvErrorMsg(msg)           __getLastCudaDrvErrorMsg  (msg, __FILE__, __LINE__)
+
+inline void __getLastCudaDrvErrorMsg(const char *msg, const char *file, const int line)
+{
+    CUresult err = cuCtxSynchronize();
+
+    if (CUDA_SUCCESS != err)
+    {
+        fprintf(stderr, "getLastCudaDrvErrorMsg -> %s", msg);
+        fprintf(stderr, "getLastCudaDrvErrorMsg -> cuCtxSynchronize API error = %04d \"%s\" in file <%s>, line %i.\n",
+                err, getCudaDrvErrorString(err), file, line);
+        exit(EXIT_FAILURE);
+    }
+}
+
+// This function wraps the CUDA Driver API into a template function
+template <class T>
+inline void getCudaAttribute(T *attribute, CUdevice_attribute device_attribute, int device)
+{
+    CUresult error_result = cuDeviceGetAttribute(attribute, device_attribute, device);
+
+    if (error_result != CUDA_SUCCESS)
+    {
+        printf("cuDeviceGetAttribute returned %d\n-> %s\n", (int)error_result, getCudaDrvErrorString(error_result));
+        exit(EXIT_SUCCESS);
+    }
+}
+#endif
+
+// Beginning of GPU Architecture definitions
+inline int _ConvertSMVer2CoresDRV(int major, int minor)
+{
+    // Defines for GPU Architecture types (using the SM version to determine the # of cores per SM
+    typedef struct
+    {
+        int SM; // 0xMm (hexidecimal notation), M = SM Major version, and m = SM minor version
+        int Cores;
+    } sSMtoCores;
+
+    sSMtoCores nGpuArchCoresPerSM[] =
+    {
+        { 0x10,  8 }, // Tesla Generation (SM 1.0) G80 class
+        { 0x11,  8 }, // Tesla Generation (SM 1.1) G8x class
+        { 0x12,  8 }, // Tesla Generation (SM 1.2) G9x class
+        { 0x13,  8 }, // Tesla Generation (SM 1.3) GT200 class
+        { 0x20, 32 }, // Fermi Generation (SM 2.0) GF100 class
+        { 0x21, 48 }, // Fermi Generation (SM 2.1) GF10x class
+        { 0x30, 192}, // Kepler Generation (SM 3.0) GK10x class
+        { 0x32, 192}, // Kepler Generation (SM 3.2) GK10x class
+        { 0x35, 192}, // Kepler Generation (SM 3.5) GK11x class
+        { 0x37, 192}, // Kepler Generation (SM 3.7) GK21x class
+        { 0x50, 128}, // Maxwell Generation (SM 5.0) GM10x class
+        { 0x52, 128}, // Maxwell Generation (SM 5.2) GM20x class
+        {   -1, -1 }
+    };
+
+    int index = 0;
+
+    while (nGpuArchCoresPerSM[index].SM != -1)
+    {
+        if (nGpuArchCoresPerSM[index].SM == ((major << 4) + minor))
+        {
+            return nGpuArchCoresPerSM[index].Cores;
+        }
+
+        index++;
+    }
+
+    // If we don't find the values, we default use the previous one to run properly
+    printf("MapSMtoCores for SM %d.%d is undefined.  Default to use %d Cores/SM\n", major, minor, nGpuArchCoresPerSM[index-1].Cores);
+    return nGpuArchCoresPerSM[index-1].Cores;
+}
+// end of GPU Architecture definitions
+
+#ifdef __cuda_cuda_h__
+// General GPU Device CUDA Initialization
+inline int gpuDeviceInitDRV(int ARGC, const char **ARGV)
+{
+    int cuDevice = 0;
+    int deviceCount = 0;
+    CUresult err = cuInit(0, __CUDA_API_VERSION, NULL);
+
+    if (CUDA_SUCCESS == err)
+    {
+        checkCudaErrors(cuDeviceGetCount(&deviceCount));
+    }
+
+    if (deviceCount == 0)
+    {
+        fprintf(stderr, "cudaDeviceInit error: no devices supporting CUDA\n");
+        exit(EXIT_FAILURE);
+    }
+
+    int dev = 0;
+    dev = getCmdLineArgumentInt(ARGC, (const char **) ARGV, "device=");
+
+    if (dev < 0)
+    {
+        dev = 0;
+    }
+
+    if (dev > deviceCount-1)
+    {
+        fprintf(stderr, "\n");
+        fprintf(stderr, ">> %d CUDA capable GPU device(s) detected. <<\n", deviceCount);
+        fprintf(stderr, ">> cudaDeviceInit (-device=%d) is not a valid GPU device. <<\n", dev);
+        fprintf(stderr, "\n");
+        return -dev;
+    }
+
+    checkCudaErrors(cuDeviceGet(&cuDevice, dev));
+    char name[100];
+    cuDeviceGetName(name, 100, cuDevice);
+
+    int computeMode;
+    getCudaAttribute<int>(&computeMode, CU_DEVICE_ATTRIBUTE_COMPUTE_MODE, dev);
+
+    if (computeMode == CU_COMPUTEMODE_PROHIBITED)
+    {
+        fprintf(stderr, "Error: device is running in <CU_COMPUTEMODE_PROHIBITED>, no threads can use this CUDA Device.\n");
+        return -1;
+    }
+
+    if (checkCmdLineFlag(ARGC, (const char **) ARGV, "quiet") == false)
+    {
+        printf("gpuDeviceInitDRV() Using CUDA Device [%d]: %s\n", dev, name);
+    }
+
+    return dev;
+}
+
+// This function returns the best GPU based on performance
+inline int gpuGetMaxGflopsDeviceIdDRV()
+{
+    CUdevice current_device = 0, max_perf_device = 0;
+    int device_count        = 0, sm_per_multiproc = 0;
+    int max_compute_perf    = 0, best_SM_arch     = 0;
+    int major = 0, minor = 0   , multiProcessorCount, clockRate;
+    int devices_prohibited = 0;
+
+    cuInit(0, __CUDA_API_VERSION, NULL);
+    checkCudaErrors(cuDeviceGetCount(&device_count));
+
+    if (device_count == 0)
+    {
+        fprintf(stderr, "gpuGetMaxGflopsDeviceIdDRV error: no devices supporting CUDA\n");
+        exit(EXIT_FAILURE);
+    }
+
+    // Find the best major SM Architecture GPU device
+    while (current_device < device_count)
+    {
+        checkCudaErrors(cuDeviceComputeCapability(&major, &minor, current_device));
+
+        if (major > 0 && major < 9999)
+        {
+            best_SM_arch = MAX(best_SM_arch, major);
+        }
+
+        current_device++;
+    }
+
+    // Find the best CUDA capable GPU device
+    current_device = 0;
+
+    while (current_device < device_count)
+    {
+        checkCudaErrors(cuDeviceGetAttribute(&multiProcessorCount,
+                                             CU_DEVICE_ATTRIBUTE_MULTIPROCESSOR_COUNT,
+                                             current_device));
+        checkCudaErrors(cuDeviceGetAttribute(&clockRate,
+                                             CU_DEVICE_ATTRIBUTE_CLOCK_RATE,
+                                             current_device));
+        checkCudaErrors(cuDeviceComputeCapability(&major, &minor, current_device));
+
+        int computeMode;
+        getCudaAttribute<int>(&computeMode, CU_DEVICE_ATTRIBUTE_COMPUTE_MODE, current_device);
+
+        if (computeMode != CU_COMPUTEMODE_PROHIBITED)
+        {
+            if (major == 9999 && minor == 9999)
+            {
+                sm_per_multiproc = 1;
+            }
+            else
+            {
+                sm_per_multiproc = _ConvertSMVer2CoresDRV(major, minor);
+            }
+
+            int compute_perf  = multiProcessorCount * sm_per_multiproc * clockRate;
+
+            if (compute_perf  > max_compute_perf)
+            {
+                // If we find GPU with SM major > 2, search only these
+                if (best_SM_arch > 2)
+                {
+                    // If our device==dest_SM_arch, choose this, or else pass
+                    if (major == best_SM_arch)
+                    {
+                        max_compute_perf  = compute_perf;
+                        max_perf_device   = current_device;
+                    }
+                }
+                else
+                {
+                    max_compute_perf  = compute_perf;
+                    max_perf_device   = current_device;
+                }
+            }
+        }
+        else
+        {
+            devices_prohibited++;
+        }
+
+        ++current_device;
+    }
+
+    if (devices_prohibited == device_count)
+    {    
+        fprintf(stderr, "gpuGetMaxGflopsDeviceIdDRV error: all devices have compute mode prohibited.\n");
+        exit(EXIT_FAILURE);
+    }    
+
+    return max_perf_device;
+}
+
+// This function returns the best Graphics GPU based on performance
+inline int gpuGetMaxGflopsGLDeviceIdDRV()
+{
+    CUdevice current_device = 0, max_perf_device = 0;
+    int device_count     = 0, sm_per_multiproc = 0;
+    int max_compute_perf = 0, best_SM_arch     = 0;
+    int major = 0, minor = 0, multiProcessorCount, clockRate;
+    int bTCC = 0;
+    int devices_prohibited = 0;
+    char deviceName[256];
+
+    cuInit(0, __CUDA_API_VERSION, NULL);
+    checkCudaErrors(cuDeviceGetCount(&device_count));
+
+    if (device_count == 0)
+    {
+        fprintf(stderr, "gpuGetMaxGflopsGLDeviceIdDRV error: no devices supporting CUDA\n");
+        exit(EXIT_FAILURE);
+    }
+
+    // Find the best major SM Architecture GPU device that are graphics devices
+    while (current_device < device_count)
+    {
+        checkCudaErrors(cuDeviceGetName(deviceName, 256, current_device));
+        checkCudaErrors(cuDeviceComputeCapability(&major, &minor, current_device));
+
+#if CUDA_VERSION >= 3020
+        checkCudaErrors(cuDeviceGetAttribute(&bTCC,  CU_DEVICE_ATTRIBUTE_TCC_DRIVER, current_device));
+#else
+
+        // Assume a Tesla GPU is running in TCC if we are running CUDA 3.1
+        if (deviceName[0] == 'T')
+        {
+            bTCC = 1;
+        }
+
+#endif
+
+        int computeMode;
+        getCudaAttribute<int>(&computeMode, CU_DEVICE_ATTRIBUTE_COMPUTE_MODE, current_device);
+
+        if (computeMode != CU_COMPUTEMODE_PROHIBITED)
+        {
+            if (!bTCC)
+            {
+                if (major > 0 && major < 9999)
+                {
+                    best_SM_arch = MAX(best_SM_arch, major);
+                }
+            }
+        }
+        else
+        {
+            devices_prohibited++;
+        }
+
+        current_device++;
+    }
+
+    if (devices_prohibited == device_count)
+    {
+        fprintf(stderr, "gpuGetMaxGflopsGLDeviceIdDRV error: all devices have compute mode prohibited.\n");
+        exit(EXIT_FAILURE);
+    }
+
+    // Find the best CUDA capable GPU device
+    current_device = 0;
+
+    while (current_device < device_count)
+    {
+        checkCudaErrors(cuDeviceGetAttribute(&multiProcessorCount,
+                                             CU_DEVICE_ATTRIBUTE_MULTIPROCESSOR_COUNT,
+                                             current_device));
+        checkCudaErrors(cuDeviceGetAttribute(&clockRate,
+                                             CU_DEVICE_ATTRIBUTE_CLOCK_RATE,
+                                             current_device));
+        checkCudaErrors(cuDeviceComputeCapability(&major, &minor, current_device));
+
+#if CUDA_VERSION >= 3020
+        checkCudaErrors(cuDeviceGetAttribute(&bTCC,  CU_DEVICE_ATTRIBUTE_TCC_DRIVER, current_device));
+#else
+
+        // Assume a Tesla GPU is running in TCC if we are running CUDA 3.1
+        if (deviceName[0] == 'T')
+        {
+            bTCC = 1;
+        }
+
+#endif
+
+        int computeMode;
+        getCudaAttribute<int>(&computeMode, CU_DEVICE_ATTRIBUTE_COMPUTE_MODE, current_device);
+
+        if (computeMode != CU_COMPUTEMODE_PROHIBITED)
+        {
+            if (major == 9999 && minor == 9999)
+            {
+                sm_per_multiproc = 1;
+            }
+            else
+            {
+                sm_per_multiproc = _ConvertSMVer2CoresDRV(major, minor);
+            }
+
+            // If this is a Tesla based GPU and SM 2.0, and TCC is disabled, this is a contendor
+            if (!bTCC)   // Is this GPU running the TCC driver?  If so we pass on this
+            {
+                int compute_perf  = multiProcessorCount * sm_per_multiproc * clockRate;
+
+                if (compute_perf  > max_compute_perf)
+                {
+                    // If we find GPU with SM major > 2, search only these
+                    if (best_SM_arch > 2)
+                    {
+                        // If our device = dest_SM_arch, then we pick this one
+                        if (major == best_SM_arch)
+                        {
+                            max_compute_perf  = compute_perf;
+                            max_perf_device   = current_device;
+                        }
+                    }
+                    else
+                    {
+                        max_compute_perf  = compute_perf;
+                        max_perf_device   = current_device;
+                    }
+                }
+            }
+        }
+
+        ++current_device;
+    }
+
+    return max_perf_device;
+}
+
+// General initialization call to pick the best CUDA Device
+inline CUdevice findCudaDeviceDRV(int argc, const char **argv)
+{
+    CUdevice cuDevice;
+    int devID = 0;
+
+    // If the command-line has a device number specified, use it
+    if (checkCmdLineFlag(argc, (const char **)argv, "device"))
+    {
+        devID = gpuDeviceInitDRV(argc, argv);
+
+        if (devID < 0)
+        {
+            printf("exiting...\n");
+            exit(EXIT_SUCCESS);
+        }
+    }
+    else
+    {
+        // Otherwise pick the device with highest Gflops/s
+        char name[100]; 
+        devID = gpuGetMaxGflopsDeviceIdDRV();
+        checkCudaErrors(cuDeviceGet(&cuDevice, devID));
+        cuDeviceGetName(name, 100, cuDevice);
+        printf("> Using CUDA Device [%d]: %s\n", devID, name);
+    }
+
+    cuDeviceGet(&cuDevice, devID);
+
+    return cuDevice;
+}
+
+// This function will pick the best CUDA device available with OpenGL interop
+inline CUdevice findCudaGLDeviceDRV(int argc, const char **argv)
+{
+    CUdevice cuDevice;
+    int devID = 0;
+
+    // If the command-line has a device number specified, use it
+    if (checkCmdLineFlag(argc, (const char **)argv, "device"))
+    {
+        devID = gpuDeviceInitDRV(argc, (const char **)argv);
+
+        if (devID < 0)
+        {
+            printf("no CUDA capable devices found, exiting...\n");
+            exit(EXIT_SUCCESS);
+        }
+    }
+    else
+    {
+        char name[100];
+        // Otherwise pick the device with highest Gflops/s
+        devID = gpuGetMaxGflopsGLDeviceIdDRV();
+        checkCudaErrors(cuDeviceGet(&cuDevice, devID));
+        cuDeviceGetName(name, 100, cuDevice);
+        printf("> Using CUDA/GL Device [%d]: %s\n", devID, name);
+    }
+
+    return devID;
+}
+
+// General check for CUDA GPU SM Capabilities
+inline bool checkCudaCapabilitiesDRV(int major_version, int minor_version, int devID)
+{
+    CUdevice cuDevice;
+    char name[256];
+    int major = 0, minor = 0;
+
+    checkCudaErrors(cuDeviceGet(&cuDevice, devID));
+    checkCudaErrors(cuDeviceGetName(name, 100, cuDevice));
+    checkCudaErrors(cuDeviceComputeCapability(&major, &minor, devID));
+
+    if ((major > major_version) ||
+        (major == major_version && minor >= minor_version))
+    {
+        printf("> Device %d: <%16s >, Compute SM %d.%d detected\n", devID, name, major, minor);
+        return true;
+    }
+    else
+    {
+        printf("No GPU device was found that can support CUDA compute capability %d.%d.\n", major_version, minor_version);
+        return false;
+    }
+}
+#endif
+
+// end of CUDA Helper Functions
+
+#endif
diff --git a/webrtc/modules/video_coding/codecs/h264/include/helper_functions.h b/webrtc/modules/video_coding/codecs/h264/include/helper_functions.h
new file mode 100644
index 0000000..c4d7d77
--- /dev/null
+++ b/webrtc/modules/video_coding/codecs/h264/include/helper_functions.h
@@ -0,0 +1,40 @@
+/*
+ * Copyright 1993-2015 NVIDIA Corporation.  All rights reserved.
+ *
+ * Please refer to the NVIDIA end user license agreement (EULA) associated
+ * with this source code for terms and conditions that govern your use of
+ * this software. Any use, reproduction, disclosure, or distribution of
+ * this software and related documentation outside the terms of the EULA
+ * is strictly prohibited.
+ *
+ */
+
+// These are helper functions for the SDK samples (string parsing, timers, image helpers, etc)
+#ifndef HELPER_FUNCTIONS_H
+#define HELPER_FUNCTIONS_H
+
+#ifdef WIN32
+#pragma warning(disable:4996)
+#endif
+
+// includes, project
+#include <stdio.h>
+#include <stdlib.h>
+#include <string>
+#include <assert.h>
+#include <exception.h>
+#include <math.h>
+
+#include <fstream>
+#include <vector>
+#include <iostream>
+#include <algorithm>
+
+// includes, timer, string parsing, image helpers
+#include <helper_timer.h>   // helper functions for timers
+
+#ifndef EXIT_WAIVED
+#define EXIT_WAIVED 2
+#endif
+
+#endif //  HELPER_FUNCTIONS_H
diff --git a/webrtc/modules/video_coding/codecs/h264/include/helper_string.h b/webrtc/modules/video_coding/codecs/h264/include/helper_string.h
new file mode 100644
index 0000000..e7128d9
--- /dev/null
+++ b/webrtc/modules/video_coding/codecs/h264/include/helper_string.h
@@ -0,0 +1,398 @@
+/*
+ * Copyright 1993-2015 NVIDIA Corporation.  All rights reserved.
+ *
+ * Please refer to the NVIDIA end user license agreement (EULA) associated
+ * with this source code for terms and conditions that govern your use of
+ * this software. Any use, reproduction, disclosure, or distribution of
+ * this software and related documentation outside the terms of the EULA
+ * is strictly prohibited.
+ *
+ */
+
+// These are helper functions for the SDK samples (string parsing, timers, etc)
+#ifndef STRING_HELPER_H
+#define STRING_HELPER_H
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <fstream>
+#include <string>
+
+#if defined(WIN32) || defined(_WIN32) || defined(WIN64)
+  #ifndef _CRT_SECURE_NO_DEPRECATE
+  #define _CRT_SECURE_NO_DEPRECATE
+  #endif
+  #ifndef STRCASECMP
+  #define STRCASECMP  _stricmp
+  #endif
+  #ifndef STRNCASECMP
+  #define STRNCASECMP _strnicmp
+  #endif
+  #ifndef STRCPY
+  #define STRCPY(sFilePath, nLength, sPath) strcpy_s(sFilePath, nLength, sPath)
+  #endif
+
+  #ifndef FOPEN
+  #define FOPEN(fHandle,filename,mode) fopen_s(&fHandle, filename, mode)
+  #endif
+  #ifndef FOPEN_FAIL
+  #define FOPEN_FAIL(result) (result != 0)
+  #endif
+  #ifndef SSCANF
+  #define SSCANF sscanf_s
+  #endif
+#else
+  #include <string.h>
+  #include <strings.h>
+
+  #ifndef STRCASECMP
+  #define STRCASECMP  strcasecmp
+  #endif
+  #ifndef STRNCASECMP
+  #define STRNCASECMP strncasecmp
+  #endif
+  #ifndef STRCPY
+  #define STRCPY(sFilePath, nLength, sPath) strcpy(sFilePath, sPath)
+  #endif
+
+  #ifndef FOPEN
+  #define FOPEN(fHandle,filename,mode) (fHandle = fopen(filename, mode))
+  #endif
+  #ifndef FOPEN_FAIL
+  #define FOPEN_FAIL(result) (result == NULL)
+  #endif
+  #ifndef SSCANF
+  #define SSCANF sscanf
+  #endif
+#endif
+
+#ifndef EXIT_WAIVED
+#define EXIT_WAIVED 2
+#endif
+
+// CUDA Utility Helper Functions
+inline int stringRemoveDelimiter(char delimiter, const char *string)
+{
+    int string_start = 0;
+
+    while (string[string_start] == delimiter)
+    {
+        string_start++;
+    }
+
+    if (string_start >= (int)strlen(string)-1)
+    {
+        return 0;
+    }
+
+    return string_start;
+}
+
+inline int getFileExtension(char *filename, char **extension)
+{
+    int string_length = (int)strlen(filename);
+
+    while (filename[string_length--] != '.') {
+        if (string_length == 0)
+            break;
+    }
+    if (string_length > 0) string_length += 2;
+
+    if (string_length == 0) 
+        *extension = NULL;
+    else 
+        *extension = &filename[string_length];
+
+    return string_length;
+}
+
+
+inline int checkCmdLineFlag(const int argc, const char **argv, const char *string_ref)
+{
+    bool bFound = false;
+
+    if (argc >= 1)
+    {
+        for (int i=1; i < argc; i++)
+        {
+            int string_start = stringRemoveDelimiter('-', argv[i]);
+            const char *string_argv = &argv[i][string_start];
+
+            const char *equal_pos = strchr(string_argv, '=');
+            int argv_length = (int)(equal_pos == 0 ? strlen(string_argv) : equal_pos - string_argv);
+
+            int length = (int)strlen(string_ref);
+
+            if (length == argv_length && !STRNCASECMP(string_argv, string_ref, length))
+            {
+
+                bFound = true;
+                continue;
+            }
+        }
+    }
+
+    return (int)bFound;
+}
+
+// This function wraps the CUDA Driver API into a template function
+template <class T>
+inline bool getCmdLineArgumentValue(const int argc, const char **argv, const char *string_ref, T *value)
+{
+    bool bFound = false;
+    if (argc >= 1) {
+        for (int i=1; i < argc; i++) {
+            int string_start = stringRemoveDelimiter('-', argv[i]);
+            const char *string_argv = &argv[i][string_start];
+            int length = (int)strlen(string_ref);
+            if (!STRNCASECMP(string_argv, string_ref, length) ) {
+                if (length+1 <= (int)strlen(string_argv)) {
+                    int auto_inc = (string_argv[length] == '=') ? 1 : 0;
+                    *value = (T)atoi(&string_argv[length + auto_inc]);
+                }
+                bFound = true;
+                i=argc;
+            }
+        }
+    }
+    return bFound;
+}
+
+inline int getCmdLineArgumentInt(const int argc, const char **argv, const char *string_ref)
+{
+    bool bFound = false;
+    int value = -1;
+
+    if (argc >= 1)
+    {
+        for (int i=1; i < argc; i++)
+        {
+            int string_start = stringRemoveDelimiter('-', argv[i]);
+            const char *string_argv = &argv[i][string_start];
+            int length = (int)strlen(string_ref);
+
+            if (!STRNCASECMP(string_argv, string_ref, length))
+            {
+                if (length+1 <= (int)strlen(string_argv))
+                {
+                    int auto_inc = (string_argv[length] == '=') ? 1 : 0;
+                    value = atoi(&string_argv[length + auto_inc]);
+                }
+                else
+                {
+                    value = 0;
+                }
+
+                bFound = true;
+                continue;
+            }
+        }
+    }
+
+    if (bFound)
+    {
+        return value;
+    }
+    else
+    {
+        return 0;
+    }
+}
+
+inline float getCmdLineArgumentFloat(const int argc, const char **argv, const char *string_ref)
+{
+    bool bFound = false;
+    float value = -1;
+
+    if (argc >= 1)
+    {
+        for (int i=1; i < argc; i++)
+        {
+            int string_start = stringRemoveDelimiter('-', argv[i]);
+            const char *string_argv = &argv[i][string_start];
+            int length = (int)strlen(string_ref);
+
+            if (!STRNCASECMP(string_argv, string_ref, length))
+            {
+                if (length+1 <= (int)strlen(string_argv))
+                {
+                    int auto_inc = (string_argv[length] == '=') ? 1 : 0;
+                    value = (float)atof(&string_argv[length + auto_inc]);
+                }
+                else
+                {
+                    value = 0.f;
+                }
+
+                bFound = true;
+                continue;
+            }
+        }
+    }
+
+    if (bFound)
+    {
+        return value;
+    }
+    else
+    {
+        return 0;
+    }
+}
+
+inline bool getCmdLineArgumentString(const int argc, const char **argv,
+                                     const char *string_ref, char **string_retval)
+{
+    bool bFound = false;
+
+    if (argc >= 1)
+    {
+        for (int i=1; i < argc; i++)
+        {
+            int string_start = stringRemoveDelimiter('-', argv[i]);
+            char *string_argv = (char *)&argv[i][string_start];
+            int length = (int)strlen(string_ref);
+
+            if (!STRNCASECMP(string_argv, string_ref, length))
+            {
+                *string_retval = &string_argv[length+1];
+                bFound = true;
+                continue;
+            }
+        }
+    }
+
+    if (!bFound)
+    {
+        *string_retval = NULL;
+    }
+
+    return bFound;
+}
+
+//////////////////////////////////////////////////////////////////////////////
+//! Find the path for a file assuming that
+//! files are found in the searchPath.
+//!
+//! @return the path if succeeded, otherwise 0
+//! @param filename         name of the file
+//! @param executable_path  optional absolute path of the executable
+//////////////////////////////////////////////////////////////////////////////
+inline char *sdkFindFilePath(const char *filename, const char *executable_path)
+{
+    // <executable_name> defines a variable that is replaced with the name of the executable
+
+    // Typical relative search paths to locate needed companion files (e.g. sample input data, or JIT source files)
+    // The origin for the relative search may be the .exe file, a .bat file launching an .exe, a browser .exe launching the .exe or .bat, etc
+    const char *searchPath[] =
+    {
+        "./",                                       // same dir
+        "./data/",                                  // "/data/" subdir
+        "./<executable_name>/data/",                // "/<executable_name>/data/" subdir
+        "./inc/",                                   // "/inc/" subdir
+        "./common/video/",                          // "/common/video/" subdir
+        "./common/kernels/ptx/",
+
+        "../",                                      // up 1 in tree
+        "../data/",                                 // up 1 in tree, "/data/" subdir
+        "../<executable_name>/data/",               // up 1 in tree, "/<executable_name>/data/" subdir
+        "../inc/",                                  // up 1 in tree, "/inc/" subdir
+        "../common/video/",                         // up 1 in tree, "/common/video/" subdir
+        "../common/kernels/ptx/",
+
+        "../../",                                   // up 2 in tree
+        "../../data/",                              // up 2 in tree, "/data/" subdir
+        "../../<executable_name>/data/",            // up 2 in tree, "/<executable_name>/data/" subdir
+        "../../inc/",                               // up 2 in tree, "/inc/" subdir
+        "../../common/video/",                      // up 2 in tree, "/common/video/" subdir
+        "../../common/kernels/ptx/",
+
+        "../../../",                                // up 3 in tree
+        "../../../data/",                           // up 3 in tree, "../../../data/" subdir
+        "../../../<executable_name>/data/",         // up 3 in tree, "/<executable_name>/data/" subdir
+        "../../../common/video/",                   // up 3 in tree, "/common/video/" subdir
+        "../../../common/kernels/ptx/",
+
+        "../../../../",                             // up 4 in tree
+        "../../../../data/",                        // up 4 in tree, "../../../data/" subdir    
+        "../../../../<executable_name>/data/",      // up 4 in tree, "/<executable_name>/data/" subdir
+        "../../../../common/video/",                // up 4 in tree, "/common/video/" subdir
+        "../../../../common/kernels/ptx/",
+    };
+
+    // Extract the executable name
+    std::string executable_name;
+
+    if (executable_path != 0)
+    {
+        executable_name = std::string(executable_path);
+
+#ifdef _WIN32
+        // Windows path delimiter
+        size_t delimiter_pos = executable_name.find_last_of('\\');
+        executable_name.erase(0, delimiter_pos + 1);
+
+        if (executable_name.rfind(".exe") != std::string::npos)
+        {
+            // we strip .exe, only if the .exe is found
+            executable_name.resize(executable_name.size() - 4);
+        }
+
+#else
+        // Linux & OSX path delimiter
+        size_t delimiter_pos = executable_name.find_last_of('/');
+        executable_name.erase(0,delimiter_pos+1);
+#endif
+    }
+
+    // Loop over all search paths and return the first hit
+    for (unsigned int i = 0; i < sizeof(searchPath)/sizeof(char *); ++i)
+    {
+        std::string path(searchPath[i]);
+        size_t executable_name_pos = path.find("<executable_name>");
+
+        // If there is executable_name variable in the searchPath
+        // replace it with the value
+        if (executable_name_pos != std::string::npos)
+        {
+            if (executable_path != 0)
+            {
+                path.replace(executable_name_pos, strlen("<executable_name>"), executable_name);
+            }
+            else
+            {
+                // Skip this path entry if no executable argument is given
+                continue;
+            }
+        }
+
+#ifdef _DEBUG
+//        printf("sdkFindFilePath <%s> in %s\n", filename, path.c_str());
+#endif
+
+        // Test if the file exists
+        path.append(filename);
+        FILE *fp;
+        FOPEN(fp, path.c_str(), "rb");
+
+        if (fp != NULL)
+        {
+            fclose(fp);
+            // File found
+            // returning an allocated array here for backwards compatibility reasons
+            char *file_path = (char *) malloc(path.length() + 1);
+            STRCPY(file_path, path.length() + 1, path.c_str());
+            return file_path;
+        }
+
+        if (fp)
+        {
+            fclose(fp);
+        }
+    }
+
+    // File not found
+    return 0;
+}
+
+#endif
diff --git a/webrtc/modules/video_coding/codecs/h264/include/helper_timer.h b/webrtc/modules/video_coding/codecs/h264/include/helper_timer.h
new file mode 100644
index 0000000..71d1330
--- /dev/null
+++ b/webrtc/modules/video_coding/codecs/h264/include/helper_timer.h
@@ -0,0 +1,499 @@
+/*
+ * Copyright 1993-2015 NVIDIA Corporation.  All rights reserved.
+ *
+ * Please refer to the NVIDIA end user license agreement (EULA) associated
+ * with this source code for terms and conditions that govern your use of
+ * this software. Any use, reproduction, disclosure, or distribution of
+ * this software and related documentation outside the terms of the EULA
+ * is strictly prohibited.
+ *
+ */
+
+// Helper Timing Functions
+#ifndef HELPER_TIMER_H
+#define HELPER_TIMER_H
+
+#ifndef EXIT_WAIVED
+#define EXIT_WAIVED 2
+#endif
+
+// includes, system
+#include <vector>
+
+// includes, project
+#include <exception.h>
+
+// Definition of the StopWatch Interface, this is used if we don't want to use the CUT functions
+// But rather in a self contained class interface
+class StopWatchInterface
+{
+    public:
+        StopWatchInterface() {};
+        virtual ~StopWatchInterface() {};
+
+    public:
+        //! Start time measurement
+        virtual void start() = 0;
+
+        //! Stop time measurement
+        virtual void stop() = 0;
+
+        //! Reset time counters to zero
+        virtual void reset() = 0;
+
+        //! Time in msec. after start. If the stop watch is still running (i.e. there
+        //! was no call to stop()) then the elapsed time is returned, otherwise the
+        //! time between the last start() and stop call is returned
+        virtual float getTime() = 0;
+
+        //! Mean time to date based on the number of times the stopwatch has been
+        //! _stopped_ (ie finished sessions) and the current total time
+        virtual float getAverageTime() = 0;
+};
+
+
+//////////////////////////////////////////////////////////////////
+// Begin Stopwatch timer class definitions for all OS platforms //
+//////////////////////////////////////////////////////////////////
+#ifdef WIN32
+// includes, system
+#define WINDOWS_LEAN_AND_MEAN
+#include <windows.h>
+#undef min
+#undef max
+
+//! Windows specific implementation of StopWatch
+class StopWatchWin : public StopWatchInterface
+{
+    public:
+        //! Constructor, default
+        StopWatchWin() :
+            start_time(),     end_time(),
+            diff_time(0.0f),  total_time(0.0f),
+            running(false), clock_sessions(0), freq(0), freq_set(false)
+        {
+            if (! freq_set)
+            {
+                // helper variable
+                LARGE_INTEGER temp;
+
+                // get the tick frequency from the OS
+                QueryPerformanceFrequency((LARGE_INTEGER *) &temp);
+
+                // convert to type in which it is needed
+                freq = ((double) temp.QuadPart) / 1000.0;
+
+                // rememeber query
+                freq_set = true;
+            }
+        };
+
+        // Destructor
+        ~StopWatchWin() { };
+
+    public:
+        //! Start time measurement
+        inline void start();
+
+        //! Stop time measurement
+        inline void stop();
+
+        //! Reset time counters to zero
+        inline void reset();
+
+        //! Time in msec. after start. If the stop watch is still running (i.e. there
+        //! was no call to stop()) then the elapsed time is returned, otherwise the
+        //! time between the last start() and stop call is returned
+        inline float getTime();
+
+        //! Mean time to date based on the number of times the stopwatch has been
+        //! _stopped_ (ie finished sessions) and the current total time
+        inline float getAverageTime();
+
+    private:
+        // member variables
+
+        //! Start of measurement
+        LARGE_INTEGER  start_time;
+        //! End of measurement
+        LARGE_INTEGER  end_time;
+
+        //! Time difference between the last start and stop
+        float  diff_time;
+
+        //! TOTAL time difference between starts and stops
+        float  total_time;
+
+        //! flag if the stop watch is running
+        bool running;
+
+        //! Number of times clock has been started
+        //! and stopped to allow averaging
+        int clock_sessions;
+
+        //! tick frequency
+        double  freq;
+
+        //! flag if the frequency has been set
+        bool  freq_set;
+};
+
+// functions, inlined
+
+////////////////////////////////////////////////////////////////////////////////
+//! Start time measurement
+////////////////////////////////////////////////////////////////////////////////
+inline void
+StopWatchWin::start()
+{
+    QueryPerformanceCounter((LARGE_INTEGER *) &start_time);
+    running = true;
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Stop time measurement and increment add to the current diff_time summation
+//! variable. Also increment the number of times this clock has been run.
+////////////////////////////////////////////////////////////////////////////////
+inline void
+StopWatchWin::stop()
+{
+    QueryPerformanceCounter((LARGE_INTEGER *) &end_time);
+    diff_time = (float)
+                (((double) end_time.QuadPart - (double) start_time.QuadPart) / freq);
+
+    total_time += diff_time;
+    clock_sessions++;
+    running = false;
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Reset the timer to 0. Does not change the timer running state but does
+//! recapture this point in time as the current start time if it is running.
+////////////////////////////////////////////////////////////////////////////////
+inline void
+StopWatchWin::reset()
+{
+    diff_time = 0;
+    total_time = 0;
+    clock_sessions = 0;
+
+    if (running)
+    {
+        QueryPerformanceCounter((LARGE_INTEGER *) &start_time);
+    }
+}
+
+
+////////////////////////////////////////////////////////////////////////////////
+//! Time in msec. after start. If the stop watch is still running (i.e. there
+//! was no call to stop()) then the elapsed time is returned added to the
+//! current diff_time sum, otherwise the current summed time difference alone
+//! is returned.
+////////////////////////////////////////////////////////////////////////////////
+inline float
+StopWatchWin::getTime()
+{
+    // Return the TOTAL time to date
+    float retval = total_time;
+
+    if (running)
+    {
+        LARGE_INTEGER temp;
+        QueryPerformanceCounter((LARGE_INTEGER *) &temp);
+        retval += (float)
+                  (((double)(temp.QuadPart - start_time.QuadPart)) / freq);
+    }
+
+    return retval;
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Time in msec. for a single run based on the total number of COMPLETED runs
+//! and the total time.
+////////////////////////////////////////////////////////////////////////////////
+inline float
+StopWatchWin::getAverageTime()
+{
+    return (clock_sessions > 0) ? (total_time/clock_sessions) : 0.0f;
+}
+#else
+// Declarations for Stopwatch on Linux and Mac OSX
+// includes, system
+#include <ctime>
+#include <sys/time.h>
+
+//! Windows specific implementation of StopWatch
+class StopWatchLinux : public StopWatchInterface
+{
+    public:
+        //! Constructor, default
+        StopWatchLinux() :
+            start_time(), diff_time(0.0), total_time(0.0),
+            running(false), clock_sessions(0)
+        { };
+
+        // Destructor
+        virtual ~StopWatchLinux()
+        { };
+
+    public:
+        //! Start time measurement
+        inline void start();
+
+        //! Stop time measurement
+        inline void stop();
+
+        //! Reset time counters to zero
+        inline void reset();
+
+        //! Time in msec. after start. If the stop watch is still running (i.e. there
+        //! was no call to stop()) then the elapsed time is returned, otherwise the
+        //! time between the last start() and stop call is returned
+        inline float getTime();
+
+        //! Mean time to date based on the number of times the stopwatch has been
+        //! _stopped_ (ie finished sessions) and the current total time
+        inline float getAverageTime();
+
+    private:
+
+        // helper functions
+
+        //! Get difference between start time and current time
+        inline float getDiffTime();
+
+    private:
+
+        // member variables
+
+        //! Start of measurement
+        struct timeval  start_time;
+
+        //! Time difference between the last start and stop
+        float  diff_time;
+
+        //! TOTAL time difference between starts and stops
+        float  total_time;
+
+        //! flag if the stop watch is running
+        bool running;
+
+        //! Number of times clock has been started
+        //! and stopped to allow averaging
+        int clock_sessions;
+};
+
+// functions, inlined
+
+////////////////////////////////////////////////////////////////////////////////
+//! Start time measurement
+////////////////////////////////////////////////////////////////////////////////
+inline void
+StopWatchLinux::start()
+{
+    gettimeofday(&start_time, 0);
+    running = true;
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Stop time measurement and increment add to the current diff_time summation
+//! variable. Also increment the number of times this clock has been run.
+////////////////////////////////////////////////////////////////////////////////
+inline void
+StopWatchLinux::stop()
+{
+    diff_time = getDiffTime();
+    total_time += diff_time;
+    running = false;
+    clock_sessions++;
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Reset the timer to 0. Does not change the timer running state but does
+//! recapture this point in time as the current start time if it is running.
+////////////////////////////////////////////////////////////////////////////////
+inline void
+StopWatchLinux::reset()
+{
+    diff_time = 0;
+    total_time = 0;
+    clock_sessions = 0;
+
+    if (running)
+    {
+        gettimeofday(&start_time, 0);
+    }
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Time in msec. after start. If the stop watch is still running (i.e. there
+//! was no call to stop()) then the elapsed time is returned added to the
+//! current diff_time sum, otherwise the current summed time difference alone
+//! is returned.
+////////////////////////////////////////////////////////////////////////////////
+inline float
+StopWatchLinux::getTime()
+{
+    // Return the TOTAL time to date
+    float retval = total_time;
+
+    if (running)
+    {
+        retval += getDiffTime();
+    }
+
+    return retval;
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Time in msec. for a single run based on the total number of COMPLETED runs
+//! and the total time.
+////////////////////////////////////////////////////////////////////////////////
+inline float
+StopWatchLinux::getAverageTime()
+{
+    return (clock_sessions > 0) ? (total_time/clock_sessions) : 0.0f;
+}
+////////////////////////////////////////////////////////////////////////////////
+
+////////////////////////////////////////////////////////////////////////////////
+inline float
+StopWatchLinux::getDiffTime()
+{
+    struct timeval t_time;
+    gettimeofday(&t_time, 0);
+
+    // time difference in milli-seconds
+    return (float)(1000.0 * (t_time.tv_sec - start_time.tv_sec)
+                   + (0.001 * (t_time.tv_usec - start_time.tv_usec)));
+}
+#endif // _WIN32
+
+////////////////////////////////////////////////////////////////////////////////
+//! Timer functionality exported
+
+////////////////////////////////////////////////////////////////////////////////
+//! Create a new timer
+//! @return true if a time has been created, otherwise false
+//! @param  name of the new timer, 0 if the creation failed
+////////////////////////////////////////////////////////////////////////////////
+inline bool
+sdkCreateTimer(StopWatchInterface **timer_interface)
+{
+    //printf("sdkCreateTimer called object %08x\n", (void *)*timer_interface);
+#ifdef _WIN32
+    *timer_interface = (StopWatchInterface *)new StopWatchWin();
+#else
+    *timer_interface = (StopWatchInterface *)new StopWatchLinux();
+#endif
+    return (*timer_interface != NULL) ? true : false;
+}
+
+
+////////////////////////////////////////////////////////////////////////////////
+//! Delete a timer
+//! @return true if a time has been deleted, otherwise false
+//! @param  name of the timer to delete
+////////////////////////////////////////////////////////////////////////////////
+inline bool
+sdkDeleteTimer(StopWatchInterface **timer_interface)
+{
+    //printf("sdkDeleteTimer called object %08x\n", (void *)*timer_interface);
+    if (*timer_interface)
+    {
+        delete *timer_interface;
+        *timer_interface = NULL;
+    }
+
+    return true;
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Start the time with name \a name
+//! @param name  name of the timer to start
+////////////////////////////////////////////////////////////////////////////////
+inline bool
+sdkStartTimer(StopWatchInterface **timer_interface)
+{
+    //printf("sdkStartTimer called object %08x\n", (void *)*timer_interface);
+    if (*timer_interface)
+    {
+        (*timer_interface)->start();
+    }
+
+    return true;
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Stop the time with name \a name. Does not reset.
+//! @param name  name of the timer to stop
+////////////////////////////////////////////////////////////////////////////////
+inline bool
+sdkStopTimer(StopWatchInterface **timer_interface)
+{
+    // printf("sdkStopTimer called object %08x\n", (void *)*timer_interface);
+    if (*timer_interface)
+    {
+        (*timer_interface)->stop();
+    }
+
+    return true;
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Resets the timer's counter.
+//! @param name  name of the timer to reset.
+////////////////////////////////////////////////////////////////////////////////
+inline bool
+sdkResetTimer(StopWatchInterface **timer_interface)
+{
+    // printf("sdkResetTimer called object %08x\n", (void *)*timer_interface);
+    if (*timer_interface)
+    {
+        (*timer_interface)->reset();
+    }
+
+    return true;
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Return the average time for timer execution as the total time
+//! for the timer dividied by the number of completed (stopped) runs the timer
+//! has made.
+//! Excludes the current running time if the timer is currently running.
+//! @param name  name of the timer to return the time of
+////////////////////////////////////////////////////////////////////////////////
+inline float
+sdkGetAverageTimerValue(StopWatchInterface **timer_interface)
+{
+    //  printf("sdkGetAverageTimerValue called object %08x\n", (void *)*timer_interface);
+    if (*timer_interface)
+    {
+        return (*timer_interface)->getAverageTime();
+    }
+    else
+    {
+        return 0.0f;
+    }
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Total execution time for the timer over all runs since the last reset
+//! or timer creation.
+//! @param name  name of the timer to obtain the value of.
+////////////////////////////////////////////////////////////////////////////////
+inline float
+sdkGetTimerValue(StopWatchInterface **timer_interface)
+{
+    // printf("sdkGetTimerValue called object %08x\n", (void *)*timer_interface);
+    if (*timer_interface)
+    {
+        return (*timer_interface)->getTime();
+    }
+    else
+    {
+        return 0.0f;
+    }
+}
+
+#endif // HELPER_TIMER_H
diff --git a/webrtc/modules/video_coding/codecs/h264/include/macros.h b/webrtc/modules/video_coding/codecs/h264/include/macros.h
new file mode 100644
index 0000000..d0410ef
--- /dev/null
+++ b/webrtc/modules/video_coding/codecs/h264/include/macros.h
@@ -0,0 +1,9 @@
+#pragma once
+
+#define SAFE_RELEASE(p)					\
+	if (p)								\
+	{									\
+		p->Release();					\
+		p = nullptr;					\
+	}
+
diff --git a/webrtc/modules/video_coding/codecs/h264/include/nvCPUOPSys.h b/webrtc/modules/video_coding/codecs/h264/include/nvCPUOPSys.h
new file mode 100644
index 0000000..f350bc7
--- /dev/null
+++ b/webrtc/modules/video_coding/codecs/h264/include/nvCPUOPSys.h
@@ -0,0 +1,28 @@
+/*
+ * Copyright 1993-2015 NVIDIA Corporation.  All rights reserved.
+ *
+ * Please refer to the NVIDIA end user license agreement (EULA) associated
+ * with this source code for terms and conditions that govern your use of
+ * this software. Any use, reproduction, disclosure, or distribution of
+ * this software and related documentation outside the terms of the EULA
+ * is strictly prohibited.
+ *
+ */
+
+#ifndef NVCPUOPSYS_H
+#define NVCPUOPSYS_H
+
+
+#if defined(_WIN32) || defined(_WIN16)
+#   define NV_WINDOWS
+#endif
+
+#if (defined(__unix__) || defined(__unix) ) && !defined(nvmacosx) && !defined(vxworks) && !defined(__DJGPP__) && !defined(NV_UNIX) && !defined(__QNX__) && !defined(__QNXNTO__)/* XXX until removed from Makefiles */
+#   define NV_UNIX
+#endif /* defined(__unix__) */
+
+#if defined(__linux__) && !defined(NV_LINUX) && !defined(NV_VMWARE)
+#   define NV_LINUX
+#endif  /* defined(__linux__) */
+
+#endif
diff --git a/webrtc/modules/video_coding/codecs/h264/include/nvEncodeAPI.h b/webrtc/modules/video_coding/codecs/h264/include/nvEncodeAPI.h
new file mode 100644
index 0000000..0565193
--- /dev/null
+++ b/webrtc/modules/video_coding/codecs/h264/include/nvEncodeAPI.h
@@ -0,0 +1,3249 @@
+/*
+ * This copyright notice applies to this header file only:
+ *
+ * Copyright (c) 2010-2015 NVIDIA Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person
+ * obtaining a copy of this software and associated documentation
+ * files (the "Software"), to deal in the Software without
+ * restriction, including without limitation the rights to use,
+ * copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the software, and to permit persons to whom the
+ * software is furnished to do so, subject to the following
+ * conditions:
+ *
+ * The above copyright notice and this permission notice shall be
+ * included in all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES
+ * OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT
+ * HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
+ * WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ */
+
+/**
+ * \file nvEncodeAPI.h
+ *   NvEncodeAPI provides a NVENC Video Encoding interface to NVIDIA GPU devices based on the Kepler architecture.
+ * \date 2011-2016
+ *  This file contains the interface constants, structure definitions and function prototypes.
+ */
+
+#ifndef _NV_ENCODEAPI_H_
+#define _NV_ENCODEAPI_H_
+
+#include <stdlib.h>
+
+#ifdef _WIN32
+#include <windows.h>
+#endif
+
+#ifdef _MSC_VER
+#ifndef _STDINT
+typedef __int32 int32_t;
+typedef unsigned __int32 uint32_t;
+typedef __int64 int64_t;
+typedef unsigned __int64 uint64_t;
+typedef signed char int8_t;
+typedef unsigned char uint8_t;
+typedef short int16_t;
+typedef unsigned short uint16_t;
+#endif
+#else
+#include <stdint.h>
+#endif
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/**
+ * \addtogroup ENCODER_STRUCTURE NvEncodeAPI Data structures
+ * @{
+ */
+
+#ifdef _WIN32
+#define NVENCAPI     __stdcall
+typedef RECT NVENC_RECT;
+#else
+#define NVENCAPI
+// =========================================================================================
+#ifndef GUID
+/*!
+ * \struct GUID
+ * Abstracts the GUID structure for non-windows platforms.
+ */
+// =========================================================================================
+typedef struct
+{
+    uint32_t Data1;                                      /**< [in]: Specifies the first 8 hexadecimal digits of the GUID.                                */
+    uint16_t Data2;                                      /**< [in]: Specifies the first group of 4 hexadecimal digits.                                   */
+    uint16_t Data3;                                      /**< [in]: Specifies the second group of 4 hexadecimal digits.                                  */
+    uint8_t  Data4[8];                                   /**< [in]: Array of 8 bytes. The first 2 bytes contain the third group of 4 hexadecimal digits.
+                                                                    The remaining 6 bytes contain the final 12 hexadecimal digits.                       */
+} GUID;
+#endif // GUID
+
+/**
+ * \struct _NVENC_RECT
+ * Defines a Rectangle. Used in ::NV_ENC_PREPROCESS_FRAME.
+ */
+typedef struct _NVENC_RECT
+{
+    uint32_t left;                                        /**< [in]: X coordinate of the upper left corner of rectangular area to be specified.       */
+    uint32_t top;                                         /**< [in]: Y coordinate of the upper left corner of the rectangular area to be specified.   */
+    uint32_t right;                                       /**< [in]: X coordinate of the bottom right corner of the rectangular area to be specified. */
+    uint32_t bottom;                                      /**< [in]: Y coordinate of the bottom right corner of the rectangular area to be specified. */
+} NVENC_RECT;
+
+#endif // _WIN32
+
+/** @} */ /* End of GUID and NVENC_RECT structure grouping*/
+
+typedef void* NV_ENC_INPUT_PTR;             /**< NVENCODE API input buffer                              */
+typedef void* NV_ENC_OUTPUT_PTR;            /**< NVENCODE API output buffer*/
+typedef void* NV_ENC_REGISTERED_PTR;        /**< A Resource that has been registered with NVENCODE API*/
+
+#define NVENCAPI_MAJOR_VERSION 7
+#define NVENCAPI_MINOR_VERSION 1
+
+#define NVENCAPI_VERSION (NVENCAPI_MAJOR_VERSION | (NVENCAPI_MINOR_VERSION << 24))
+
+/**
+ * Macro to generate per-structure version for use with API.
+ */
+#define NVENCAPI_STRUCT_VERSION(ver) ((uint32_t)NVENCAPI_VERSION | ((ver)<<16) | (0x7 << 28))
+
+
+#define NVENC_INFINITE_GOPLENGTH  0xffffffff
+
+#define NV_MAX_SEQ_HDR_LEN  (512)
+
+// =========================================================================================
+// Encode Codec GUIDS supported by the NvEncodeAPI interface.
+// =========================================================================================
+
+// {6BC82762-4E63-4ca4-AA85-1E50F321F6BF}
+static const GUID NV_ENC_CODEC_H264_GUID =
+{ 0x6bc82762, 0x4e63, 0x4ca4, { 0xaa, 0x85, 0x1e, 0x50, 0xf3, 0x21, 0xf6, 0xbf } };
+
+// {790CDC88-4522-4d7b-9425-BDA9975F7603}
+static const GUID NV_ENC_CODEC_HEVC_GUID = 
+{ 0x790cdc88, 0x4522, 0x4d7b, { 0x94, 0x25, 0xbd, 0xa9, 0x97, 0x5f, 0x76, 0x3 } };
+
+
+
+// =========================================================================================
+// *   Encode Profile GUIDS supported by the NvEncodeAPI interface.
+// =========================================================================================
+
+// {BFD6F8E7-233C-4341-8B3E-4818523803F4}
+static const GUID NV_ENC_CODEC_PROFILE_AUTOSELECT_GUID = 
+{ 0xbfd6f8e7, 0x233c, 0x4341, { 0x8b, 0x3e, 0x48, 0x18, 0x52, 0x38, 0x3, 0xf4 } };
+
+// {0727BCAA-78C4-4c83-8C2F-EF3DFF267C6A}
+static const GUID  NV_ENC_H264_PROFILE_BASELINE_GUID =
+{ 0x727bcaa, 0x78c4, 0x4c83, { 0x8c, 0x2f, 0xef, 0x3d, 0xff, 0x26, 0x7c, 0x6a } };
+
+// {60B5C1D4-67FE-4790-94D5-C4726D7B6E6D}
+static const GUID  NV_ENC_H264_PROFILE_MAIN_GUID =
+{ 0x60b5c1d4, 0x67fe, 0x4790, { 0x94, 0xd5, 0xc4, 0x72, 0x6d, 0x7b, 0x6e, 0x6d } };
+
+// {E7CBC309-4F7A-4b89-AF2A-D537C92BE310}
+static const GUID NV_ENC_H264_PROFILE_HIGH_GUID =
+{ 0xe7cbc309, 0x4f7a, 0x4b89, { 0xaf, 0x2a, 0xd5, 0x37, 0xc9, 0x2b, 0xe3, 0x10 } };
+
+// {7AC663CB-A598-4960-B844-339B261A7D52}
+static const GUID  NV_ENC_H264_PROFILE_HIGH_444_GUID = 
+{ 0x7ac663cb, 0xa598, 0x4960, { 0xb8, 0x44, 0x33, 0x9b, 0x26, 0x1a, 0x7d, 0x52 } };
+
+// {40847BF5-33F7-4601-9084-E8FE3C1DB8B7}
+static const GUID NV_ENC_H264_PROFILE_STEREO_GUID =
+{ 0x40847bf5, 0x33f7, 0x4601, { 0x90, 0x84, 0xe8, 0xfe, 0x3c, 0x1d, 0xb8, 0xb7 } };
+
+// {CE788D20-AAA9-4318-92BB-AC7E858C8D36}
+static const GUID NV_ENC_H264_PROFILE_SVC_TEMPORAL_SCALABILTY =
+{ 0xce788d20, 0xaaa9, 0x4318, { 0x92, 0xbb, 0xac, 0x7e, 0x85, 0x8c, 0x8d, 0x36 } };
+
+// {B405AFAC-F32B-417B-89C4-9ABEED3E5978}
+static const GUID NV_ENC_H264_PROFILE_PROGRESSIVE_HIGH_GUID = 
+{ 0xb405afac, 0xf32b, 0x417b, { 0x89, 0xc4, 0x9a, 0xbe, 0xed, 0x3e, 0x59, 0x78 } };
+
+// {AEC1BD87-E85B-48f2-84C3-98BCA6285072}
+static const GUID NV_ENC_H264_PROFILE_CONSTRAINED_HIGH_GUID = 
+{ 0xaec1bd87, 0xe85b, 0x48f2, { 0x84, 0xc3, 0x98, 0xbc, 0xa6, 0x28, 0x50, 0x72 } };
+
+// {B514C39A-B55B-40fa-878F-F1253B4DFDEC}
+static const GUID NV_ENC_HEVC_PROFILE_MAIN_GUID = 
+{ 0xb514c39a, 0xb55b, 0x40fa, { 0x87, 0x8f, 0xf1, 0x25, 0x3b, 0x4d, 0xfd, 0xec } };
+
+// {fa4d2b6c-3a5b-411a-8018-0a3f5e3c9be5}
+static const GUID NV_ENC_HEVC_PROFILE_MAIN10_GUID = 
+{ 0xfa4d2b6c, 0x3a5b, 0x411a, { 0x80, 0x18, 0x0a, 0x3f, 0x5e, 0x3c, 0x9b, 0xe5 } };
+
+// For HEVC Main 444 8 bit and HEVC Main 444 10 bit profiles only
+// {51ec32b5-1b4c-453c-9cbd-b616bd621341}
+static const GUID NV_ENC_HEVC_PROFILE_FREXT_GUID = 
+{ 0x51ec32b5, 0x1b4c, 0x453c, { 0x9c, 0xbd, 0xb6, 0x16, 0xbd, 0x62, 0x13, 0x41 } };
+
+// =========================================================================================
+// *   Preset GUIDS supported by the NvEncodeAPI interface.
+// =========================================================================================
+// {B2DFB705-4EBD-4C49-9B5F-24A777D3E587}
+static const GUID NV_ENC_PRESET_DEFAULT_GUID =
+{ 0xb2dfb705, 0x4ebd, 0x4c49, { 0x9b, 0x5f, 0x24, 0xa7, 0x77, 0xd3, 0xe5, 0x87 } };
+
+// {60E4C59F-E846-4484-A56D-CD45BE9FDDF6}
+static const GUID NV_ENC_PRESET_HP_GUID =
+{ 0x60e4c59f, 0xe846, 0x4484, { 0xa5, 0x6d, 0xcd, 0x45, 0xbe, 0x9f, 0xdd, 0xf6 } };
+
+// {34DBA71D-A77B-4B8F-9C3E-B6D5DA24C012}
+static const GUID NV_ENC_PRESET_HQ_GUID =
+{ 0x34dba71d, 0xa77b, 0x4b8f, { 0x9c, 0x3e, 0xb6, 0xd5, 0xda, 0x24, 0xc0, 0x12 } };
+
+// {82E3E450-BDBB-4e40-989C-82A90DF9EF32}
+static const GUID NV_ENC_PRESET_BD_GUID  = 
+{ 0x82e3e450, 0xbdbb, 0x4e40, { 0x98, 0x9c, 0x82, 0xa9, 0xd, 0xf9, 0xef, 0x32 } };
+
+// {49DF21C5-6DFA-4feb-9787-6ACC9EFFB726}
+static const GUID NV_ENC_PRESET_LOW_LATENCY_DEFAULT_GUID  = 
+{ 0x49df21c5, 0x6dfa, 0x4feb, { 0x97, 0x87, 0x6a, 0xcc, 0x9e, 0xff, 0xb7, 0x26 } };
+
+// {C5F733B9-EA97-4cf9-BEC2-BF78A74FD105}
+static const GUID NV_ENC_PRESET_LOW_LATENCY_HQ_GUID  = 
+{ 0xc5f733b9, 0xea97, 0x4cf9, { 0xbe, 0xc2, 0xbf, 0x78, 0xa7, 0x4f, 0xd1, 0x5 } };
+
+// {67082A44-4BAD-48FA-98EA-93056D150A58}
+static const GUID NV_ENC_PRESET_LOW_LATENCY_HP_GUID =
+{ 0x67082a44, 0x4bad, 0x48fa, { 0x98, 0xea, 0x93, 0x5, 0x6d, 0x15, 0xa, 0x58 } };
+
+// {D5BFB716-C604-44e7-9BB8-DEA5510FC3AC}
+static const GUID NV_ENC_PRESET_LOSSLESS_DEFAULT_GUID = 
+{ 0xd5bfb716, 0xc604, 0x44e7, { 0x9b, 0xb8, 0xde, 0xa5, 0x51, 0xf, 0xc3, 0xac } };
+
+// {149998E7-2364-411d-82EF-179888093409}
+static const GUID NV_ENC_PRESET_LOSSLESS_HP_GUID = 
+{ 0x149998e7, 0x2364, 0x411d, { 0x82, 0xef, 0x17, 0x98, 0x88, 0x9, 0x34, 0x9 } };
+
+/**
+ * \addtogroup ENCODER_STRUCTURE NvEncodeAPI Data structures
+ * @{
+ */
+
+/**
+ * Input frame encode modes
+ */
+typedef enum _NV_ENC_PARAMS_FRAME_FIELD_MODE
+{
+    NV_ENC_PARAMS_FRAME_FIELD_MODE_FRAME = 0x01,  /**< Frame mode */
+    NV_ENC_PARAMS_FRAME_FIELD_MODE_FIELD = 0x02,  /**< Field mode */
+    NV_ENC_PARAMS_FRAME_FIELD_MODE_MBAFF = 0x03   /**< MB adaptive frame/field */
+} NV_ENC_PARAMS_FRAME_FIELD_MODE;
+
+/**
+ * Rate Control Modes
+ */
+typedef enum _NV_ENC_PARAMS_RC_MODE
+{
+    NV_ENC_PARAMS_RC_CONSTQP                = 0x0,       /**< Constant QP mode */
+    NV_ENC_PARAMS_RC_VBR                    = 0x1,       /**< Variable bitrate mode */
+    NV_ENC_PARAMS_RC_CBR                    = 0x2,       /**< Constant bitrate mode */
+    NV_ENC_PARAMS_RC_CBR_LOWDELAY_HQ        = 0x8,       /**< low-delay CBR, high quality */
+    NV_ENC_PARAMS_RC_CBR_HQ                 = 0x10,      /**< CBR, high quality (slower) */
+    NV_ENC_PARAMS_RC_VBR_HQ                 = 0x20       /**< VBR, high quality (slower) */
+} NV_ENC_PARAMS_RC_MODE;
+
+#define NV_ENC_PARAMS_RC_VBR_MINQP              (NV_ENC_PARAMS_RC_MODE)0x4          /**< Deprecated */
+#define NV_ENC_PARAMS_RC_2_PASS_QUALITY         NV_ENC_PARAMS_RC_CBR_LOWDELAY_HQ    /**< Deprecated */
+#define NV_ENC_PARAMS_RC_2_PASS_FRAMESIZE_CAP   NV_ENC_PARAMS_RC_CBR_HQ             /**< Deprecated */
+#define NV_ENC_PARAMS_RC_2_PASS_VBR             NV_ENC_PARAMS_RC_VBR_HQ             /**< Deprecated */
+#define NV_ENC_PARAMS_RC_CBR2                   NV_ENC_PARAMS_RC_CBR                /**< Deprecated */
+
+/**
+ * Input picture structure
+ */
+typedef enum _NV_ENC_PIC_STRUCT
+{
+    NV_ENC_PIC_STRUCT_FRAME             = 0x01,                 /**< Progressive frame */
+    NV_ENC_PIC_STRUCT_FIELD_TOP_BOTTOM  = 0x02,                 /**< Field encoding top field first */
+    NV_ENC_PIC_STRUCT_FIELD_BOTTOM_TOP  = 0x03                  /**< Field encoding bottom field first */
+} NV_ENC_PIC_STRUCT;
+
+/**
+ * Input picture type
+ */
+typedef enum _NV_ENC_PIC_TYPE
+{
+    NV_ENC_PIC_TYPE_P               = 0x0,     /**< Forward predicted */
+    NV_ENC_PIC_TYPE_B               = 0x01,    /**< Bi-directionally predicted picture */
+    NV_ENC_PIC_TYPE_I               = 0x02,    /**< Intra predicted picture */
+    NV_ENC_PIC_TYPE_IDR             = 0x03,    /**< IDR picture */
+    NV_ENC_PIC_TYPE_BI              = 0x04,    /**< Bi-directionally predicted with only Intra MBs */
+    NV_ENC_PIC_TYPE_SKIPPED         = 0x05,    /**< Picture is skipped */
+    NV_ENC_PIC_TYPE_INTRA_REFRESH   = 0x06,    /**< First picture in intra refresh cycle */
+    NV_ENC_PIC_TYPE_UNKNOWN         = 0xFF     /**< Picture type unknown */
+} NV_ENC_PIC_TYPE;
+
+/**
+ * Motion vector precisions
+ */
+typedef enum _NV_ENC_MV_PRECISION
+{
+    NV_ENC_MV_PRECISION_DEFAULT     = 0x0,       /**<Driver selects QuarterPel motion vector precision by default*/
+    NV_ENC_MV_PRECISION_FULL_PEL    = 0x01,    /**< FullPel  motion vector precision */
+    NV_ENC_MV_PRECISION_HALF_PEL    = 0x02,    /**< HalfPel motion vector precision */
+    NV_ENC_MV_PRECISION_QUARTER_PEL = 0x03     /**< QuarterPel motion vector precision */
+} NV_ENC_MV_PRECISION;
+
+
+/**
+ * Input buffer formats
+ */
+typedef enum _NV_ENC_BUFFER_FORMAT
+{
+    NV_ENC_BUFFER_FORMAT_UNDEFINED                       = 0x00000000,  /**< Undefined buffer format */
+                                                                       
+    NV_ENC_BUFFER_FORMAT_NV12                            = 0x00000001,  /**< Semi-Planar YUV [Y plane followed by interleaved UV plane] */
+    NV_ENC_BUFFER_FORMAT_YV12                            = 0x00000010,  /**< Planar YUV [Y plane followed by V and U planes] */
+    NV_ENC_BUFFER_FORMAT_IYUV                            = 0x00000100,  /**< Planar YUV [Y plane followed by U and V planes] */
+    NV_ENC_BUFFER_FORMAT_YUV444                          = 0x00001000,  /**< Planar YUV [Y plane followed by U and V planes] */
+    NV_ENC_BUFFER_FORMAT_YUV420_10BIT                    = 0x00010000,  /**< 10 bit Semi-Planar YUV [Y plane followed by interleaved UV plane]. Each pixel of size 2 bytes. Most Significant 10 bits contain pixel data. */
+    NV_ENC_BUFFER_FORMAT_YUV444_10BIT                    = 0x00100000,  /**< 10 bit Planar YUV444 [Y plane followed by U and V planes]. Each pixel of size 2 bytes. Most Significant 10 bits contain pixel data.  */
+    NV_ENC_BUFFER_FORMAT_ARGB                            = 0x01000000,  /**< 8 bit Packed A8R8G8B8 */
+    NV_ENC_BUFFER_FORMAT_ARGB10                          = 0x02000000,  /**< 10 bit Packed A2R10G10B10. Each pixel of size 2 bytes. Most Significant 10 bits contain pixel data.  */
+    NV_ENC_BUFFER_FORMAT_AYUV                            = 0x04000000,  /**< 8 bit Packed A8Y8U8V8 */
+    NV_ENC_BUFFER_FORMAT_ABGR                            = 0x10000000,  /**< 8 bit Packed A8B8G8R8 */
+    NV_ENC_BUFFER_FORMAT_ABGR10                          = 0x20000000,  /**< 10 bit Packed A2B10G10R10. Each pixel of size 2 bytes. Most Significant 10 bits contain pixel data.  */
+} NV_ENC_BUFFER_FORMAT;
+
+#define NV_ENC_BUFFER_FORMAT_NV12_PL NV_ENC_BUFFER_FORMAT_NV12
+#define NV_ENC_BUFFER_FORMAT_YV12_PL NV_ENC_BUFFER_FORMAT_YV12
+#define NV_ENC_BUFFER_FORMAT_IYUV_PL NV_ENC_BUFFER_FORMAT_IYUV
+#define NV_ENC_BUFFER_FORMAT_YUV444_PL NV_ENC_BUFFER_FORMAT_YUV444
+
+/**
+ * Encoding levels
+ */
+typedef enum _NV_ENC_LEVEL
+{
+    NV_ENC_LEVEL_AUTOSELECT         = 0,
+    
+    NV_ENC_LEVEL_H264_1             = 10,
+    NV_ENC_LEVEL_H264_1b            = 9,
+    NV_ENC_LEVEL_H264_11            = 11,
+    NV_ENC_LEVEL_H264_12            = 12,
+    NV_ENC_LEVEL_H264_13            = 13,
+    NV_ENC_LEVEL_H264_2             = 20,
+    NV_ENC_LEVEL_H264_21            = 21,
+    NV_ENC_LEVEL_H264_22            = 22,
+    NV_ENC_LEVEL_H264_3             = 30,
+    NV_ENC_LEVEL_H264_31            = 31,
+    NV_ENC_LEVEL_H264_32            = 32,
+    NV_ENC_LEVEL_H264_4             = 40,
+    NV_ENC_LEVEL_H264_41            = 41,
+    NV_ENC_LEVEL_H264_42            = 42,
+    NV_ENC_LEVEL_H264_5             = 50,
+    NV_ENC_LEVEL_H264_51            = 51,
+    NV_ENC_LEVEL_H264_52            = 52,
+
+
+    NV_ENC_LEVEL_HEVC_1             = 30,
+    NV_ENC_LEVEL_HEVC_2             = 60,
+    NV_ENC_LEVEL_HEVC_21            = 63,
+    NV_ENC_LEVEL_HEVC_3             = 90,
+    NV_ENC_LEVEL_HEVC_31            = 93,
+    NV_ENC_LEVEL_HEVC_4             = 120,
+    NV_ENC_LEVEL_HEVC_41            = 123,
+    NV_ENC_LEVEL_HEVC_5             = 150,
+    NV_ENC_LEVEL_HEVC_51            = 153,
+    NV_ENC_LEVEL_HEVC_52            = 156,
+    NV_ENC_LEVEL_HEVC_6             = 180,
+    NV_ENC_LEVEL_HEVC_61            = 183,
+    NV_ENC_LEVEL_HEVC_62            = 186,
+
+    NV_ENC_TIER_HEVC_MAIN           = 0,
+    NV_ENC_TIER_HEVC_HIGH           = 1
+} NV_ENC_LEVEL;
+
+/**
+ * Error Codes
+ */
+typedef enum _NVENCSTATUS
+{
+    /**
+     * This indicates that API call returned with no errors.
+     */
+    NV_ENC_SUCCESS,
+
+    /**
+     * This indicates that no encode capable devices were detected.
+     */
+    NV_ENC_ERR_NO_ENCODE_DEVICE,
+
+    /**
+     * This indicates that devices pass by the client is not supported.
+     */
+    NV_ENC_ERR_UNSUPPORTED_DEVICE,
+
+    /**
+     * This indicates that the encoder device supplied by the client is not 
+     * valid.
+     */
+    NV_ENC_ERR_INVALID_ENCODERDEVICE,
+
+    /**
+     * This indicates that device passed to the API call is invalid.
+     */
+    NV_ENC_ERR_INVALID_DEVICE,
+
+    /**
+     * This indicates that device passed to the API call is no longer available and 
+     * needs to be reinitialized. The clients need to destroy the current encoder  
+     * session by freeing the allocated input output buffers and destroying the device 
+     * and create a new encoding session.
+     */
+    NV_ENC_ERR_DEVICE_NOT_EXIST,
+
+    /**
+     * This indicates that one or more of the pointers passed to the API call
+     * is invalid.
+     */
+    NV_ENC_ERR_INVALID_PTR,
+
+    /**
+     * This indicates that completion event passed in ::NvEncEncodePicture() call
+     * is invalid.
+     */
+    NV_ENC_ERR_INVALID_EVENT,
+
+    /**
+     * This indicates that one or more of the parameter passed to the API call
+     * is invalid.
+     */
+    NV_ENC_ERR_INVALID_PARAM,
+
+    /**
+     * This indicates that an API call was made in wrong sequence/order.
+     */
+    NV_ENC_ERR_INVALID_CALL,
+
+    /**
+     * This indicates that the API call failed because it was unable to allocate 
+     * enough memory to perform the requested operation.
+     */
+    NV_ENC_ERR_OUT_OF_MEMORY,
+    
+    /**
+     * This indicates that the encoder has not been initialized with
+     * ::NvEncInitializeEncoder() or that initialization has failed.
+     * The client cannot allocate input or output buffers or do any encoding
+     * related operation before successfully initializing the encoder.
+     */
+    NV_ENC_ERR_ENCODER_NOT_INITIALIZED,
+
+    /**
+     * This indicates that an unsupported parameter was passed by the client.
+     */
+    NV_ENC_ERR_UNSUPPORTED_PARAM,
+
+    /**
+     * This indicates that the ::NvEncLockBitstream() failed to lock the output 
+     * buffer. This happens when the client makes a non blocking lock call to 
+     * access the output bitstream by passing NV_ENC_LOCK_BITSTREAM::doNotWait flag.
+     * This is not a fatal error and client should retry the same operation after
+     * few milliseconds.
+     */
+    NV_ENC_ERR_LOCK_BUSY,
+
+    /**
+     * This indicates that the size of the user buffer passed by the client is 
+     * insufficient for the requested operation.
+     */
+    NV_ENC_ERR_NOT_ENOUGH_BUFFER,
+
+    /**
+     * This indicates that an invalid struct version was used by the client.
+     */
+    NV_ENC_ERR_INVALID_VERSION,
+
+    /**
+     * This indicates that ::NvEncMapInputResource() API failed to map the client
+     * provided input resource.
+     */
+    NV_ENC_ERR_MAP_FAILED,
+
+    /**
+     * This indicates encode driver requires more input buffers to produce an output
+     * bitstream. If this error is returned from ::NvEncEncodePicture() API, this
+     * is not a fatal error. If the client is encoding with B frames then,
+     * ::NvEncEncodePicture() API might be buffering the input frame for re-ordering. 
+     * 
+     * A client operating in synchronous mode cannot call ::NvEncLockBitstream()
+     * API on the output bitstream buffer if ::NvEncEncodePicture() returned the 
+     * ::NV_ENC_ERR_NEED_MORE_INPUT error code.
+     * The client must continue providing input frames until encode driver returns
+     * ::NV_ENC_SUCCESS. After receiving ::NV_ENC_SUCCESS status the client can call
+     * ::NvEncLockBitstream() API on the output buffers in the same order in which
+     * it has called ::NvEncEncodePicture().
+     */
+    NV_ENC_ERR_NEED_MORE_INPUT,
+
+    /**
+     * This indicates that the HW encoder is busy encoding and is unable to encode  
+     * the input. The client should call ::NvEncEncodePicture() again after few
+     * milliseconds.
+     */
+    NV_ENC_ERR_ENCODER_BUSY,
+
+    /**
+     * This indicates that the completion event passed in ::NvEncEncodePicture()
+     * API has not been registered with encoder driver using ::NvEncRegisterAsyncEvent().
+     */
+    NV_ENC_ERR_EVENT_NOT_REGISTERD,
+
+    /**
+     * This indicates that an unknown internal error has occurred.
+     */
+    NV_ENC_ERR_GENERIC,
+    
+    /**
+     * This indicates that the client is attempting to use a feature
+     * that is not available for the license type for the current system.
+     */
+    NV_ENC_ERR_INCOMPATIBLE_CLIENT_KEY,
+    
+    /**
+     * This indicates that the client is attempting to use a feature
+     * that is not implemented for the current version.
+     */
+    NV_ENC_ERR_UNIMPLEMENTED,
+
+    /**
+     * This indicates that the ::NvEncRegisterResource API failed to register the resource.
+     */
+    NV_ENC_ERR_RESOURCE_REGISTER_FAILED,
+
+    /**
+     * This indicates that the client is attempting to unregister a resource
+     * that has not been successfully registered.
+     */
+    NV_ENC_ERR_RESOURCE_NOT_REGISTERED,
+
+    /**
+     * This indicates that the client is attempting to unmap a resource
+     * that has not been successfully mapped.
+     */
+    NV_ENC_ERR_RESOURCE_NOT_MAPPED,
+
+} NVENCSTATUS;
+
+/**
+ * Encode Picture encode flags.
+ */
+typedef enum _NV_ENC_PIC_FLAGS
+{
+    NV_ENC_PIC_FLAG_FORCEINTRA         = 0x1,   /**< Encode the current picture as an Intra picture */
+    NV_ENC_PIC_FLAG_FORCEIDR           = 0x2,   /**< Encode the current picture as an IDR picture. 
+                                                     This flag is only valid when Picture type decision is taken by the Encoder
+                                                     [_NV_ENC_INITIALIZE_PARAMS::enablePTD == 1]. */
+    NV_ENC_PIC_FLAG_OUTPUT_SPSPPS      = 0x4,   /**< Write the sequence and picture header in encoded bitstream of the current picture */
+    NV_ENC_PIC_FLAG_EOS                = 0x8,   /**< Indicates end of the input stream */ 
+} NV_ENC_PIC_FLAGS;
+
+/**
+ * Memory heap to allocate input and output buffers.
+ */
+typedef enum _NV_ENC_MEMORY_HEAP
+{
+    NV_ENC_MEMORY_HEAP_AUTOSELECT      = 0, /**< Memory heap to be decided by the encoder driver based on the usage */
+    NV_ENC_MEMORY_HEAP_VID             = 1, /**< Memory heap is in local video memory */
+    NV_ENC_MEMORY_HEAP_SYSMEM_CACHED   = 2, /**< Memory heap is in cached system memory */
+    NV_ENC_MEMORY_HEAP_SYSMEM_UNCACHED = 3  /**< Memory heap is in uncached system memory */
+} NV_ENC_MEMORY_HEAP;
+
+
+/**
+ * H.264 entropy coding modes.
+ */
+typedef enum _NV_ENC_H264_ENTROPY_CODING_MODE
+{
+    NV_ENC_H264_ENTROPY_CODING_MODE_AUTOSELECT = 0x0,   /**< Entropy coding mode is auto selected by the encoder driver */
+    NV_ENC_H264_ENTROPY_CODING_MODE_CABAC      = 0x1,   /**< Entropy coding mode is CABAC */
+    NV_ENC_H264_ENTROPY_CODING_MODE_CAVLC      = 0x2    /**< Entropy coding mode is CAVLC */
+} NV_ENC_H264_ENTROPY_CODING_MODE;
+
+/**
+ * H.264 specific Bdirect modes
+ */
+typedef enum _NV_ENC_H264_BDIRECT_MODE
+{
+    NV_ENC_H264_BDIRECT_MODE_AUTOSELECT = 0x0,          /**< BDirect mode is auto selected by the encoder driver */
+    NV_ENC_H264_BDIRECT_MODE_DISABLE    = 0x1,          /**< Disable BDirect mode */
+    NV_ENC_H264_BDIRECT_MODE_TEMPORAL   = 0x2,          /**< Temporal BDirect mode */
+    NV_ENC_H264_BDIRECT_MODE_SPATIAL    = 0x3           /**< Spatial BDirect mode */
+} NV_ENC_H264_BDIRECT_MODE;
+
+/**
+ * H.264 specific FMO usage
+ */
+typedef enum _NV_ENC_H264_FMO_MODE
+{
+    NV_ENC_H264_FMO_AUTOSELECT          = 0x0,          /**< FMO usage is auto selected by the encoder driver */
+    NV_ENC_H264_FMO_ENABLE              = 0x1,          /**< Enable FMO */
+    NV_ENC_H264_FMO_DISABLE             = 0x2,          /**< Disble FMO */
+} NV_ENC_H264_FMO_MODE;
+
+/**
+ * H.264 specific Adaptive Transform modes
+ */
+typedef enum _NV_ENC_H264_ADAPTIVE_TRANSFORM_MODE
+{
+    NV_ENC_H264_ADAPTIVE_TRANSFORM_AUTOSELECT = 0x0,   /**< Adaptive Transform 8x8 mode is auto selected by the encoder driver*/
+    NV_ENC_H264_ADAPTIVE_TRANSFORM_DISABLE    = 0x1,   /**< Adaptive Transform 8x8 mode disabled */
+    NV_ENC_H264_ADAPTIVE_TRANSFORM_ENABLE     = 0x2,   /**< Adaptive Transform 8x8 mode should be used */
+} NV_ENC_H264_ADAPTIVE_TRANSFORM_MODE;
+
+/**
+ * Stereo frame packing modes.
+ */
+typedef enum _NV_ENC_STEREO_PACKING_MODE
+{
+    NV_ENC_STEREO_PACKING_MODE_NONE             = 0x0,  /**< No Stereo packing required */
+    NV_ENC_STEREO_PACKING_MODE_CHECKERBOARD     = 0x1,  /**< Checkerboard mode for packing stereo frames */
+    NV_ENC_STEREO_PACKING_MODE_COLINTERLEAVE    = 0x2,  /**< Column Interleave mode for packing stereo frames */
+    NV_ENC_STEREO_PACKING_MODE_ROWINTERLEAVE    = 0x3,  /**< Row Interleave mode for packing stereo frames */
+    NV_ENC_STEREO_PACKING_MODE_SIDEBYSIDE       = 0x4,  /**< Side-by-side mode for packing stereo frames */
+    NV_ENC_STEREO_PACKING_MODE_TOPBOTTOM        = 0x5,  /**< Top-Bottom mode for packing stereo frames */
+    NV_ENC_STEREO_PACKING_MODE_FRAMESEQ         = 0x6   /**< Frame Sequential mode for packing stereo frames */
+} NV_ENC_STEREO_PACKING_MODE;
+
+/**
+ *  Input Resource type
+ */
+typedef enum _NV_ENC_INPUT_RESOURCE_TYPE
+{
+    NV_ENC_INPUT_RESOURCE_TYPE_DIRECTX          = 0x0,   /**< input resource type is a directx9 surface*/
+    NV_ENC_INPUT_RESOURCE_TYPE_CUDADEVICEPTR    = 0x1,   /**< input resource type is a cuda device pointer surface*/
+    NV_ENC_INPUT_RESOURCE_TYPE_CUDAARRAY        = 0x2,   /**< input resource type is a cuda array surface */
+} NV_ENC_INPUT_RESOURCE_TYPE;
+
+/**
+ *  Encoder Device type
+ */
+typedef enum _NV_ENC_DEVICE_TYPE
+{
+    NV_ENC_DEVICE_TYPE_DIRECTX          = 0x0,   /**< encode device type is a directx9 device */
+    NV_ENC_DEVICE_TYPE_CUDA             = 0x1,   /**< encode device type is a cuda device */
+} NV_ENC_DEVICE_TYPE;
+
+/**
+ * Encoder capabilities enumeration.
+ */
+typedef enum _NV_ENC_CAPS
+{
+    /**
+     * Maximum number of B-Frames supported.
+     */
+    NV_ENC_CAPS_NUM_MAX_BFRAMES,
+
+    /**
+     * Rate control modes supported.
+     * \n The API return value is a bitmask of the values in NV_ENC_PARAMS_RC_MODE.
+     */
+    NV_ENC_CAPS_SUPPORTED_RATECONTROL_MODES,
+
+    /** 
+     * Indicates HW support for field mode encoding.
+     * \n 0 : Interlaced mode encoding is not supported.
+     * \n 1 : Interlaced field mode encoding is supported.
+     * \n 2 : Interlaced frame encoding and field mode encoding are both supported.
+     */
+     NV_ENC_CAPS_SUPPORT_FIELD_ENCODING,
+
+    /**
+     * Indicates HW support for monochrome mode encoding.
+     * \n 0 : Monochrome mode not supported.
+     * \n 1 : Monochrome mode supported.
+     */
+    NV_ENC_CAPS_SUPPORT_MONOCHROME,
+
+    /**
+     * Indicates HW support for FMO.
+     * \n 0 : FMO not supported.
+     * \n 1 : FMO supported.
+     */
+    NV_ENC_CAPS_SUPPORT_FMO,
+
+    /**
+     * Indicates HW capability for Quarter pel motion estimation.
+     * \n 0 : QuarterPel Motion Estimation not supported.
+     * \n 1 : QuarterPel Motion Estimation supported.
+     */
+    NV_ENC_CAPS_SUPPORT_QPELMV,
+
+    /**
+     * H.264 specific. Indicates HW support for BDirect modes.
+     * \n 0 : BDirect mode encoding not supported.
+     * \n 1 : BDirect mode encoding supported.
+     */
+    NV_ENC_CAPS_SUPPORT_BDIRECT_MODE,
+
+    /**
+     * H264 specific. Indicates HW support for CABAC entropy coding mode.
+     * \n 0 : CABAC entropy coding not supported.
+     * \n 1 : CABAC entropy coding supported.
+     */
+    NV_ENC_CAPS_SUPPORT_CABAC,
+
+    /**
+     * Indicates HW support for Adaptive Transform.
+     * \n 0 : Adaptive Transform not supported.
+     * \n 1 : Adaptive Transform supported.
+     */
+    NV_ENC_CAPS_SUPPORT_ADAPTIVE_TRANSFORM,
+
+    /**
+     * Reserved enum field.
+     */
+    NV_ENC_CAPS_SUPPORT_RESERVED,
+
+    /**
+     * Indicates HW support for encoding Temporal layers.
+     * \n 0 : Encoding Temporal layers not supported.
+     * \n 1 : Encoding Temporal layers supported.
+     */
+    NV_ENC_CAPS_NUM_MAX_TEMPORAL_LAYERS,
+
+    /**
+     * Indicates HW support for Hierarchical P frames.
+     * \n 0 : Hierarchical P frames not supported.
+     * \n 1 : Hierarchical P frames supported.
+     */
+    NV_ENC_CAPS_SUPPORT_HIERARCHICAL_PFRAMES,
+
+    /**
+     * Indicates HW support for Hierarchical B frames.
+     * \n 0 : Hierarchical B frames not supported.
+     * \n 1 : Hierarchical B frames supported.
+     */
+    NV_ENC_CAPS_SUPPORT_HIERARCHICAL_BFRAMES,
+
+    /**
+     * Maximum Encoding level supported (See ::NV_ENC_LEVEL for details).
+     */
+    NV_ENC_CAPS_LEVEL_MAX,
+ 
+    /**
+     * Minimum Encoding level supported (See ::NV_ENC_LEVEL for details).
+     */
+    NV_ENC_CAPS_LEVEL_MIN,
+
+    /**
+     * Indicates HW support for separate colour plane encoding.
+     * \n 0 : Separate colour plane encoding not supported.
+     * \n 1 : Separate colour plane encoding supported.
+     */
+    NV_ENC_CAPS_SEPARATE_COLOUR_PLANE,
+    
+    /**
+     * Maximum output width supported.
+     */
+    NV_ENC_CAPS_WIDTH_MAX,
+    
+    /**
+     * Maximum output height supported.
+     */
+    NV_ENC_CAPS_HEIGHT_MAX,
+
+    /**
+     * Indicates Temporal Scalability Support.
+     * \n 0 : Temporal SVC encoding not supported.
+     * \n 1 : Temporal SVC encoding supported.
+     */
+    NV_ENC_CAPS_SUPPORT_TEMPORAL_SVC,
+
+    /**
+     * Indicates Dynamic Encode Resolution Change Support.
+     * Support added from NvEncodeAPI version 2.0.
+     * \n 0 : Dynamic Encode Resolution Change not supported.
+     * \n 1 : Dynamic Encode Resolution Change supported.
+     */
+    NV_ENC_CAPS_SUPPORT_DYN_RES_CHANGE,
+
+    /**
+     * Indicates Dynamic Encode Bitrate Change Support.
+     * Support added from NvEncodeAPI version 2.0.
+     * \n 0 : Dynamic Encode bitrate change not supported.
+     * \n 1 : Dynamic Encode bitrate change supported.
+     */
+    NV_ENC_CAPS_SUPPORT_DYN_BITRATE_CHANGE,
+        
+    /**
+     * Indicates Forcing Constant QP On The Fly Support.
+     * Support added from NvEncodeAPI version 2.0.
+     * \n 0 : Forcing constant QP on the fly not supported.
+     * \n 1 : Forcing constant QP on the fly supported.
+     */
+    NV_ENC_CAPS_SUPPORT_DYN_FORCE_CONSTQP,
+
+    /**
+     * Indicates Dynamic rate control mode Change Support.    
+     * \n 0 : Dynamic rate control mode change not supported.
+     * \n 1 : Dynamic rate control mode change supported.
+     */
+    NV_ENC_CAPS_SUPPORT_DYN_RCMODE_CHANGE,
+
+    /**
+     * Indicates Subframe readback support for slice-based encoding.
+     * \n 0 : Subframe readback not supported.
+     * \n 1 : Subframe readback supported.
+     */
+    NV_ENC_CAPS_SUPPORT_SUBFRAME_READBACK,
+    
+    /**
+     * Indicates Constrained Encoding mode support.
+     * Support added from NvEncodeAPI version 2.0.
+     * \n 0 : Constrained encoding mode not supported.
+     * \n 1 : Constarined encoding mode supported.
+     * If this mode is supported client can enable this during initialisation.
+     * Client can then force a picture to be coded as constrained picture where
+     * each slice in a constrained picture will have constrained_intra_pred_flag set to 1
+     * and disable_deblocking_filter_idc will be set to 2 and prediction vectors for inter
+     * macroblocks in each slice will be restricted to the slice region.
+     */
+    NV_ENC_CAPS_SUPPORT_CONSTRAINED_ENCODING,
+
+    /**
+     * Indicates Intra Refresh Mode Support.
+     * Support added from NvEncodeAPI version 2.0.
+     * \n 0 : Intra Refresh Mode not supported.
+     * \n 1 : Intra Refresh Mode supported.
+     */
+    NV_ENC_CAPS_SUPPORT_INTRA_REFRESH,
+
+    /**
+     * Indicates Custom VBV Bufer Size support. It can be used for capping frame size.
+     * Support added from NvEncodeAPI version 2.0.
+     * \n 0 : Custom VBV buffer size specification from client, not supported.
+     * \n 1 : Custom VBV buffer size specification from client, supported.
+     */
+    NV_ENC_CAPS_SUPPORT_CUSTOM_VBV_BUF_SIZE,
+
+    /**
+     * Indicates Dynamic Slice Mode Support.
+     * Support added from NvEncodeAPI version 2.0.
+     * \n 0 : Dynamic Slice Mode not supported.
+     * \n 1 : Dynamic Slice Mode supported.
+     */
+    NV_ENC_CAPS_SUPPORT_DYNAMIC_SLICE_MODE,
+
+    /**
+     * Indicates Reference Picture Invalidation Support.
+     * Support added from NvEncodeAPI version 2.0.
+     * \n 0 : Reference Picture Invalidation not supported.
+     * \n 1 : Reference Picture Invalidation supported.
+     */
+    NV_ENC_CAPS_SUPPORT_REF_PIC_INVALIDATION,
+    
+    /**
+     * Indicates support for PreProcessing.
+     * The API return value is a bitmask of the values defined in ::NV_ENC_PREPROC_FLAGS
+     */
+    NV_ENC_CAPS_PREPROC_SUPPORT,
+
+    /**
+    * Indicates support Async mode.
+    * \n 0 : Async Encode mode not supported.
+    * \n 1 : Async Encode mode supported.
+    */
+    NV_ENC_CAPS_ASYNC_ENCODE_SUPPORT,
+
+    /**
+     * Maximum MBs per frame supported.
+     */
+    NV_ENC_CAPS_MB_NUM_MAX,
+
+    /**
+     * Maximum aggregate throughput in MBs per sec.
+     */
+    NV_ENC_CAPS_MB_PER_SEC_MAX,
+
+    /**
+     * Indicates HW support for YUV444 mode encoding.
+     * \n 0 : YUV444 mode encoding not supported.
+     * \n 1 : YUV444 mode encoding supported.
+     */
+    NV_ENC_CAPS_SUPPORT_YUV444_ENCODE,
+
+    /**
+     * Indicates HW support for lossless encoding.
+     * \n 0 : lossless encoding not supported.
+     * \n 1 : lossless encoding supported.
+     */
+    NV_ENC_CAPS_SUPPORT_LOSSLESS_ENCODE,
+    
+     /**
+     * Indicates HW support for Sample Adaptive Offset.
+     * \n 0 : SAO not supported.
+     * \n 1 : SAO encoding supported.
+     */
+    NV_ENC_CAPS_SUPPORT_SAO,
+
+    /**
+     * Indicates HW support for MEOnly Mode.
+     * \n 0 : MEOnly Mode not supported.
+     * \n 1 : MEOnly Mode supported.
+     */
+    NV_ENC_CAPS_SUPPORT_MEONLY_MODE,
+
+    /**
+     * Indicates HW support for lookahead encoding (enableLookahead=1).
+     * \n 0 : Lookahead not supported.
+     * \n 1 : Lookahead supported.
+     */
+    NV_ENC_CAPS_SUPPORT_LOOKAHEAD,
+
+    /**
+     * Indicates HW support for temporal AQ encoding (enableTemporalAQ=1).
+     * \n 0 : Temporal AQ not supported.
+     * \n 1 : Temporal AQ supported.
+     */
+    NV_ENC_CAPS_SUPPORT_TEMPORAL_AQ,
+    /**
+     * Indicates HW support for 10 bit encoding.
+     * \n 0 : 10 bit encoding not supported.
+     * \n 1 : 10 bit encoding supported.
+     */
+    NV_ENC_CAPS_SUPPORT_10BIT_ENCODE,
+
+    /**
+     * Reserved - Not to be used by clients.
+     */
+    NV_ENC_CAPS_EXPOSED_COUNT
+} NV_ENC_CAPS;
+
+/**
+ *  HEVC CU SIZE
+ */
+typedef enum _NV_ENC_HEVC_CUSIZE
+{
+    NV_ENC_HEVC_CUSIZE_AUTOSELECT = 0,
+    NV_ENC_HEVC_CUSIZE_8x8        = 1,
+    NV_ENC_HEVC_CUSIZE_16x16      = 2,
+    NV_ENC_HEVC_CUSIZE_32x32      = 3,
+    NV_ENC_HEVC_CUSIZE_64x64      = 4,
+}NV_ENC_HEVC_CUSIZE;
+
+/**
+ * Input struct for querying Encoding capabilities.
+ */
+typedef struct _NV_ENC_CAPS_PARAM
+{
+    uint32_t version;                                  /**< [in]: Struct version. Must be set to ::NV_ENC_CAPS_PARAM_VER */
+    NV_ENC_CAPS  capsToQuery;                          /**< [in]: Specifies the encode capability to be queried. Client should pass a member for ::NV_ENC_CAPS enum. */
+    uint32_t reserved[62];                             /**< [in]: Reserved and must be set to 0 */
+} NV_ENC_CAPS_PARAM;
+
+/** NV_ENC_CAPS_PARAM struct version. */
+#define NV_ENC_CAPS_PARAM_VER NVENCAPI_STRUCT_VERSION(1)
+
+
+/**
+ * Creation parameters for input buffer.
+ */
+typedef struct _NV_ENC_CREATE_INPUT_BUFFER
+{
+    uint32_t                  version;                 /**< [in]: Struct version. Must be set to ::NV_ENC_CREATE_INPUT_BUFFER_VER */
+    uint32_t                  width;                   /**< [in]: Input buffer width */
+    uint32_t                  height;                  /**< [in]: Input buffer width */
+    NV_ENC_MEMORY_HEAP        memoryHeap;              /**< [in]: Deprecated. Will be removed in sdk 8.0 */
+    NV_ENC_BUFFER_FORMAT      bufferFmt;               /**< [in]: Input buffer format */
+    uint32_t                  reserved;                /**< [in]: Reserved and must be set to 0 */
+    NV_ENC_INPUT_PTR          inputBuffer;             /**< [out]: Pointer to input buffer */
+    void*                     pSysMemBuffer;           /**< [in]: Pointer to existing sysmem buffer */
+    uint32_t                  reserved1[57];           /**< [in]: Reserved and must be set to 0 */
+    void*                     reserved2[63];           /**< [in]: Reserved and must be set to NULL */
+} NV_ENC_CREATE_INPUT_BUFFER;
+
+/** NV_ENC_CREATE_INPUT_BUFFER struct version. */
+#define NV_ENC_CREATE_INPUT_BUFFER_VER NVENCAPI_STRUCT_VERSION(1) 
+
+/**
+ * Creation parameters for output bitstream buffer.
+ */
+typedef struct _NV_ENC_CREATE_BITSTREAM_BUFFER
+{
+    uint32_t              version;                     /**< [in]: Struct version. Must be set to ::NV_ENC_CREATE_BITSTREAM_BUFFER_VER */
+    uint32_t              size;                        /**< [in]: Size of the bitstream buffer to be created */
+    NV_ENC_MEMORY_HEAP    memoryHeap;                  /**< [in]: Deprecated. Will be removed in sdk 8.0 */
+    uint32_t              reserved;                    /**< [in]: Reserved and must be set to 0 */
+    NV_ENC_OUTPUT_PTR     bitstreamBuffer;             /**< [out]: Pointer to the output bitstream buffer */
+    void*                 bitstreamBufferPtr;          /**< [out]: Reserved and should not be used */
+    uint32_t              reserved1[58];               /**< [in]: Reserved and should be set to 0 */
+    void*                 reserved2[64];               /**< [in]: Reserved and should be set to NULL */
+} NV_ENC_CREATE_BITSTREAM_BUFFER;
+
+/** NV_ENC_CREATE_BITSTREAM_BUFFER struct version. */
+#define NV_ENC_CREATE_BITSTREAM_BUFFER_VER NVENCAPI_STRUCT_VERSION(1)
+
+/**
+ * Structs needed for ME only mode. 
+ */
+typedef struct _NV_ENC_MVECTOR
+{
+    int16_t             mvx;               /**< the x component of MV in qpel units */
+    int16_t             mvy;               /**< the y component of MV in qpel units */
+} NV_ENC_MVECTOR;
+
+/** 
+ * Motion vector structure per macroblock for H264 motion estimation.
+ */
+typedef struct _NV_ENC_H264_MV_DATA
+{
+    NV_ENC_MVECTOR      mv[4];             /**< up to 4 vectors for 8x8 partition */
+    uint8_t             mbType;            /**< 0 (I), 1 (P), 2 (IPCM), 3 (B) */
+    uint8_t             partitionType;     /**< Specifies the block partition type. 0:16x16, 1:8x8, 2:16x8, 3:8x16 */
+    uint16_t            reserved;          /**< reserved padding for alignment */
+    uint32_t            mbCost;
+} NV_ENC_H264_MV_DATA;
+
+/**
+ * Motion vector structure per CU for HEVC motion estimation.
+ */
+typedef struct _NV_ENC_HEVC_MV_DATA
+{
+    NV_ENC_MVECTOR    mv[4];               /**< up to 4 vectors within a CU */
+    uint8_t           cuType;              /**< 0 (I), 1(P), 2 (Skip) */
+    uint8_t           cuSize;              /**< 0: 8x8, 1: 16x16, 2: 32x32, 3: 64x64 */
+    uint8_t           partitionMode;       /**< The CU partition mode
+                                                0 (2Nx2N), 1 (2NxN), 2(Nx2N), 3 (NxN),
+                                                4 (2NxnU), 5 (2NxnD), 6(nLx2N), 7 (nRx2N) */
+    uint8_t           lastCUInCTB;         /**< Marker to separate CUs in the current CTB from CUs in the next CTB */
+} NV_ENC_HEVC_MV_DATA;
+
+/**
+ * Creation parameters for output motion vector buffer for ME only mode.
+ */
+typedef struct _NV_ENC_CREATE_MV_BUFFER
+{
+    uint32_t            version;           /**< [in]: Struct version. Must be set to NV_ENC_CREATE_MV_BUFFER_VER */
+    NV_ENC_OUTPUT_PTR   mvBuffer;          /**< [out]: Pointer to the output motion vector buffer */
+    uint32_t            reserved1[255];    /**< [in]: Reserved and should be set to 0 */
+    void*               reserved2[63];     /**< [in]: Reserved and should be set to NULL */
+} NV_ENC_CREATE_MV_BUFFER;
+
+/** NV_ENC_CREATE_MV_BUFFER struct version*/
+#define NV_ENC_CREATE_MV_BUFFER_VER NVENCAPI_STRUCT_VERSION(1)
+
+/** 
+ * QP value for frames
+ */
+typedef struct _NV_ENC_QP
+{
+    uint32_t        qpInterP;
+    uint32_t        qpInterB;
+    uint32_t        qpIntra;
+} NV_ENC_QP;
+
+/**
+ * Rate Control Configuration Paramters
+ */
+ typedef struct _NV_ENC_RC_PARAMS
+ {
+    uint32_t                        version;
+    NV_ENC_PARAMS_RC_MODE           rateControlMode;                             /**< [in]: Specifies the rate control mode. Check support for various rate control modes using ::NV_ENC_CAPS_SUPPORTED_RATECONTROL_MODES caps. */
+    NV_ENC_QP                       constQP;                                     /**< [in]: Specifies the initial QP to be used for encoding, these values would be used for all frames if in Constant QP mode. */
+    uint32_t                        averageBitRate;                              /**< [in]: Specifies the average bitrate(in bits/sec) used for encoding. */
+    uint32_t                        maxBitRate;                                  /**< [in]: Specifies the maximum bitrate for the encoded output. This is used for VBR and ignored for CBR mode. */
+    uint32_t                        vbvBufferSize;                               /**< [in]: Specifies the VBV(HRD) buffer size. in bits. Set 0 to use the default VBV  buffer size. */
+    uint32_t                        vbvInitialDelay;                             /**< [in]: Specifies the VBV(HRD) initial delay in bits. Set 0 to use the default VBV  initial delay .*/
+    uint32_t                        enableMinQP          :1;                     /**< [in]: Set this to 1 if minimum QP used for rate control. */
+    uint32_t                        enableMaxQP          :1;                     /**< [in]: Set this to 1 if maximum QP used for rate control. */
+    uint32_t                        enableInitialRCQP    :1;                     /**< [in]: Set this to 1 if user suppplied initial QP is used for rate control. */
+    uint32_t                        enableAQ             :1;                     /**< [in]: Set this to 1 to enable adaptive quantization (Spatial). */
+    uint32_t                        enableExtQPDeltaMap  :1;                     /**< [in]: Set this to 1 to enable additional QP modifier for each MB supplied by client though signed byte array pointed to by NV_ENC_PIC_PARAMS::qpDeltaMap (Not Supported when AQ(Spatial/Temporal) is enabled) */
+    uint32_t                        enableLookahead      :1;                     /**< [in]: Set this to 1 to enable lookahead with depth <lookaheadDepth> (if lookahead is enabled, input frames must remain available to the encoder until encode completion) */
+    uint32_t                        disableIadapt        :1;                     /**< [in]: Set this to 1 to disable adaptive I-frame insertion at scene cuts (only has an effect when lookahead is enabled) */
+    uint32_t                        disableBadapt        :1;                     /**< [in]: Set this to 1 to disable adaptive B-frame decision (only has an effect when lookahead is enabled) */
+    uint32_t                        enableTemporalAQ     :1;                     /**< [in]: Set this to 1 to enable temporal AQ for H.264 */
+    uint32_t                        zeroReorderDelay     :1;                     /**< [in]: Set this to 1 to indicate zero latency operation (no reordering delay, num_reorder_frames=0) */
+    uint32_t                        enableNonRefP        :1;                     /**< [in]: Set this to 1 to enable automatic insertion of non-reference P-frames (no effect if enablePTD=0) */
+    uint32_t                        strictGOPTarget      :1;                     /**< [in]: Set this to 1 to minimize GOP-to-GOP rate fluctuations */
+    uint32_t                        aqStrength           :4;                     /**< [in]: When AQ (Spatial) is enabled (i.e. NV_ENC_RC_PARAMS::enableAQ is set), this field is used to specify AQ strength. AQ strength scale is from 1 (low) - 15 (aggressive). If not set, strength is autoselected by driver. Currently supported only with h264 */
+    uint32_t                        reservedBitFields    :16;                    /**< [in]: Reserved bitfields and must be set to 0 */
+    NV_ENC_QP                       minQP;                                       /**< [in]: Specifies the minimum QP used for rate control. Client must set NV_ENC_CONFIG::enableMinQP to 1. */
+    NV_ENC_QP                       maxQP;                                       /**< [in]: Specifies the maximum QP used for rate control. Client must set NV_ENC_CONFIG::enableMaxQP to 1. */
+    NV_ENC_QP                       initialRCQP;                                 /**< [in]: Specifies the initial QP used for rate control. Client must set NV_ENC_CONFIG::enableInitialRCQP to 1. */
+    uint32_t                        temporallayerIdxMask;                        /**< [in]: Specifies the temporal layers (as a bitmask) whose QPs have changed. Valid max bitmask is [2^NV_ENC_CAPS_NUM_MAX_TEMPORAL_LAYERS - 1] */
+    uint8_t                         temporalLayerQP[8];                          /**< [in]: Specifies the temporal layer QPs used for rate control. Temporal layer index is used as as the array index */
+    uint16_t                        targetQuality;                               /**< [in]: Target CQ (Constant Quality) level for VBR mode (range 0-51 with 0-automatic)  */
+    uint16_t                        lookaheadDepth;                              /**< [in]: Maximum depth of lookahead with range 0-32 (only used if enableLookahead=1) */
+    uint32_t                        reserved[9];
+ } NV_ENC_RC_PARAMS;
+ 
+/** macro for constructing the version field of ::_NV_ENC_RC_PARAMS */
+#define NV_ENC_RC_PARAMS_VER NVENCAPI_STRUCT_VERSION(1)
+ 
+
+
+/**
+ * \struct _NV_ENC_CONFIG_H264_VUI_PARAMETERS
+ * H264 Video Usability Info parameters
+ */
+typedef struct _NV_ENC_CONFIG_H264_VUI_PARAMETERS
+{
+    uint32_t    overscanInfoPresentFlag;              /**< [in]: if set to 1 , it specifies that the overscanInfo is present */
+    uint32_t    overscanInfo;                         /**< [in]: Specifies the overscan info(as defined in Annex E of the ITU-T Specification). */
+    uint32_t    videoSignalTypePresentFlag;           /**< [in]: If set to 1, it specifies  that the videoFormat, videoFullRangeFlag and colourDescriptionPresentFlag are present. */
+    uint32_t    videoFormat;                          /**< [in]: Specifies the source video format(as defined in Annex E of the ITU-T Specification).*/
+    uint32_t    videoFullRangeFlag;                   /**< [in]: Specifies the output range of the luma and chroma samples(as defined in Annex E of the ITU-T Specification). */
+    uint32_t    colourDescriptionPresentFlag;         /**< [in]: If set to 1, it specifies that the colourPrimaries, transferCharacteristics and colourMatrix are present. */
+    uint32_t    colourPrimaries;                      /**< [in]: Specifies color primaries for converting to RGB(as defined in Annex E of the ITU-T Specification) */
+    uint32_t    transferCharacteristics;              /**< [in]: Specifies the opto-electronic transfer characteristics to use (as defined in Annex E of the ITU-T Specification) */
+    uint32_t    colourMatrix;                         /**< [in]: Specifies the matrix coefficients used in deriving the luma and chroma from the RGB primaries (as defined in Annex E of the ITU-T Specification). */
+    uint32_t    chromaSampleLocationFlag;             /**< [in]: if set to 1 , it specifies that the chromaSampleLocationTop and chromaSampleLocationBot are present.*/
+    uint32_t    chromaSampleLocationTop;              /**< [in]: Specifies the chroma sample location for top field(as defined in Annex E of the ITU-T Specification) */
+    uint32_t    chromaSampleLocationBot;              /**< [in]: Specifies the chroma sample location for bottom field(as defined in Annex E of the ITU-T Specification) */
+    uint32_t    bitstreamRestrictionFlag;             /**< [in]: if set to 1, it specifies the bitstream restriction parameters are present in the bitstream.*/
+    uint32_t    reserved[15];
+}NV_ENC_CONFIG_H264_VUI_PARAMETERS;
+
+typedef NV_ENC_CONFIG_H264_VUI_PARAMETERS NV_ENC_CONFIG_HEVC_VUI_PARAMETERS;
+
+/**
+ * \struct _NVENC_EXTERNAL_ME_HINT_COUNTS_PER_BLOCKTYPE
+ * External motion vector hint counts per block type.
+ */
+typedef struct _NVENC_EXTERNAL_ME_HINT_COUNTS_PER_BLOCKTYPE
+{
+    uint32_t   numCandsPerBlk16x16                   : 4;   /**< [in]: Specifies the number of candidates per 16x16 block. */
+    uint32_t   numCandsPerBlk16x8                    : 4;   /**< [in]: Specifies the number of candidates per 16x8 block. */
+    uint32_t   numCandsPerBlk8x16                    : 4;   /**< [in]: Specifies the number of candidates per 8x16 block. */
+    uint32_t   numCandsPerBlk8x8                     : 4;   /**< [in]: Specifies the number of candidates per 8x8 block. */
+    uint32_t   reserved                              : 16;  /**< [in]: Reserved for padding. */
+    uint32_t   reserved1[3];                                /**< [in]: Reserved for future use. */
+} NVENC_EXTERNAL_ME_HINT_COUNTS_PER_BLOCKTYPE;
+
+
+/**
+ * \struct _NVENC_EXTERNAL_ME_HINT
+ * External Motion Vector hint structure.
+ */
+typedef struct _NVENC_EXTERNAL_ME_HINT
+{
+    int32_t    mvx         : 12;                        /**< [in]: Specifies the x component of integer pixel MV (relative to current MB) S12.0. */
+    int32_t    mvy         : 10;                        /**< [in]: Specifies the y component of integer pixel MV (relative to current MB) S10.0 .*/
+    int32_t    refidx      : 5;                         /**< [in]: Specifies the reference index (31=invalid). Current we support only 1 reference frame per direction for external hints, so \p refidx must be 0. */
+    int32_t    dir         : 1;                         /**< [in]: Specifies the direction of motion estimation . 0=L0 1=L1.*/
+    int32_t    partType    : 2;                         /**< [in]: Specifies the block partition type.0=16x16 1=16x8 2=8x16 3=8x8 (blocks in partition must be consecutive).*/
+    int32_t    lastofPart  : 1;                         /**< [in]: Set to 1 for the last MV of (sub) partition  */
+    int32_t    lastOfMB    : 1;                         /**< [in]: Set to 1 for the last MV of macroblock. */
+} NVENC_EXTERNAL_ME_HINT;
+
+
+/**
+ * \struct _NV_ENC_CONFIG_H264
+ * H264 encoder configuration parameters
+ */
+typedef struct _NV_ENC_CONFIG_H264
+{
+    uint32_t enableTemporalSVC         :1;                          /**< [in]: Set to 1 to enable SVC temporal*/
+    uint32_t enableStereoMVC           :1;                          /**< [in]: Set to 1 to enable stereo MVC*/
+    uint32_t hierarchicalPFrames       :1;                          /**< [in]: Set to 1 to enable hierarchical PFrames */
+    uint32_t hierarchicalBFrames       :1;                          /**< [in]: Set to 1 to enable hierarchical BFrames */
+    uint32_t outputBufferingPeriodSEI  :1;                          /**< [in]: Set to 1 to write SEI buffering period syntax in the bitstream */
+    uint32_t outputPictureTimingSEI    :1;                          /**< [in]: Set to 1 to write SEI picture timing syntax in the bitstream.  When set for following rateControlMode : NV_ENC_PARAMS_RC_CBR, NV_ENC_PARAMS_RC_CBR_LOWDELAY_HQ,
+                                                                               NV_ENC_PARAMS_RC_CBR_HQ, filler data is inserted if needed to achieve hrd bitrate */ 
+    uint32_t outputAUD                 :1;                          /**< [in]: Set to 1 to write access unit delimiter syntax in bitstream */
+    uint32_t disableSPSPPS             :1;                          /**< [in]: Set to 1 to disable writing of Sequence and Picture parameter info in bitstream */
+    uint32_t outputFramePackingSEI     :1;                          /**< [in]: Set to 1 to enable writing of frame packing arrangement SEI messages to bitstream */
+    uint32_t outputRecoveryPointSEI    :1;                          /**< [in]: Set to 1 to enable writing of recovery point SEI message */
+    uint32_t enableIntraRefresh        :1;                          /**< [in]: Set to 1 to enable gradual decoder refresh or intra refresh. If the GOP structure uses B frames this will be ignored */
+    uint32_t enableConstrainedEncoding :1;                          /**< [in]: Set this to 1 to enable constrainedFrame encoding where each slice in the constarined picture is independent of other slices
+                                                                               Check support for constrained encoding using ::NV_ENC_CAPS_SUPPORT_CONSTRAINED_ENCODING caps. */
+    uint32_t repeatSPSPPS              :1;                          /**< [in]: Set to 1 to enable writing of Sequence and Picture parameter for every IDR frame */
+    uint32_t enableVFR                 :1;                          /**< [in]: Set to 1 to enable variable frame rate. */
+    uint32_t enableLTR                 :1;                          /**< [in]: Currently this feature is not available and must be set to 0. Set to 1 to enable LTR support and auto-mark the first */
+    uint32_t qpPrimeYZeroTransformBypassFlag :1;                    /**< [in]: To enable lossless encode set this to 1, set QP to 0 and RC_mode to NV_ENC_PARAMS_RC_CONSTQP and profile to HIGH_444_PREDICTIVE_PROFILE.
+                                                                               Check support for lossless encoding using ::NV_ENC_CAPS_SUPPORT_LOSSLESS_ENCODE caps.  */
+    uint32_t useConstrainedIntraPred   :1;                          /**< [in]: Set 1 to enable constrained intra prediction. */
+    uint32_t reservedBitFields         :15;                         /**< [in]: Reserved bitfields and must be set to 0 */
+    uint32_t level;                                                 /**< [in]: Specifies the encoding level. Client is recommended to set this to NV_ENC_LEVEL_AUTOSELECT in order to enable the NvEncodeAPI interface to select the correct level. */
+    uint32_t idrPeriod;                                             /**< [in]: Specifies the IDR interval. If not set, this is made equal to gopLength in NV_ENC_CONFIG.Low latency application client can set IDR interval to NVENC_INFINITE_GOPLENGTH so that IDR frames are not inserted automatically. */
+    uint32_t separateColourPlaneFlag;                               /**< [in]: Set to 1 to enable 4:4:4 separate colour planes */
+    uint32_t disableDeblockingFilterIDC;                            /**< [in]: Specifies the deblocking filter mode. Permissible value range: [0,2] */
+    uint32_t numTemporalLayers;                                     /**< [in]: Specifies max temporal layers to be used for hierarchical coding. Valid value range is [1,::NV_ENC_CAPS_NUM_MAX_TEMPORAL_LAYERS] */
+    uint32_t spsId;                                                 /**< [in]: Specifies the SPS id of the sequence header. Currently reserved and must be set to 0. */
+    uint32_t ppsId;                                                 /**< [in]: Specifies the PPS id of the picture header. Currently reserved and must be set to 0. */
+    NV_ENC_H264_ADAPTIVE_TRANSFORM_MODE adaptiveTransformMode;      /**< [in]: Specifies the AdaptiveTransform Mode. Check support for AdaptiveTransform mode using ::NV_ENC_CAPS_SUPPORT_ADAPTIVE_TRANSFORM caps. */
+    NV_ENC_H264_FMO_MODE                fmoMode;                    /**< [in]: Specified the FMO Mode. Check support for FMO using ::NV_ENC_CAPS_SUPPORT_FMO caps. */
+    NV_ENC_H264_BDIRECT_MODE            bdirectMode;                /**< [in]: Specifies the BDirect mode. Check support for BDirect mode using ::NV_ENC_CAPS_SUPPORT_BDIRECT_MODE caps.*/
+    NV_ENC_H264_ENTROPY_CODING_MODE     entropyCodingMode;          /**< [in]: Specifies the entropy coding mode. Check support for CABAC mode using ::NV_ENC_CAPS_SUPPORT_CABAC caps. */
+    NV_ENC_STEREO_PACKING_MODE          stereoMode;                 /**< [in]: Specifies the stereo frame packing mode which is to be signalled in frame packing arrangement SEI */
+    uint32_t                            intraRefreshPeriod;         /**< [in]: Specifies the interval between successive intra refresh if enableIntrarefresh is set. Requires enableIntraRefresh to be set.
+                                                                               Will be disabled if NV_ENC_CONFIG::gopLength is not set to NVENC_INFINITE_GOPLENGTH. */
+    uint32_t                            intraRefreshCnt;            /**< [in]: Specifies the length of intra refresh in number of frames for periodic intra refresh. This value should be smaller than intraRefreshPeriod */
+    uint32_t                            maxNumRefFrames;            /**< [in]: Specifies the DPB size used for encoding. Setting it to 0 will let driver use the default dpb size. 
+                                                                               The low latency application which wants to invalidate reference frame as an error resilience tool
+                                                                               is recommended to use a large DPB size so that the encoder can keep old reference frames which can be used if recent
+                                                                               frames are invalidated. */
+    uint32_t                            sliceMode;                  /**< [in]: This parameter in conjunction with sliceModeData specifies the way in which the picture is divided into slices
+                                                                               sliceMode = 0 MB based slices, sliceMode = 1 Byte based slices, sliceMode = 2 MB row based slices, sliceMode = 3, numSlices in Picture
+                                                                               When forceIntraRefreshWithFrameCnt is set it will have priority over sliceMode setting
+                                                                               When sliceMode == 0 and sliceModeData == 0 whole picture will be coded with one slice */
+    uint32_t                            sliceModeData;              /**< [in]: Specifies the parameter needed for sliceMode. For:
+                                                                               sliceMode = 0, sliceModeData specifies # of MBs in each slice (except last slice)
+                                                                               sliceMode = 1, sliceModeData specifies maximum # of bytes in each slice (except last slice)
+                                                                               sliceMode = 2, sliceModeData specifies # of MB rows in each slice (except last slice)
+                                                                               sliceMode = 3, sliceModeData specifies number of slices in the picture. Driver will divide picture into slices optimally */
+    NV_ENC_CONFIG_H264_VUI_PARAMETERS   h264VUIParameters;          /**< [in]: Specifies the H264 video usability info pamameters */
+    uint32_t                            ltrNumFrames;               /**< [in]: Specifies the number of LTR frames used. 
+                                                                               If ltrTrustMode=1, encoder will mark first numLTRFrames base layer reference frames within each IDR interval as LTR.
+                                                                               If ltrMarkFrame=1, ltrNumFrames specifies maximum number of ltr frames in DPB.
+                                                                               If ltrNumFrames value is more that DPB size(maxNumRefFrames) encoder will take decision on its own. */
+    uint32_t                            ltrTrustMode;               /**< [in]: Specifies the LTR operating mode. 
+                                                                               Set to 0 to disallow encoding using LTR frames until later specified.
+                                                                               Set to 1 to allow encoding using LTR frames unless later invalidated.*/
+    uint32_t                            chromaFormatIDC;            /**< [in]: Specifies the chroma format. Should be set to 1 for yuv420 input, 3 for yuv444 input.
+                                                                               Check support for YUV444 encoding using ::NV_ENC_CAPS_SUPPORT_YUV444_ENCODE caps.*/
+    uint32_t                            maxTemporalLayers;          /**< [in]: Specifies the max temporal layer used for hierarchical coding. */ 
+    uint32_t                            reserved1[270];             /**< [in]: Reserved and must be set to 0 */
+    void*                               reserved2[64];              /**< [in]: Reserved and must be set to NULL */
+} NV_ENC_CONFIG_H264;
+
+
+/**
+ * \struct _NV_ENC_CONFIG_HEVC
+ * HEVC encoder configuration parameters to be set during initialization.
+ */
+typedef struct _NV_ENC_CONFIG_HEVC
+{
+    uint32_t level;                                                 /**< [in]: Specifies the level of the encoded bitstream.*/
+    uint32_t tier;                                                  /**< [in]: Specifies the level tier of the encoded bitstream.*/
+    NV_ENC_HEVC_CUSIZE minCUSize;                                   /**< [in]: Specifies the minimum size of luma coding unit.*/
+    NV_ENC_HEVC_CUSIZE maxCUSize;                                   /**< [in]: Specifies the maximum size of luma coding unit. Currently NVENC SDK only supports maxCUSize equal to NV_ENC_HEVC_CUSIZE_32x32.*/
+    uint32_t useConstrainedIntraPred               :1;              /**< [in]: Set 1 to enable constrained intra prediction. */
+    uint32_t disableDeblockAcrossSliceBoundary     :1;              /**< [in]: Set 1 to disable in loop filtering across slice boundary.*/
+    uint32_t outputBufferingPeriodSEI              :1;              /**< [in]: Set 1 to write SEI buffering period syntax in the bitstream */
+    uint32_t outputPictureTimingSEI                :1;              /**< [in]: Set 1 to write SEI picture timing syntax in the bitstream */
+    uint32_t outputAUD                             :1;              /**< [in]: Set 1 to write Access Unit Delimiter syntax. */
+    uint32_t enableLTR                             :1;              /**< [in]: Set 1 to enable use of long term reference pictures for inter prediction. */
+    uint32_t disableSPSPPS                         :1;              /**< [in]: Set 1 to disable VPS,SPS and PPS signalling in the bitstream. */
+    uint32_t repeatSPSPPS                          :1;              /**< [in]: Set 1 to output VPS,SPS and PPS for every IDR frame.*/
+    uint32_t enableIntraRefresh                    :1;              /**< [in]: Set 1 to enable gradual decoder refresh or intra refresh. If the GOP structure uses B frames this will be ignored */
+    uint32_t chromaFormatIDC                       :2;              /**< [in]: Specifies the chroma format. Should be set to 1 for yuv420 input, 3 for yuv444 input.*/
+    uint32_t pixelBitDepthMinus8                   :3;              /**< [in]: Specifies pixel bit depth minus 8. Should be set to 0 for 8 bit input, 2 for 10 bit input.*/
+    uint32_t reserved                              :18;             /**< [in]: Reserved bitfields.*/
+    uint32_t idrPeriod;                                             /**< [in]: Specifies the IDR interval. If not set, this is made equal to gopLength in NV_ENC_CONFIG.Low latency application client can set IDR interval to NVENC_INFINITE_GOPLENGTH so that IDR frames are not inserted automatically. */
+    uint32_t intraRefreshPeriod;                                    /**< [in]: Specifies the interval between successive intra refresh if enableIntrarefresh is set. Requires enableIntraRefresh to be set.
+                                                                    Will be disabled if NV_ENC_CONFIG::gopLength is not set to NVENC_INFINITE_GOPLENGTH. */
+    uint32_t intraRefreshCnt;                                       /**< [in]: Specifies the length of intra refresh in number of frames for periodic intra refresh. This value should be smaller than intraRefreshPeriod */
+    uint32_t maxNumRefFramesInDPB;                                  /**< [in]: Specifies the maximum number of references frames in the DPB.*/
+    uint32_t ltrNumFrames;                                          /**< [in]: Specifies the number of LTR frames used. 
+                                                                               If ltrTrustMode=1, encoder will mark first numLTRFrames base layer reference frames within each IDR interval as LTR.
+                                                                               If ltrMarkFrame=1, ltrNumFrames specifies maximum number of ltr frames in DPB.
+                                                                               If ltrNumFrames value is more that DPB size(maxNumRefFramesInDPB) encoder will take decision on its own. */
+    uint32_t vpsId;                                                 /**< [in]: Specifies the VPS id of the video parameter set. Currently reserved and must be set to 0. */
+    uint32_t spsId;                                                 /**< [in]: Specifies the SPS id of the sequence header. Currently reserved and must be set to 0. */
+    uint32_t ppsId;                                                 /**< [in]: Specifies the PPS id of the picture header. Currently reserved and must be set to 0. */
+    uint32_t sliceMode;                                             /**< [in]: This parameter in conjunction with sliceModeData specifies the way in which the picture is divided into slices
+                                                                                sliceMode = 0 CTU based slices, sliceMode = 1 Byte based slices, sliceMode = 2 CTU row based slices, sliceMode = 3, numSlices in Picture
+                                                                                When sliceMode == 0 and sliceModeData == 0 whole picture will be coded with one slice */
+    uint32_t sliceModeData;                                         /**< [in]: Specifies the parameter needed for sliceMode. For:
+                                                                                sliceMode = 0, sliceModeData specifies # of CTUs in each slice (except last slice)
+                                                                                sliceMode = 1, sliceModeData specifies maximum # of bytes in each slice (except last slice)
+                                                                                sliceMode = 2, sliceModeData specifies # of CTU rows in each slice (except last slice)
+                                                                                sliceMode = 3, sliceModeData specifies number of slices in the picture. Driver will divide picture into slices optimally */
+    uint32_t maxTemporalLayersMinus1;                               /**< [in]: Specifies the max temporal layer used for hierarchical coding. */
+    NV_ENC_CONFIG_HEVC_VUI_PARAMETERS   hevcVUIParameters;          /**< [in]: Specifies the HEVC video usability info pamameters */
+    uint32_t ltrTrustMode;                                          /**< [in]: Specifies the LTR operating mode.
+                                                                               Set to 0 to disallow encoding using LTR frames until later specified.
+                                                                               Set to 1 to allow encoding using LTR frames unless later invalidated.*/    
+    uint32_t reserved1[217];                                        /**< [in]: Reserved and must be set to 0.*/
+    void*    reserved2[64];                                         /**< [in]: Reserved and must be set to NULL */
+} NV_ENC_CONFIG_HEVC;
+
+/**
+ * \struct _NV_ENC_CONFIG_H264_MEONLY
+ * H264 encoder configuration parameters for ME only Mode
+ * 
+ */
+typedef struct _NV_ENC_CONFIG_H264_MEONLY
+{
+    uint32_t disablePartition16x16 :1;                          /**< [in]: Disable MotionEstimation on 16x16 blocks*/
+    uint32_t disablePartition8x16  :1;                          /**< [in]: Disable MotionEstimation on 8x16 blocks*/
+    uint32_t disablePartition16x8  :1;                          /**< [in]: Disable MotionEstimation on 16x8 blocks*/
+    uint32_t disablePartition8x8   :1;                          /**< [in]: Disable MotionEstimation on 8x8 blocks*/
+    uint32_t disableIntraSearch    :1;                          /**< [in]: Disable Intra search during MotionEstimation*/
+    uint32_t bStereoEnable         :1;                          /**< [in]: Enable Stereo Mode for Motion Estimation where each view is independently executed*/
+    uint32_t reserved              :26;                         /**< [in]: Reserved and must be set to 0 */
+    uint32_t reserved1 [255];                                   /**< [in]: Reserved and must be set to 0 */
+    void*    reserved2[64];                                     /**< [in]: Reserved and must be set to NULL */
+} NV_ENC_CONFIG_H264_MEONLY;
+
+
+/**
+ * \struct _NV_ENC_CONFIG_HEVC_MEONLY
+ * HEVC encoder configuration parameters for ME only Mode
+ * 
+ */
+typedef struct _NV_ENC_CONFIG_HEVC_MEONLY
+{
+    uint32_t reserved [256];                                   /**< [in]: Reserved and must be set to 0 */
+    void*    reserved1[64];                                     /**< [in]: Reserved and must be set to NULL */
+} NV_ENC_CONFIG_HEVC_MEONLY;
+
+/**
+ * \struct _NV_ENC_CODEC_CONFIG
+ * Codec-specific encoder configuration parameters to be set during initialization.
+ */
+typedef union _NV_ENC_CODEC_CONFIG
+{
+    NV_ENC_CONFIG_H264        h264Config;                /**< [in]: Specifies the H.264-specific encoder configuration. */
+    NV_ENC_CONFIG_HEVC        hevcConfig;                /**< [in]: Specifies the HEVC-specific encoder configuration. */
+    NV_ENC_CONFIG_H264_MEONLY h264MeOnlyConfig;          /**< [in]: Specifies the H.264-specific ME only encoder configuration. */
+    NV_ENC_CONFIG_HEVC_MEONLY hevcMeOnlyConfig;          /**< [in]: Specifies the HEVC-specific ME only encoder configuration. */
+    uint32_t                reserved[320];               /**< [in]: Reserved and must be set to 0 */
+} NV_ENC_CODEC_CONFIG;
+
+
+/**
+ * \struct _NV_ENC_CONFIG
+ * Encoder configuration parameters to be set during initialization.
+ */
+typedef struct _NV_ENC_CONFIG
+{
+    uint32_t                        version;                                     /**< [in]: Struct version. Must be set to ::NV_ENC_CONFIG_VER. */
+    GUID                            profileGUID;                                 /**< [in]: Specifies the codec profile guid. If client specifies \p NV_ENC_CODEC_PROFILE_AUTOSELECT_GUID the NvEncodeAPI interface will select the appropriate codec profile. */
+    uint32_t                        gopLength;                                   /**< [in]: Specifies the number of pictures in one GOP. Low latency application client can set goplength to NVENC_INFINITE_GOPLENGTH so that keyframes are not inserted automatically. */
+    int32_t                         frameIntervalP;                              /**< [in]: Specifies the GOP pattern as follows: \p frameIntervalP = 0: I, 1: IPP, 2: IBP, 3: IBBP  If goplength is set to NVENC_INFINITE_GOPLENGTH \p frameIntervalP should be set to 1. */
+    uint32_t                        monoChromeEncoding;                          /**< [in]: Set this to 1 to enable monochrome encoding for this session. */
+    NV_ENC_PARAMS_FRAME_FIELD_MODE  frameFieldMode;                              /**< [in]: Specifies the frame/field mode.
+                                                                                            Check support for field encoding using ::NV_ENC_CAPS_SUPPORT_FIELD_ENCODING caps.
+                                                                                            Using a frameFieldMode other than NV_ENC_PARAMS_FRAME_FIELD_MODE_FRAME for RGB input is not supported. */
+    NV_ENC_MV_PRECISION             mvPrecision;                                 /**< [in]: Specifies the desired motion vector prediction precision. */
+    NV_ENC_RC_PARAMS                rcParams;                                    /**< [in]: Specifies the rate control parameters for the current encoding session. */
+    NV_ENC_CODEC_CONFIG             encodeCodecConfig;                           /**< [in]: Specifies the codec specific config parameters through this union. */
+    uint32_t                        reserved [278];                              /**< [in]: Reserved and must be set to 0 */
+    void*                           reserved2[64];                               /**< [in]: Reserved and must be set to NULL */
+} NV_ENC_CONFIG;
+
+/** macro for constructing the version field of ::_NV_ENC_CONFIG */
+#define NV_ENC_CONFIG_VER (NVENCAPI_STRUCT_VERSION(6) | ( 1<<31 ))
+
+
+/**
+ * \struct _NV_ENC_INITIALIZE_PARAMS
+ * Encode Session Initialization parameters.
+ */
+typedef struct _NV_ENC_INITIALIZE_PARAMS
+{
+    uint32_t                                   version;                         /**< [in]: Struct version. Must be set to ::NV_ENC_INITIALIZE_PARAMS_VER. */
+    GUID                                       encodeGUID;                      /**< [in]: Specifies the Encode GUID for which the encoder is being created. ::NvEncInitializeEncoder() API will fail if this is not set, or set to unsupported value. */
+    GUID                                       presetGUID;                      /**< [in]: Specifies the preset for encoding. If the preset GUID is set then , the preset configuration will be applied before any other parameter. */
+    uint32_t                                   encodeWidth;                     /**< [in]: Specifies the encode width. If not set ::NvEncInitializeEncoder() API will fail. */
+    uint32_t                                   encodeHeight;                    /**< [in]: Specifies the encode height. If not set ::NvEncInitializeEncoder() API will fail. */
+    uint32_t                                   darWidth;                        /**< [in]: Specifies the display aspect ratio Width. */
+    uint32_t                                   darHeight;                       /**< [in]: Specifies the display aspect ratio height. */
+    uint32_t                                   frameRateNum;                    /**< [in]: Specifies the numerator for frame rate used for encoding in frames per second ( Frame rate = frameRateNum / frameRateDen ). */
+    uint32_t                                   frameRateDen;                    /**< [in]: Specifies the denominator for frame rate used for encoding in frames per second ( Frame rate = frameRateNum / frameRateDen ). */
+    uint32_t                                   enableEncodeAsync;               /**< [in]: Set this to 1 to enable asynchronous mode and is expected to use events to get picture completion notification. */
+    uint32_t                                   enablePTD;                       /**< [in]: Set this to 1 to enable the Picture Type Decision is be taken by the NvEncodeAPI interface. */
+    uint32_t                                   reportSliceOffsets        :1;    /**< [in]: Set this to 1 to enable reporting slice offsets in ::_NV_ENC_LOCK_BITSTREAM. NV_ENC_INITIALIZE_PARAMS::enableEncodeAsync must be set to 0 to use this feature. Client must set this to 0 if NV_ENC_CONFIG_H264::sliceMode is 1 on Kepler GPUs */
+    uint32_t                                   enableSubFrameWrite       :1;    /**< [in]: Set this to 1 to write out available bitstream to memory at subframe intervals */
+    uint32_t                                   enableExternalMEHints     :1;    /**< [in]: Set to 1 to enable external ME hints for the current frame. For NV_ENC_INITIALIZE_PARAMS::enablePTD=1 with B frames, programming L1 hints is optional for B frames since Client doesn't know internal GOP structure. 
+                                                                                           NV_ENC_PIC_PARAMS::meHintRefPicDist should preferably be set with enablePTD=1. */
+    uint32_t                                   enableMEOnlyMode          :1;    /**< [in]: Set to 1 to enable ME Only Mode .*/
+    uint32_t                                   reservedBitFields         :28;   /**< [in]: Reserved bitfields and must be set to 0 */
+    uint32_t                                   privDataSize;                    /**< [in]: Reserved private data buffer size and must be set to 0 */
+    void*                                      privData;                        /**< [in]: Reserved private data buffer and must be set to NULL */
+    NV_ENC_CONFIG*                             encodeConfig;                    /**< [in]: Specifies the advanced codec specific structure. If client has sent a valid codec config structure, it will override parameters set by the NV_ENC_INITIALIZE_PARAMS::presetGUID parameter. If set to NULL the NvEncodeAPI interface will use the NV_ENC_INITIALIZE_PARAMS::presetGUID to set the codec specific parameters.
+                                                                                           Client can also optionally query the NvEncodeAPI interface to get codec specific parameters for a presetGUID using ::NvEncGetEncodePresetConfig() API. It can then modify (if required) some of the codec config parameters and send down a custom config structure as part of ::_NV_ENC_INITIALIZE_PARAMS.
+                                                                                           Even in this case client is recommended to pass the same preset guid it has used in ::NvEncGetEncodePresetConfig() API to query the config structure; as NV_ENC_INITIALIZE_PARAMS::presetGUID. This will not override the custom config structure but will be used to determine other Encoder HW specific parameters not exposed in the API. */
+    uint32_t                                   maxEncodeWidth;                  /**< [in]: Maximum encode width to be used for current Encode session.
+                                                                                           Client should allocate output buffers according to this dimension for dynamic resolution change. If set to 0, Encoder will not allow dynamic resolution change. */
+    uint32_t                                   maxEncodeHeight;                 /**< [in]: Maximum encode height to be allowed for current Encode session.
+                                                                                           Client should allocate output buffers according to this dimension for dynamic resolution change. If set to 0, Encode will not allow dynamic resolution change. */
+    NVENC_EXTERNAL_ME_HINT_COUNTS_PER_BLOCKTYPE maxMEHintCountsPerBlock[2];      /**< [in]: If Client wants to pass external motion vectors in NV_ENC_PIC_PARAMS::meExternalHints buffer it must specify the maximum number of hint candidates per block per direction for the encode session.
+                                                                                           The NV_ENC_INITIALIZE_PARAMS::maxMEHintCountsPerBlock[0] is for L0 predictors and NV_ENC_INITIALIZE_PARAMS::maxMEHintCountsPerBlock[1] is for L1 predictors.
+                                                                                           This client must also set NV_ENC_INITIALIZE_PARAMS::enableExternalMEHints to 1. */
+    uint32_t                                   reserved [289];                  /**< [in]: Reserved and must be set to 0 */
+    void*                                      reserved2[64];                   /**< [in]: Reserved and must be set to NULL */
+} NV_ENC_INITIALIZE_PARAMS;
+
+/** macro for constructing the version field of ::_NV_ENC_INITIALIZE_PARAMS */
+#define NV_ENC_INITIALIZE_PARAMS_VER (NVENCAPI_STRUCT_VERSION(5) | ( 1<<31 ))
+
+
+/**
+ * \struct _NV_ENC_RECONFIGURE_PARAMS
+ * Encode Session Reconfigured parameters.
+ */
+typedef struct _NV_ENC_RECONFIGURE_PARAMS
+{
+    uint32_t                                    version;                        /**< [in]: Struct version. Must be set to ::NV_ENC_RECONFIGURE_PARAMS_VER. */
+    NV_ENC_INITIALIZE_PARAMS                    reInitEncodeParams;             /**< [in]: Encoder session re-initialization parameters. */
+    uint32_t                                    resetEncoder            :1;     /**< [in]: This resets the rate control states and other internal encoder states. This should be used only with an IDR frame.
+                                                                                           If NV_ENC_INITIALIZE_PARAMS::enablePTD is set to 1, encoder will force the frame type to IDR */
+    uint32_t                                    forceIDR                :1;     /**< [in]: Encode the current picture as an IDR picture. This flag is only valid when Picture type decision is taken by the Encoder
+                                                                                           [_NV_ENC_INITIALIZE_PARAMS::enablePTD == 1]. */
+    uint32_t                                    reserved                :30;
+
+}NV_ENC_RECONFIGURE_PARAMS;
+
+/** macro for constructing the version field of ::_NV_ENC_RECONFIGURE_PARAMS */
+#define NV_ENC_RECONFIGURE_PARAMS_VER (NVENCAPI_STRUCT_VERSION(1) | ( 1<<31 ))
+
+/**
+ * \struct _NV_ENC_PRESET_CONFIG
+ * Encoder preset config
+ */ 
+typedef struct _NV_ENC_PRESET_CONFIG
+{
+    uint32_t      version;                               /**< [in]:  Struct version. Must be set to ::NV_ENC_PRESET_CONFIG_VER. */
+    NV_ENC_CONFIG presetCfg;                             /**< [out]: preset config returned by the Nvidia Video Encoder interface. */
+    uint32_t      reserved1[255];                        /**< [in]: Reserved and must be set to 0 */
+    void*         reserved2[64];                         /**< [in]: Reserved and must be set to NULL */
+}NV_ENC_PRESET_CONFIG;
+
+/** macro for constructing the version field of ::_NV_ENC_PRESET_CONFIG */
+#define NV_ENC_PRESET_CONFIG_VER (NVENCAPI_STRUCT_VERSION(4) | ( 1<<31 ))
+
+
+/**
+ * \struct _NV_ENC_SEI_PAYLOAD
+ *  User SEI message
+ */
+typedef struct _NV_ENC_SEI_PAYLOAD
+{
+    uint32_t payloadSize;            /**< [in] SEI payload size in bytes. SEI payload must be byte aligned, as described in Annex D */
+    uint32_t payloadType;            /**< [in] SEI payload types and syntax can be found in Annex D of the H.264 Specification. */
+    uint8_t *payload;                /**< [in] pointer to user data */
+} NV_ENC_SEI_PAYLOAD;
+
+#define NV_ENC_H264_SEI_PAYLOAD NV_ENC_SEI_PAYLOAD
+
+/**
+ * \struct _NV_ENC_PIC_PARAMS_H264
+ * H264 specific enc pic params. sent on a per frame basis.
+ */ 
+typedef struct _NV_ENC_PIC_PARAMS_H264
+{
+    uint32_t displayPOCSyntax;                           /**< [in]: Specifies the display POC syntax This is required to be set if client is handling the picture type decision. */
+    uint32_t reserved3;                                  /**< [in]: Reserved and must be set to 0 */
+    uint32_t refPicFlag;                                 /**< [in]: Set to 1 for a reference picture. This is ignored if NV_ENC_INITIALIZE_PARAMS::enablePTD is set to 1. */
+    uint32_t colourPlaneId;                              /**< [in]: Specifies the colour plane ID associated with the current input. */
+    uint32_t forceIntraRefreshWithFrameCnt;              /**< [in]: Forces an intra refresh with duration equal to intraRefreshFrameCnt. 
+                                                                    When outputRecoveryPointSEI is set this is value is used for recovery_frame_cnt in recovery point SEI message 
+                                                                    forceIntraRefreshWithFrameCnt cannot be used if B frames are used in the GOP structure specified */
+    uint32_t constrainedFrame           :1;              /**< [in]: Set to 1 if client wants to encode this frame with each slice completely independent of other slices in the frame. 
+                                                                    NV_ENC_INITIALIZE_PARAMS::enableConstrainedEncoding should be set to 1 */
+    uint32_t sliceModeDataUpdate        :1;              /**< [in]: Set to 1 if client wants to change the sliceModeData field to specify new sliceSize Parameter
+                                                                    When forceIntraRefreshWithFrameCnt is set it will have priority over sliceMode setting */
+    uint32_t ltrMarkFrame               :1;              /**< [in]: Set to 1 if client wants to mark this frame as LTR */
+    uint32_t ltrUseFrames               :1;              /**< [in]: Set to 1 if client allows encoding this frame using the LTR frames specified in ltrFrameBitmap */
+    uint32_t reservedBitFields          :28;             /**< [in]: Reserved bit fields and must be set to 0 */
+    uint8_t* sliceTypeData;                              /**< [in]: Deprecated. */
+    uint32_t sliceTypeArrayCnt;                          /**< [in]: Deprecated. */
+    uint32_t seiPayloadArrayCnt;                         /**< [in]: Specifies the number of elements allocated in  seiPayloadArray array. */
+    NV_ENC_SEI_PAYLOAD* seiPayloadArray;                 /**< [in]: Array of SEI payloads which will be inserted for this frame. */
+    uint32_t sliceMode;                                  /**< [in]: This parameter in conjunction with sliceModeData specifies the way in which the picture is divided into slices
+                                                                    sliceMode = 0 MB based slices, sliceMode = 1 Byte based slices, sliceMode = 2 MB row based slices, sliceMode = 3, numSlices in Picture
+                                                                    When forceIntraRefreshWithFrameCnt is set it will have priority over sliceMode setting
+                                                                    When sliceMode == 0 and sliceModeData == 0 whole picture will be coded with one slice */
+    uint32_t sliceModeData;                              /**< [in]: Specifies the parameter needed for sliceMode. For:
+                                                                    sliceMode = 0, sliceModeData specifies # of MBs in each slice (except last slice)
+                                                                    sliceMode = 1, sliceModeData specifies maximum # of bytes in each slice (except last slice)
+                                                                    sliceMode = 2, sliceModeData specifies # of MB rows in each slice (except last slice)
+                                                                    sliceMode = 3, sliceModeData specifies number of slices in the picture. Driver will divide picture into slices optimally */
+    uint32_t ltrMarkFrameIdx;                            /**< [in]: Specifies the long term referenceframe index to use for marking this frame as LTR.*/
+    uint32_t ltrUseFrameBitmap;                          /**< [in]: Specifies the the associated bitmap of LTR frame indices when encoding this frame. */
+    uint32_t ltrUsageMode;                               /**< [in]: Specifies additional usage constraints for encoding using LTR frames from this point further. 0: no constraints, 1: no short term refs older than current, no previous LTR frames.*/
+    uint32_t reserved [243];                             /**< [in]: Reserved and must be set to 0. */
+    void*    reserved2[62];                              /**< [in]: Reserved and must be set to NULL. */
+} NV_ENC_PIC_PARAMS_H264;
+
+/**
+ * \struct _NV_ENC_PIC_PARAMS_HEVC
+ * HEVC specific enc pic params. sent on a per frame basis.
+ */
+typedef struct _NV_ENC_PIC_PARAMS_HEVC
+{
+    uint32_t displayPOCSyntax;                           /**< [in]: Specifies the display POC syntax This is required to be set if client is handling the picture type decision. */
+    uint32_t refPicFlag;                                 /**< [in]: Set to 1 for a reference picture. This is ignored if NV_ENC_INITIALIZE_PARAMS::enablePTD is set to 1. */
+    uint32_t temporalId;                                 /**< [in]: Specifies the temporal id of the picture */
+    uint32_t forceIntraRefreshWithFrameCnt;              /**< [in]: Forces an intra refresh with duration equal to intraRefreshFrameCnt. 
+                                                                    When outputRecoveryPointSEI is set this is value is used for recovery_frame_cnt in recovery point SEI message 
+                                                                    forceIntraRefreshWithFrameCnt cannot be used if B frames are used in the GOP structure specified */
+    uint32_t constrainedFrame           :1;              /**< [in]: Set to 1 if client wants to encode this frame with each slice completely independent of other slices in the frame. 
+                                                                    NV_ENC_INITIALIZE_PARAMS::enableConstrainedEncoding should be set to 1 */
+    uint32_t sliceModeDataUpdate        :1;              /**< [in]: Set to 1 if client wants to change the sliceModeData field to specify new sliceSize Parameter
+                                                                    When forceIntraRefreshWithFrameCnt is set it will have priority over sliceMode setting */
+    uint32_t ltrMarkFrame               :1;              /**< [in]: Set to 1 if client wants to mark this frame as LTR */
+    uint32_t ltrUseFrames               :1;              /**< [in]: Set to 1 if client allows encoding this frame using the LTR frames specified in ltrFrameBitmap */
+    uint32_t reservedBitFields          :28;             /**< [in]: Reserved bit fields and must be set to 0 */
+    uint8_t* sliceTypeData;                              /**< [in]: Array which specifies the slice type used to force intra slice for a particular slice. Currently supported only for NV_ENC_CONFIG_H264::sliceMode == 3. 
+                                                                    Client should allocate array of size sliceModeData where sliceModeData is specified in field of ::_NV_ENC_CONFIG_H264 
+                                                                    Array element with index n corresponds to nth slice. To force a particular slice to intra client should set corresponding array element to NV_ENC_SLICE_TYPE_I
+                                                                    all other array elements should be set to NV_ENC_SLICE_TYPE_DEFAULT */
+    uint32_t sliceTypeArrayCnt;                          /**< [in]: Client should set this to the number of elements allocated in sliceTypeData array. If sliceTypeData is NULL then this should be set to 0 */
+    uint32_t sliceMode;                                  /**< [in]: This parameter in conjunction with sliceModeData specifies the way in which the picture is divided into slices
+                                                                    sliceMode = 0 CTU based slices, sliceMode = 1 Byte based slices, sliceMode = 2 CTU row based slices, sliceMode = 3, numSlices in Picture
+                                                                    When forceIntraRefreshWithFrameCnt is set it will have priority over sliceMode setting
+                                                                    When sliceMode == 0 and sliceModeData == 0 whole picture will be coded with one slice */
+    uint32_t sliceModeData;                              /**< [in]: Specifies the parameter needed for sliceMode. For:
+                                                                    sliceMode = 0, sliceModeData specifies # of CTUs in each slice (except last slice)
+                                                                    sliceMode = 1, sliceModeData specifies maximum # of bytes in each slice (except last slice)
+                                                                    sliceMode = 2, sliceModeData specifies # of CTU rows in each slice (except last slice)
+                                                                    sliceMode = 3, sliceModeData specifies number of slices in the picture. Driver will divide picture into slices optimally */
+    uint32_t ltrMarkFrameIdx;                            /**< [in]: Specifies the long term reference frame index to use for marking this frame as LTR.*/
+    uint32_t ltrUseFrameBitmap;                          /**< [in]: Specifies the associated bitmap of LTR frame indices when encoding this frame. */
+    uint32_t ltrUsageMode;                               /**< [in]: Specifies additional usage constraints for encoding using LTR frames from this point further. 0: no constraints, 1: no short term refs older than current, no previous LTR frames.*/
+    uint32_t seiPayloadArrayCnt;                         /**< [in]: Specifies the number of elements allocated in  seiPayloadArray array. */
+    uint32_t reserved;                                   /**< [in]: Reserved and must be set to 0. */
+    NV_ENC_SEI_PAYLOAD* seiPayloadArray;                 /**< [in]: Array of SEI payloads which will be inserted for this frame. */
+    uint32_t reserved2 [244];                             /**< [in]: Reserved and must be set to 0. */
+    void*    reserved3[61];                              /**< [in]: Reserved and must be set to NULL. */
+} NV_ENC_PIC_PARAMS_HEVC;
+
+
+/**
+ * Codec specific per-picture encoding parameters.
+ */
+typedef union _NV_ENC_CODEC_PIC_PARAMS
+{
+    NV_ENC_PIC_PARAMS_H264 h264PicParams;                /**< [in]: H264 encode picture params. */
+    NV_ENC_PIC_PARAMS_HEVC hevcPicParams;                /**< [in]: HEVC encode picture params. Currently unsupported and must not to be used.  */
+    uint32_t               reserved[256];                /**< [in]: Reserved and must be set to 0. */
+} NV_ENC_CODEC_PIC_PARAMS;
+
+/**
+ * \struct _NV_ENC_PIC_PARAMS
+ * Encoding parameters that need to be sent on a per frame basis.
+ */
+typedef struct _NV_ENC_PIC_PARAMS
+{
+    uint32_t                                    version;                        /**< [in]: Struct version. Must be set to ::NV_ENC_PIC_PARAMS_VER. */
+    uint32_t                                    inputWidth;                     /**< [in]: Specifies the input buffer width */
+    uint32_t                                    inputHeight;                    /**< [in]: Specifies the input buffer height */
+    uint32_t                                    inputPitch;                     /**< [in]: Specifies the input buffer pitch. If pitch value is not known, set this to inputWidth. */
+    uint32_t                                    encodePicFlags;                 /**< [in]: Specifies bit-wise OR`ed encode pic flags. See ::NV_ENC_PIC_FLAGS enum. */
+    uint32_t                                    frameIdx;                       /**< [in]: Specifies the frame index associated with the input frame [optional]. */
+    uint64_t                                    inputTimeStamp;                 /**< [in]: Specifies presentation timestamp associated with the input picture. */
+    uint64_t                                    inputDuration;                  /**< [in]: Specifies duration of the input picture */
+    NV_ENC_INPUT_PTR                            inputBuffer;                    /**< [in]: Specifies the input buffer pointer. Client must use a pointer obtained from ::NvEncCreateInputBuffer() or ::NvEncMapInputResource() APIs.*/
+    NV_ENC_OUTPUT_PTR                           outputBitstream;                /**< [in]: Specifies the pointer to output buffer. Client should use a pointer obtained from ::NvEncCreateBitstreamBuffer() API. */
+    void*                                       completionEvent;                /**< [in]: Specifies an event to be signalled on completion of encoding of this Frame [only if operating in Asynchronous mode]. Each output buffer should be associated with a distinct event pointer. */
+    NV_ENC_BUFFER_FORMAT                        bufferFmt;                      /**< [in]: Specifies the input buffer format. */
+    NV_ENC_PIC_STRUCT                           pictureStruct;                  /**< [in]: Specifies structure of the input picture. */
+    NV_ENC_PIC_TYPE                             pictureType;                    /**< [in]: Specifies input picture type. Client required to be set explicitly by the client if the client has not set NV_ENC_INITALIZE_PARAMS::enablePTD to 1 while calling NvInitializeEncoder. */
+    NV_ENC_CODEC_PIC_PARAMS                     codecPicParams;                 /**< [in]: Specifies the codec specific per-picture encoding parameters. */
+    NVENC_EXTERNAL_ME_HINT_COUNTS_PER_BLOCKTYPE meHintCountsPerBlock[2];        /**< [in]: Specifies the number of hint candidates per block per direction for the current frame. meHintCountsPerBlock[0] is for L0 predictors and meHintCountsPerBlock[1] is for L1 predictors.
+                                                                                           The candidate count in NV_ENC_PIC_PARAMS::meHintCountsPerBlock[lx] must never exceed NV_ENC_INITIALIZE_PARAMS::maxMEHintCountsPerBlock[lx] provided during encoder intialization. */
+    NVENC_EXTERNAL_ME_HINT                     *meExternalHints;                /**< [in]: Specifies the pointer to ME external hints for the current frame. The size of ME hint buffer should be equal to number of macroblocks multiplied by the total number of candidates per macroblock.
+                                                                                           The total number of candidates per MB per direction = 1*meHintCountsPerBlock[Lx].numCandsPerBlk16x16 + 2*meHintCountsPerBlock[Lx].numCandsPerBlk16x8 + 2*meHintCountsPerBlock[Lx].numCandsPerBlk8x8  
+                                                                                           + 4*meHintCountsPerBlock[Lx].numCandsPerBlk8x8. For frames using bidirectional ME , the total number of candidates for single macroblock is sum of total number of candidates per MB for each direction (L0 and L1) */
+    uint32_t                                    reserved1[6];                    /**< [in]: Reserved and must be set to 0 */
+    void*                                       reserved2[2];                    /**< [in]: Reserved and must be set to NULL */
+    int8_t                                     *qpDeltaMap;                      /**< [in]: Specifies the pointer to signed byte array containing QP delta value per MB in raster scan order in the current picture. This QP modifier is applied on top of the QP chosen by rate control. */
+    uint32_t                                    qpDeltaMapSize;                  /**< [in]: Specifies the size in bytes of qpDeltaMap surface allocated by client and pointed to by NV_ENC_PIC_PARAMS::qpDeltaMap. Surface (array) should be picWidthInMbs * picHeightInMbs */
+    uint32_t                                    reservedBitFields;               /**< [in]: Reserved bitfields and must be set to 0 */
+    uint16_t                                    meHintRefPicDist[2];             /**< [in]: Specifies temporal distance for reference picture (NVENC_EXTERNAL_ME_HINT::refidx = 0) used during external ME with NV_ENC_INITALIZE_PARAMS::enablePTD = 1 . meHintRefPicDist[0] is for L0 hints and meHintRefPicDist[1] is for L1 hints. 
+                                                                                            If not set, will internally infer distance of 1. Ignored for NV_ENC_INITALIZE_PARAMS::enablePTD = 0 */
+    uint32_t                                    reserved3[286];                  /**< [in]: Reserved and must be set to 0 */
+    void*                                       reserved4[60];                   /**< [in]: Reserved and must be set to NULL */
+} NV_ENC_PIC_PARAMS;
+
+/** Macro for constructing the version field of ::_NV_ENC_PIC_PARAMS */
+#define NV_ENC_PIC_PARAMS_VER (NVENCAPI_STRUCT_VERSION(4) | ( 1<<31 ))
+
+
+/**
+ * \struct _NV_ENC_MEONLY_PARAMS
+ * MEOnly parameters that need to be sent on a per motion estimation basis.
+ */
+typedef struct _NV_ENC_MEONLY_PARAMS
+{
+    uint32_t                version;                            /**< [in]: Struct version. Must be set to NV_ENC_MEONLY_PARAMS_VER.*/
+    uint32_t                inputWidth;                         /**< [in]: Specifies the input buffer width */
+    uint32_t                inputHeight;                        /**< [in]: Specifies the input buffer height */
+    NV_ENC_INPUT_PTR        inputBuffer;                        /**< [in]: Specifies the input buffer pointer. Client must use a pointer obtained from NvEncCreateInputBuffer() or NvEncMapInputResource() APIs. */
+    NV_ENC_INPUT_PTR        referenceFrame;                     /**< [in]: Specifies the reference frame pointer */
+    NV_ENC_OUTPUT_PTR       mvBuffer;                           /**< [in]: Specifies the pointer to motion vector data buffer allocated by NvEncCreateMVBuffer. Client must lock mvBuffer using ::NvEncLockBitstream() API to get the motion vector data. */
+    NV_ENC_BUFFER_FORMAT    bufferFmt;                          /**< [in]: Specifies the input buffer format. */
+    void*                   completionEvent;                    /**< [in]: Specifies an event to be signalled on completion of motion estimation 
+                                                                           of this Frame [only if operating in Asynchronous mode]. 
+                                                                           Each output buffer should be associated with a distinct event pointer. */
+    uint32_t                viewID;                              /**< [in]: Specifies left,right viewID if NV_ENC_CONFIG_H264_MEONLY::bStereoEnable is set.
+                                                                            viewID can be 0,1 if bStereoEnable is set, 0 otherwise. */
+    uint32_t                reserved1[251];                     /**< [in]: Reserved and must be set to 0 */
+    void*                   reserved2[60];                      /**< [in]: Reserved and must be set to NULL */
+} NV_ENC_MEONLY_PARAMS;
+
+/** NV_ENC_MEONLY_PARAMS struct version*/
+#define NV_ENC_MEONLY_PARAMS_VER NVENCAPI_STRUCT_VERSION(3)
+
+
+/**
+ * \struct _NV_ENC_LOCK_BITSTREAM
+ * Bitstream buffer lock parameters.
+ */
+typedef struct _NV_ENC_LOCK_BITSTREAM
+{ 
+    uint32_t                version;                     /**< [in]: Struct version. Must be set to ::NV_ENC_LOCK_BITSTREAM_VER. */
+    uint32_t                doNotWait         :1;        /**< [in]: If this flag is set, the NvEncodeAPI interface will return buffer pointer even if operation is not completed. If not set, the call will block until operation completes. */
+    uint32_t                ltrFrame          :1;        /**< [out]: Flag indicating this frame is marked as LTR frame */
+    uint32_t                reservedBitFields :30;       /**< [in]: Reserved bit fields and must be set to 0 */
+    void*                   outputBitstream;             /**< [in]: Pointer to the bitstream buffer being locked. */
+    uint32_t*               sliceOffsets;                /**< [in,out]: Array which receives the slice offsets. This is not supported if NV_ENC_CONFIG_H264::sliceMode is 1 on Kepler GPUs. Array size must be equal to size of frame in MBs. */
+    uint32_t                frameIdx;                    /**< [out]: Frame no. for which the bitstream is being retrieved. */ 
+    uint32_t                hwEncodeStatus;              /**< [out]: The NvEncodeAPI interface status for the locked picture. */
+    uint32_t                numSlices;                   /**< [out]: Number of slices in the encoded picture. Will be reported only if NV_ENC_INITIALIZE_PARAMS::reportSliceOffsets set to 1. */
+    uint32_t                bitstreamSizeInBytes;        /**< [out]: Actual number of bytes generated and copied to the memory pointed by bitstreamBufferPtr. */
+    uint64_t                outputTimeStamp;             /**< [out]: Presentation timestamp associated with the encoded output. */
+    uint64_t                outputDuration;              /**< [out]: Presentation duration associates with the encoded output. */
+    void*                   bitstreamBufferPtr;          /**< [out]: Pointer to the generated output bitstream. 
+                                                                     For MEOnly mode _NV_ENC_LOCK_BITSTREAM::bitstreamBufferPtr should be typecast to
+                                                                     NV_ENC_H264_MV_DATA/NV_ENC_HEVC_MV_DATA pointer respectively for H264/HEVC  */
+    NV_ENC_PIC_TYPE         pictureType;                 /**< [out]: Picture type of the encoded picture. */
+    NV_ENC_PIC_STRUCT       pictureStruct;               /**< [out]: Structure of the generated output picture. */
+    uint32_t                frameAvgQP;                  /**< [out]: Average QP of the frame. */
+    uint32_t                frameSatd;                   /**< [out]: Total SATD cost for whole frame. */
+    uint32_t                ltrFrameIdx;                 /**< [out]: Frame index associated with this LTR frame. */
+    uint32_t                ltrFrameBitmap;              /**< [out]: Bitmap of LTR frames indices which were used for encoding this frame. Value of 0 if no LTR frames were used. */
+    uint32_t                reserved [236];              /**< [in]: Reserved and must be set to 0 */
+    void*                   reserved2[64];               /**< [in]: Reserved and must be set to NULL */
+} NV_ENC_LOCK_BITSTREAM;
+
+/** Macro for constructing the version field of ::_NV_ENC_LOCK_BITSTREAM */
+#define NV_ENC_LOCK_BITSTREAM_VER NVENCAPI_STRUCT_VERSION(1)
+
+
+/**
+ * \struct _NV_ENC_LOCK_INPUT_BUFFER
+ * Uncompressed Input Buffer lock parameters.
+ */
+typedef struct _NV_ENC_LOCK_INPUT_BUFFER
+{
+    uint32_t                  version;                   /**< [in]:  Struct version. Must be set to ::NV_ENC_LOCK_INPUT_BUFFER_VER. */
+    uint32_t                  doNotWait         :1;      /**< [in]:  Set to 1 to make ::NvEncLockInputBuffer() a unblocking call. If the encoding is not completed, driver will return ::NV_ENC_ERR_ENCODER_BUSY error code. */
+    uint32_t                  reservedBitFields :31;     /**< [in]:  Reserved bitfields and must be set to 0 */
+    NV_ENC_INPUT_PTR          inputBuffer;               /**< [in]:  Pointer to the input buffer to be locked, client should pass the pointer obtained from ::NvEncCreateInputBuffer() or ::NvEncMapInputResource API. */
+    void*                     bufferDataPtr;             /**< [out]: Pointed to the locked input buffer data. Client can only access input buffer using the \p bufferDataPtr. */
+    uint32_t                  pitch;                     /**< [out]: Pitch of the locked input buffer. */
+    uint32_t                  reserved1[251];            /**< [in]:  Reserved and must be set to 0  */
+    void*                     reserved2[64];             /**< [in]:  Reserved and must be set to NULL  */
+} NV_ENC_LOCK_INPUT_BUFFER;
+
+/** Macro for constructing the version field of ::_NV_ENC_LOCK_INPUT_BUFFER */
+#define NV_ENC_LOCK_INPUT_BUFFER_VER NVENCAPI_STRUCT_VERSION(1)
+
+
+/**
+ * \struct _NV_ENC_MAP_INPUT_RESOURCE
+ * Map an input resource to a Nvidia Encoder Input Buffer
+ */
+typedef struct _NV_ENC_MAP_INPUT_RESOURCE
+{
+    uint32_t                   version;                   /**< [in]:  Struct version. Must be set to ::NV_ENC_MAP_INPUT_RESOURCE_VER. */
+    uint32_t                   subResourceIndex;          /**< [in]:  Deprecated. Do not use. */
+    void*                      inputResource;             /**< [in]:  Deprecated. Do not use. */
+    NV_ENC_REGISTERED_PTR      registeredResource;        /**< [in]:  The Registered resource handle obtained by calling NvEncRegisterInputResource. */
+    NV_ENC_INPUT_PTR           mappedResource;            /**< [out]: Mapped pointer corresponding to the registeredResource. This pointer must be used in NV_ENC_PIC_PARAMS::inputBuffer parameter in ::NvEncEncodePicture() API. */
+    NV_ENC_BUFFER_FORMAT       mappedBufferFmt;           /**< [out]: Buffer format of the outputResource. This buffer format must be used in NV_ENC_PIC_PARAMS::bufferFmt if client using the above mapped resource pointer. */
+    uint32_t                   reserved1[251];            /**< [in]:  Reserved and must be set to 0. */
+    void*                      reserved2[63];             /**< [in]:  Reserved and must be set to NULL */
+} NV_ENC_MAP_INPUT_RESOURCE;
+
+/** Macro for constructing the version field of ::_NV_ENC_MAP_INPUT_RESOURCE */
+#define NV_ENC_MAP_INPUT_RESOURCE_VER NVENCAPI_STRUCT_VERSION(4)
+
+/**
+ * \struct _NV_ENC_REGISTER_RESOURCE
+ * Register a resource for future use with the Nvidia Video Encoder Interface.
+ */
+typedef struct _NV_ENC_REGISTER_RESOURCE
+{
+    uint32_t                    version;                        /**< [in]: Struct version. Must be set to ::NV_ENC_REGISTER_RESOURCE_VER. */
+    NV_ENC_INPUT_RESOURCE_TYPE  resourceType;                   /**< [in]: Specifies the type of resource to be registered. Supported values are ::NV_ENC_INPUT_RESOURCE_TYPE_DIRECTX, ::NV_ENC_INPUT_RESOURCE_TYPE_CUDADEVICEPTR. */
+    uint32_t                    width;                          /**< [in]: Input buffer Width. */
+    uint32_t                    height;                         /**< [in]: Input buffer Height. */
+    uint32_t                    pitch;                          /**< [in]: Input buffer Pitch.  */
+    uint32_t                    subResourceIndex;               /**< [in]: Subresource Index of the DirectX resource to be registered. Should be set to 0 for other interfaces. */
+    void*                       resourceToRegister;             /**< [in]: Handle to the resource that is being registered. */
+    NV_ENC_REGISTERED_PTR       registeredResource;             /**< [out]: Registered resource handle. This should be used in future interactions with the Nvidia Video Encoder Interface. */
+    NV_ENC_BUFFER_FORMAT        bufferFormat;                   /**< [in]: Buffer format of resource to be registered. */
+    uint32_t                    reserved1[248];                 /**< [in]: Reserved and must be set to 0. */
+    void*                       reserved2[62];                  /**< [in]: Reserved and must be set to NULL. */
+} NV_ENC_REGISTER_RESOURCE;
+
+/** Macro for constructing the version field of ::_NV_ENC_REGISTER_RESOURCE */
+#define NV_ENC_REGISTER_RESOURCE_VER NVENCAPI_STRUCT_VERSION(3)
+
+/**
+ * \struct _NV_ENC_STAT
+ * Encode Stats structure.
+ */
+typedef struct _NV_ENC_STAT
+{
+    uint32_t            version;                         /**< [in]:  Struct version. Must be set to ::NV_ENC_STAT_VER. */
+    uint32_t            reserved;                        /**< [in]:  Reserved and must be set to 0 */
+    NV_ENC_OUTPUT_PTR   outputBitStream;                 /**< [out]: Specifies the pointer to output bitstream. */
+    uint32_t            bitStreamSize;                   /**< [out]: Size of generated bitstream in bytes. */
+    uint32_t            picType;                         /**< [out]: Picture type of encoded picture. See ::NV_ENC_PIC_TYPE. */
+    uint32_t            lastValidByteOffset;             /**< [out]: Offset of last valid bytes of completed bitstream */
+    uint32_t            sliceOffsets[16];                /**< [out]: Offsets of each slice */
+    uint32_t            picIdx;                          /**< [out]: Picture number */
+    uint32_t            reserved1[233];                  /**< [in]:  Reserved and must be set to 0 */
+    void*               reserved2[64];                   /**< [in]:  Reserved and must be set to NULL */
+} NV_ENC_STAT;
+
+/** Macro for constructing the version field of ::_NV_ENC_STAT */
+#define NV_ENC_STAT_VER NVENCAPI_STRUCT_VERSION(1)
+
+
+/**
+ * \struct _NV_ENC_SEQUENCE_PARAM_PAYLOAD
+ * Sequence and picture paramaters payload.
+ */
+typedef struct _NV_ENC_SEQUENCE_PARAM_PAYLOAD
+{
+    uint32_t            version;                         /**< [in]:  Struct version. Must be set to ::NV_ENC_INITIALIZE_PARAMS_VER. */
+    uint32_t            inBufferSize;                    /**< [in]:  Specifies the size of the spsppsBuffer provied by the client */
+    uint32_t            spsId;                           /**< [in]:  Specifies the SPS id to be used in sequence header. Default value is 0.  */
+    uint32_t            ppsId;                           /**< [in]:  Specifies the PPS id to be used in picture header. Default value is 0.  */
+    void*               spsppsBuffer;                    /**< [in]:  Specifies bitstream header pointer of size NV_ENC_SEQUENCE_PARAM_PAYLOAD::inBufferSize. It is the client's responsibility to manage this memory. */
+    uint32_t*           outSPSPPSPayloadSize;            /**< [out]: Size of the sequence and picture header in  bytes written by the NvEncodeAPI interface to the SPSPPSBuffer. */
+    uint32_t            reserved [250];                  /**< [in]:  Reserved and must be set to 0 */
+    void*               reserved2[64];                   /**< [in]:  Reserved and must be set to NULL */
+} NV_ENC_SEQUENCE_PARAM_PAYLOAD;
+
+/** Macro for constructing the version field of ::_NV_ENC_SEQUENCE_PARAM_PAYLOAD */
+#define NV_ENC_SEQUENCE_PARAM_PAYLOAD_VER NVENCAPI_STRUCT_VERSION(1)
+
+
+/**
+ * Event registration/unregistration parameters.
+ */
+typedef struct _NV_ENC_EVENT_PARAMS
+{
+    uint32_t            version;                          /**< [in]: Struct version. Must be set to ::NV_ENC_EVENT_PARAMS_VER. */
+    uint32_t            reserved;                         /**< [in]: Reserved and must be set to 0 */
+    void*               completionEvent;                  /**< [in]: Handle to event to be registered/unregistered with the NvEncodeAPI interface. */
+    uint32_t            reserved1[253];                   /**< [in]: Reserved and must be set to 0    */
+    void*               reserved2[64];                    /**< [in]: Reserved and must be set to NULL */
+} NV_ENC_EVENT_PARAMS;
+
+/** Macro for constructing the version field of ::_NV_ENC_EVENT_PARAMS */
+#define NV_ENC_EVENT_PARAMS_VER NVENCAPI_STRUCT_VERSION(1)
+
+/**
+ * Encoder Session Creation parameters
+ */
+typedef struct _NV_ENC_OPEN_ENCODE_SESSIONEX_PARAMS
+{
+    uint32_t            version;                          /**< [in]: Struct version. Must be set to ::NV_ENC_OPEN_ENCODE_SESSION_EX_PARAMS_VER. */
+    NV_ENC_DEVICE_TYPE  deviceType;                       /**< [in]: Specified the device Type */
+    void*               device;                           /**< [in]: Pointer to client device. */
+    void*               reserved;                         /**< [in]: Reserved and must be set to 0. */
+    uint32_t            apiVersion;                       /**< [in]: API version. Should be set to NVENCAPI_VERSION. */
+    uint32_t            reserved1[253];                   /**< [in]: Reserved and must be set to 0    */
+    void*               reserved2[64];                    /**< [in]: Reserved and must be set to NULL */
+} NV_ENC_OPEN_ENCODE_SESSION_EX_PARAMS;
+/** Macro for constructing the version field of ::_NV_ENC_OPEN_ENCODE_SESSIONEX_PARAMS */
+#define NV_ENC_OPEN_ENCODE_SESSION_EX_PARAMS_VER NVENCAPI_STRUCT_VERSION(1)
+
+/** @} */ /* END ENCODER_STRUCTURE */
+
+
+/**
+ * \addtogroup ENCODE_FUNC NvEncodeAPI Functions
+ * @{
+ */
+
+// NvEncOpenEncodeSession
+/**
+ * \brief Opens an encoding session.
+ * 
+ * Deprecated.
+ *
+ * \return
+ * ::NV_ENC_ERR_INVALID_CALL\n
+ *
+ */
+NVENCSTATUS NVENCAPI NvEncOpenEncodeSession                     (void* device, uint32_t deviceType, void** encoder);
+
+// NvEncGetEncodeGuidCount
+/**
+ * \brief Retrieves the number of supported encode GUIDs.
+ *
+ * The function returns the number of codec guids supported by the NvEncodeAPI
+ * interface.
+ *  
+ * \param [in] encoder  
+ *   Pointer to the NvEncodeAPI interface.
+ * \param [out] encodeGUIDCount 
+ *   Number of supported encode GUIDs.
+ *
+ * \return
+ * ::NV_ENC_SUCCESS \n
+ * ::NV_ENC_ERR_INVALID_PTR \n
+ * ::NV_ENC_ERR_INVALID_ENCODERDEVICE \n
+ * ::NV_ENC_ERR_DEVICE_NOT_EXIST \n
+ * ::NV_ENC_ERR_UNSUPPORTED_PARAM \n
+ * ::NV_ENC_ERR_OUT_OF_MEMORY \n
+ * ::NV_ENC_ERR_INVALID_PARAM \n
+ * ::NV_ENC_ERR_GENERIC \n
+ *
+ */
+NVENCSTATUS NVENCAPI NvEncGetEncodeGUIDCount                    (void* encoder, uint32_t* encodeGUIDCount);
+
+
+// NvEncGetEncodeGUIDs
+/**
+ * \brief Retrieves an array of supported encoder codec GUIDs.
+ *
+ * The function returns an array of codec guids supported by the NvEncodeAPI interface.
+ * The client must allocate an array where the NvEncodeAPI interface can
+ * fill the supported guids and pass the pointer in \p *GUIDs parameter.
+ * The size of the array can be determined by using ::NvEncGetEncodeGUIDCount() API.
+ * The Nvidia Encoding interface returns the number of codec guids it has actually
+ * filled in the guid array in the \p GUIDCount parameter.
+ *
+ * \param [in] encoder
+ *   Pointer to the NvEncodeAPI interface.
+ * \param [in] guidArraySize
+ *   Number of GUIDs to retrieved. Should be set to the number retrieved using
+ *   ::NvEncGetEncodeGUIDCount.
+ * \param [out] GUIDs
+ *   Array of supported Encode GUIDs.
+ * \param [out] GUIDCount
+ *   Number of supported Encode GUIDs.
+ *
+ * \return
+ * ::NV_ENC_SUCCESS \n
+ * ::NV_ENC_ERR_INVALID_PTR \n
+ * ::NV_ENC_ERR_INVALID_ENCODERDEVICE \n
+ * ::NV_ENC_ERR_DEVICE_NOT_EXIST \n
+ * ::NV_ENC_ERR_UNSUPPORTED_PARAM \n
+ * ::NV_ENC_ERR_OUT_OF_MEMORY \n
+ * ::NV_ENC_ERR_INVALID_PARAM \n
+ * ::NV_ENC_ERR_GENERIC \n
+ *
+ */
+NVENCSTATUS NVENCAPI NvEncGetEncodeGUIDs                        (void* encoder, GUID* GUIDs, uint32_t guidArraySize, uint32_t* GUIDCount);
+
+
+// NvEncGetEncodeProfileGuidCount
+/**
+ * \brief Retrieves the number of supported profile GUIDs.
+ *
+ * The function returns the number of profile GUIDs supported for a given codec. 
+ * The client must first enumerate the codec guids supported by the NvEncodeAPI 
+ * interface. After determining the codec guid, it can query the NvEncodeAPI
+ * interface to determine the number of profile guids supported for a particular
+ * codec guid.
+ *
+ * \param [in] encoder  
+ *   Pointer to the NvEncodeAPI interface.
+ * \param [in] encodeGUID 
+ *   The codec guid for which the profile guids are being enumerated.
+ * \param [out] encodeProfileGUIDCount
+ *   Number of encode profiles supported for the given encodeGUID.
+ *
+ * \return
+ * ::NV_ENC_SUCCESS \n
+ * ::NV_ENC_ERR_INVALID_PTR \n
+ * ::NV_ENC_ERR_INVALID_ENCODERDEVICE \n
+ * ::NV_ENC_ERR_DEVICE_NOT_EXIST \n
+ * ::NV_ENC_ERR_UNSUPPORTED_PARAM \n
+ * ::NV_ENC_ERR_OUT_OF_MEMORY \n
+ * ::NV_ENC_ERR_INVALID_PARAM \n
+ * ::NV_ENC_ERR_GENERIC \n
+ *
+ */
+NVENCSTATUS NVENCAPI NvEncGetEncodeProfileGUIDCount                    (void* encoder, GUID encodeGUID, uint32_t* encodeProfileGUIDCount);
+
+
+// NvEncGetEncodeProfileGUIDs
+/**
+ * \brief Retrieves an array of supported encode profile GUIDs.
+ *
+ * The function returns an array of supported profile guids for a particular
+ * codec guid. The client must allocate an array where the NvEncodeAPI interface
+ * can populate the profile guids. The client can determine the array size using 
+ * ::NvEncGetEncodeProfileGUIDCount() API. The client must also validiate that the
+ * NvEncodeAPI interface supports the GUID the client wants to pass as \p encodeGUID
+ * parameter.
+ *
+ * \param [in] encoder
+ *   Pointer to the NvEncodeAPI interface.
+ * \param [in] encodeGUID
+ *   The encode guid whose profile guids are being enumerated.
+ * \param [in] guidArraySize
+ *   Number of GUIDs to be retrieved. Should be set to the number retrieved using 
+ *   ::NvEncGetEncodeProfileGUIDCount.
+ * \param [out] profileGUIDs
+ *   Array of supported Encode Profile GUIDs
+ * \param [out] GUIDCount
+ *   Number of valid encode profile GUIDs in \p profileGUIDs array.
+ *
+ * \return
+ * ::NV_ENC_SUCCESS \n
+ * ::NV_ENC_ERR_INVALID_PTR \n
+ * ::NV_ENC_ERR_INVALID_ENCODERDEVICE \n
+ * ::NV_ENC_ERR_DEVICE_NOT_EXIST \n
+ * ::NV_ENC_ERR_UNSUPPORTED_PARAM \n
+ * ::NV_ENC_ERR_OUT_OF_MEMORY \n
+ * ::NV_ENC_ERR_INVALID_PARAM \n
+ * ::NV_ENC_ERR_GENERIC \n
+ *
+ */
+NVENCSTATUS NVENCAPI NvEncGetEncodeProfileGUIDs                               (void* encoder, GUID encodeGUID, GUID* profileGUIDs, uint32_t guidArraySize, uint32_t* GUIDCount);
+
+// NvEncGetInputFormatCount
+/**
+ * \brief Retrieve the number of supported Input formats.
+ *
+ * The function returns the number of supported input formats. The client must
+ * query the NvEncodeAPI interface to determine the supported input formats
+ * before creating the input surfaces.
+ *
+ * \param [in] encoder
+ *   Pointer to the NvEncodeAPI interface.
+ * \param [in] encodeGUID
+ *   Encode GUID, corresponding to which the number of supported input formats 
+ *   is to be retrieved.
+ * \param [out] inputFmtCount
+ *   Number of input formats supported for specified Encode GUID.
+ *
+ * \return
+ * ::NV_ENC_SUCCESS \n
+ * ::NV_ENC_ERR_INVALID_PTR \n
+ * ::NV_ENC_ERR_INVALID_ENCODERDEVICE \n
+ * ::NV_ENC_ERR_DEVICE_NOT_EXIST \n
+ * ::NV_ENC_ERR_UNSUPPORTED_PARAM \n
+ * ::NV_ENC_ERR_OUT_OF_MEMORY \n
+ * ::NV_ENC_ERR_INVALID_PARAM \n
+ * ::NV_ENC_ERR_GENERIC \n
+ */
+NVENCSTATUS NVENCAPI NvEncGetInputFormatCount                   (void* encoder, GUID encodeGUID, uint32_t* inputFmtCount);
+
+
+// NvEncGetInputFormats
+/**
+ * \brief Retrieves an array of supported Input formats
+ *
+ * Returns an array of supported input formats  The client must use the input 
+ * format to create input surface using ::NvEncCreateInputBuffer() API.
+ * 
+ * \param [in] encoder 
+ *   Pointer to the NvEncodeAPI interface.
+ * \param [in] encodeGUID
+ *   Encode GUID, corresponding to which the number of supported input formats 
+ *   is to be retrieved.
+ *\param [in] inputFmtArraySize
+ *   Size input format count array passed in \p inputFmts.
+ *\param [out] inputFmts
+ *   Array of input formats supported for this Encode GUID.
+ *\param [out] inputFmtCount
+ *   The number of valid input format types returned by the NvEncodeAPI
+ *   interface in \p inputFmts array.
+ *
+ * \return
+ * ::NV_ENC_SUCCESS \n
+ * ::NV_ENC_ERR_INVALID_PTR \n
+ * ::NV_ENC_ERR_INVALID_ENCODERDEVICE \n
+ * ::NV_ENC_ERR_DEVICE_NOT_EXIST \n
+ * ::NV_ENC_ERR_UNSUPPORTED_PARAM \n
+ * ::NV_ENC_ERR_OUT_OF_MEMORY \n
+ * ::NV_ENC_ERR_INVALID_PARAM \n
+ * ::NV_ENC_ERR_GENERIC \n
+ *
+ */
+NVENCSTATUS NVENCAPI NvEncGetInputFormats                       (void* encoder, GUID encodeGUID, NV_ENC_BUFFER_FORMAT* inputFmts, uint32_t inputFmtArraySize, uint32_t* inputFmtCount);
+
+
+// NvEncGetEncodeCaps
+/**
+ * \brief Retrieves the capability value for a specified encoder attribute.
+ *
+ * The function returns the capability value for a given encoder attribute. The 
+ * client must validate the encodeGUID using ::NvEncGetEncodeGUIDs() API before 
+ * calling this function. The encoder attribute being queried are enumerated in 
+ * ::NV_ENC_CAPS_PARAM enum.
+ *
+ * \param [in] encoder
+ *   Pointer to the NvEncodeAPI interface.
+ * \param [in] encodeGUID
+ *   Encode GUID, corresponding to which the capability attribute is to be retrieved.
+ * \param [in] capsParam
+ *   Used to specify attribute being queried. Refer ::NV_ENC_CAPS_PARAM for  more 
+ * details.
+ * \param [out] capsVal
+ *   The value corresponding to the capability attribute being queried.
+ *
+ * \return
+ * ::NV_ENC_SUCCESS \n
+ * ::NV_ENC_ERR_INVALID_PTR \n
+ * ::NV_ENC_ERR_INVALID_ENCODERDEVICE \n
+ * ::NV_ENC_ERR_DEVICE_NOT_EXIST \n
+ * ::NV_ENC_ERR_UNSUPPORTED_PARAM \n
+ * ::NV_ENC_ERR_OUT_OF_MEMORY \n
+ * ::NV_ENC_ERR_INVALID_PARAM \n
+ * ::NV_ENC_ERR_GENERIC \n
+ */
+NVENCSTATUS NVENCAPI NvEncGetEncodeCaps                     (void* encoder, GUID encodeGUID, NV_ENC_CAPS_PARAM* capsParam, int* capsVal);
+
+
+// NvEncGetEncodePresetCount
+/**
+ * \brief Retrieves the number of supported preset GUIDs.
+ *
+ * The function returns the number of preset GUIDs available for a given codec. 
+ * The client must validate the codec guid using ::NvEncGetEncodeGUIDs() API 
+ * before calling this function.
+ *
+ * \param [in] encoder
+ *   Pointer to the NvEncodeAPI interface.
+ * \param [in] encodeGUID
+ *   Encode GUID, corresponding to which the number of supported presets is to 
+ *   be retrieved.
+ * \param [out] encodePresetGUIDCount
+ *   Receives the number of supported preset GUIDs.
+ *
+ * \return
+ * ::NV_ENC_SUCCESS \n
+ * ::NV_ENC_ERR_INVALID_PTR \n
+ * ::NV_ENC_ERR_INVALID_ENCODERDEVICE \n
+ * ::NV_ENC_ERR_DEVICE_NOT_EXIST \n
+ * ::NV_ENC_ERR_UNSUPPORTED_PARAM \n
+ * ::NV_ENC_ERR_OUT_OF_MEMORY \n
+ * ::NV_ENC_ERR_INVALID_PARAM \n
+ * ::NV_ENC_ERR_GENERIC \n
+ *
+ */
+NVENCSTATUS NVENCAPI NvEncGetEncodePresetCount              (void* encoder, GUID encodeGUID, uint32_t* encodePresetGUIDCount);
+
+
+// NvEncGetEncodePresetGUIDs
+/**
+ * \brief Receives an array of supported encoder preset GUIDs.
+ *
+ * The function returns an array of encode preset guids available for a given codec. 
+ * The client can directly use one of the preset guids based upon the use case
+ * or target device. The preset guid chosen can be directly used in 
+ * NV_ENC_INITIALIZE_PARAMS::presetGUID parameter to ::NvEncEncodePicture() API. 
+ * Alternately client can  also use the preset guid to retrieve the encoding config 
+ * parameters being used by NvEncodeAPI interface for that given preset, using
+ * ::NvEncGetEncodePresetConfig() API. It can then modify preset config parameters
+ * as per its use case and send it to NvEncodeAPI interface as part of 
+ * NV_ENC_INITIALIZE_PARAMS::encodeConfig parameter for NvEncInitializeEncoder()
+ * API.
+ *
+ *
+ * \param [in] encoder 
+ *   Pointer to the NvEncodeAPI interface.
+ * \param [in] encodeGUID
+ *   Encode GUID, corresponding to which the list of supported presets is to be
+ *   retrieved.
+ * \param [in] guidArraySize
+ *   Size of array of preset guids passed in \p preset GUIDs
+ * \param [out] presetGUIDs
+ *   Array of supported Encode preset GUIDs from the NvEncodeAPI interface 
+ *   to client.
+ * \param [out] encodePresetGUIDCount
+ *   Receives the number of preset GUIDs returned by the NvEncodeAPI 
+ *   interface.
+ *
+ * \return
+ * ::NV_ENC_SUCCESS \n
+ * ::NV_ENC_ERR_INVALID_PTR \n
+ * ::NV_ENC_ERR_INVALID_ENCODERDEVICE \n
+ * ::NV_ENC_ERR_DEVICE_NOT_EXIST \n
+ * ::NV_ENC_ERR_UNSUPPORTED_PARAM \n
+ * ::NV_ENC_ERR_OUT_OF_MEMORY \n
+ * ::NV_ENC_ERR_INVALID_PARAM \n
+ * ::NV_ENC_ERR_GENERIC \n
+ *
+ */
+NVENCSTATUS NVENCAPI NvEncGetEncodePresetGUIDs                  (void* encoder, GUID encodeGUID, GUID* presetGUIDs, uint32_t guidArraySize, uint32_t* encodePresetGUIDCount);
+
+
+// NvEncGetEncodePresetConfig
+/**
+ * \brief Returns a preset config structure supported for given preset GUID.
+ *
+ * The function returns a preset config structure for a given preset guid. Before  
+ * using this function the client must enumerate the preset guids available for 
+ * a given codec. The preset config structure can be modified by the client depending
+ * upon its use case and can be then used to initialize the encoder using 
+ * ::NvEncInitializeEncoder() API. The client can use this function only if it 
+ * wants to modify the NvEncodeAPI preset configuration, otherwise it can 
+ * directly use the preset guid.
+ * 
+ * \param [in] encoder
+ *   Pointer to the NvEncodeAPI interface. 
+ * \param [in] encodeGUID
+ *   Encode GUID, corresponding to which the list of supported presets is to be
+ *   retrieved.
+ * \param [in] presetGUID
+ *   Preset GUID, corresponding to which the Encoding configurations is to be 
+ *   retrieved.
+ * \param [out] presetConfig
+ *   The requested Preset Encoder Attribute set. Refer ::_NV_ENC_CONFIG for
+*    more details.
+ *
+ * \return
+ * ::NV_ENC_SUCCESS \n
+ * ::NV_ENC_ERR_INVALID_PTR \n
+ * ::NV_ENC_ERR_INVALID_ENCODERDEVICE \n
+ * ::NV_ENC_ERR_DEVICE_NOT_EXIST \n
+ * ::NV_ENC_ERR_UNSUPPORTED_PARAM \n
+ * ::NV_ENC_ERR_OUT_OF_MEMORY \n
+ * ::NV_ENC_ERR_INVALID_PARAM \n
+ * ::NV_ENC_ERR_INVALID_VERSION \n
+ * ::NV_ENC_ERR_GENERIC \n
+ *
+ */
+NVENCSTATUS NVENCAPI NvEncGetEncodePresetConfig               (void* encoder, GUID encodeGUID, GUID  presetGUID, NV_ENC_PRESET_CONFIG* presetConfig);
+
+// NvEncInitializeEncoder
+/**
+ * \brief Initialize the encoder.
+ *
+ * This API must be used to initialize the encoder. The initialization parameter
+ * is passed using \p *createEncodeParams  The client must send the following
+ * fields of the _NV_ENC_INITIALIZE_PARAMS structure with a valid value.
+ * - NV_ENC_INITIALIZE_PARAMS::encodeGUID
+ * - NV_ENC_INITIALIZE_PARAMS::encodeWidth
+ * - NV_ENC_INITIALIZE_PARAMS::encodeHeight
+ * 
+ * The client can pass a preset guid directly to the NvEncodeAPI interface using
+ * NV_ENC_INITIALIZE_PARAMS::presetGUID field. If the client doesn't pass 
+ * NV_ENC_INITIALIZE_PARAMS::encodeConfig structure, the codec specific parameters
+ * will be selected based on the preset guid. The preset guid must have been 
+ * validated by the client using ::NvEncGetEncodePresetGUIDs() API.
+ * If the client passes a custom ::_NV_ENC_CONFIG structure through
+ * NV_ENC_INITIALIZE_PARAMS::encodeConfig , it will override the codec specific parameters
+ * based on the preset guid. It is recommended that even if the client passes a custom config,
+ * it should also send a preset guid. In this case, the preset guid passed by the client
+ * will not override any of the custom config parameters programmed by the client,
+ * it is only used as a hint by the NvEncodeAPI interface to determine certain encoder parameters
+ * which are not exposed to the client.
+ *
+ * There are two modes of operation for the encoder namely:
+ * - Asynchronous mode
+ * - Synchronous mode
+ *
+ * The client can select asynchronous or synchronous mode by setting the \p
+ * enableEncodeAsync field in ::_NV_ENC_INITIALIZE_PARAMS to 1 or 0 respectively.
+ *\par Asynchronous mode of operation:
+ * The Asynchronous mode can be enabled by setting NV_ENC_INITIALIZE_PARAMS::enableEncodeAsync to 1.
+ * The client operating in asynchronous mode must allocate completion event object
+ * for each output buffer and pass the completion event object in the
+ * ::NvEncEncodePicture() API. The client can create another thread and wait on
+ * the event object to be signalled by NvEncodeAPI interface on completion of the
+ * encoding process for the output frame. This should unblock the main thread from
+ * submitting work to the encoder. When the event is signalled the client can call
+ * NvEncodeAPI interfaces to copy the bitstream data using ::NvEncLockBitstream()
+ * API. This is the preferred mode of operation.
+ *
+ * NOTE: Asynchronous mode is not supported on Linux.
+ *
+ *\par Synchronous mode of operation:
+ * The client can select synchronous mode by setting NV_ENC_INITIALIZE_PARAMS::enableEncodeAsync to 0.
+ * The client working in synchronous mode can work in a single threaded or multi
+ * threaded mode. The client need not allocate any event objects. The client can
+ * only lock the bitstream data after NvEncodeAPI interface has returned
+ * ::NV_ENC_SUCCESS from encode picture. The NvEncodeAPI interface can return 
+ * ::NV_ENC_ERR_NEED_MORE_INPUT error code from ::NvEncEncodePicture() API. The
+ * client must not lock the output buffer in such case but should send the next
+ * frame for encoding. The client must keep on calling ::NvEncEncodePicture() API
+ * until it returns ::NV_ENC_SUCCESS. \n
+ * The client must always lock the bitstream data in order in which it has submitted.
+ * This is true for both asynchronous and synchronous mode.
+ *
+ *\par Picture type decision:
+ * If the client is taking the picture type decision and it must disable the picture
+ * type decision module in NvEncodeAPI by setting NV_ENC_INITIALIZE_PARAMS::enablePTD
+ * to 0. In this case the client is  required to send the picture in encoding 
+ * order to NvEncodeAPI by doing the re-ordering for B frames. \n
+ * If the client doesn't want to take the picture type decision it can enable 
+ * picture type decision module in the NvEncodeAPI interface by setting 
+ * NV_ENC_INITIALIZE_PARAMS::enablePTD to 1 and send the input pictures in display 
+ * order.
+ * 
+ * \param [in] encoder
+ *   Pointer to the NvEncodeAPI interface.
+ * \param [in] createEncodeParams 
+ *   Refer ::_NV_ENC_INITIALIZE_PARAMS for details.
+ *
+ * \return
+ * ::NV_ENC_SUCCESS \n
+ * ::NV_ENC_ERR_INVALID_PTR \n
+ * ::NV_ENC_ERR_INVALID_ENCODERDEVICE \n
+ * ::NV_ENC_ERR_DEVICE_NOT_EXIST \n
+ * ::NV_ENC_ERR_UNSUPPORTED_PARAM \n
+ * ::NV_ENC_ERR_OUT_OF_MEMORY \n
+ * ::NV_ENC_ERR_INVALID_PARAM \n
+ * ::NV_ENC_ERR_INVALID_VERSION \n
+ * ::NV_ENC_ERR_GENERIC \n
+ *
+ */
+NVENCSTATUS NVENCAPI NvEncInitializeEncoder                     (void* encoder, NV_ENC_INITIALIZE_PARAMS* createEncodeParams);
+
+
+// NvEncCreateInputBuffer
+/**
+ * \brief Allocates Input buffer.
+ *
+ * This function is used to allocate an input buffer. The client must enumerate
+ * the input buffer format before allocating the input buffer resources. The 
+ * NV_ENC_INPUT_PTR returned by the NvEncodeAPI interface in the 
+ * NV_ENC_CREATE_INPUT_BUFFER::inputBuffer field can be directly used in
+ * ::NvEncEncodePicture() API. The number of input buffers to be allocated by the 
+ * client must be at least 4 more than the number of B frames being used for encoding.
+ *
+ * \param [in] encoder
+ *   Pointer to the NvEncodeAPI interface.
+ * \param [in,out] createInputBufferParams
+ *  Pointer to the ::NV_ENC_CREATE_INPUT_BUFFER structure.
+ *
+ * \return
+ * ::NV_ENC_SUCCESS \n
+ * ::NV_ENC_ERR_INVALID_PTR \n
+ * ::NV_ENC_ERR_INVALID_ENCODERDEVICE \n
+ * ::NV_ENC_ERR_DEVICE_NOT_EXIST \n
+ * ::NV_ENC_ERR_UNSUPPORTED_PARAM \n
+ * ::NV_ENC_ERR_OUT_OF_MEMORY \n
+ * ::NV_ENC_ERR_INVALID_PARAM \n
+ * ::NV_ENC_ERR_INVALID_VERSION \n
+ * ::NV_ENC_ERR_GENERIC \n
+ * 
+ */
+NVENCSTATUS NVENCAPI NvEncCreateInputBuffer                     (void* encoder, NV_ENC_CREATE_INPUT_BUFFER* createInputBufferParams);
+
+
+// NvEncDestroyInputBuffer
+/**
+ * \brief Release an input buffers.
+ *
+ * This function is used to free an input buffer. If the client has allocated
+ * any input buffer using ::NvEncCreateInputBuffer() API, it must free those
+ * input buffers by calling this function. The client must release the input
+ * buffers before destroying the encoder using ::NvEncDestroyEncoder() API.
+ *
+ * \param [in] encoder
+ *   Pointer to the NvEncodeAPI interface.
+ * \param [in] inputBuffer 
+ *   Pointer to the input buffer to be released.
+ *
+ * \return
+ * ::NV_ENC_SUCCESS \n
+ * ::NV_ENC_ERR_INVALID_PTR \n
+ * ::NV_ENC_ERR_INVALID_ENCODERDEVICE \n
+ * ::NV_ENC_ERR_DEVICE_NOT_EXIST \n
+ * ::NV_ENC_ERR_UNSUPPORTED_PARAM \n
+ * ::NV_ENC_ERR_OUT_OF_MEMORY \n
+ * ::NV_ENC_ERR_INVALID_PARAM \n
+ * ::NV_ENC_ERR_INVALID_VERSION \n
+ * ::NV_ENC_ERR_GENERIC \n
+ *
+ */
+NVENCSTATUS NVENCAPI NvEncDestroyInputBuffer                    (void* encoder, NV_ENC_INPUT_PTR inputBuffer);
+
+
+// NvEncCreateBitstreamBuffer
+/**
+ * \brief Allocates an output bitstream buffer 
+ *
+ * This function is used to allocate an output bitstream buffer and returns a 
+ * NV_ENC_OUTPUT_PTR to bitstream  buffer to the client in the 
+ * NV_ENC_CREATE_BITSTREAM_BUFFER::bitstreamBuffer field.
+ * The client can only call this function after the encoder session has been 
+ * initialized using ::NvEncInitializeEncoder() API. The minimum number of output 
+ * buffers allocated by the client must be at least 4 more than the number of B
+ * B frames being used for encoding. The client can only access the output 
+ * bitsteam data by locking the \p bitstreamBuffer using the ::NvEncLockBitstream()
+ * function.
+ *
+ * \param [in] encoder
+ *   Pointer to the NvEncodeAPI interface.
+ * \param [in,out] createBitstreamBufferParams
+ *   Pointer ::NV_ENC_CREATE_BITSTREAM_BUFFER for details.
+ *
+ * \return
+ * ::NV_ENC_SUCCESS \n
+ * ::NV_ENC_ERR_INVALID_PTR \n
+ * ::NV_ENC_ERR_INVALID_ENCODERDEVICE \n
+ * ::NV_ENC_ERR_DEVICE_NOT_EXIST \n
+ * ::NV_ENC_ERR_UNSUPPORTED_PARAM \n
+ * ::NV_ENC_ERR_OUT_OF_MEMORY \n
+ * ::NV_ENC_ERR_INVALID_PARAM \n
+ * ::NV_ENC_ERR_INVALID_VERSION \n
+ * ::NV_ENC_ERR_ENCODER_NOT_INITIALIZED \n
+ * ::NV_ENC_ERR_GENERIC \n
+ *
+ */
+NVENCSTATUS NVENCAPI NvEncCreateBitstreamBuffer                 (void* encoder, NV_ENC_CREATE_BITSTREAM_BUFFER* createBitstreamBufferParams);
+
+
+// NvEncDestroyBitstreamBuffer
+/**
+ * \brief Release a bitstream buffer. 
+ *
+ * This function is used to release the output bitstream buffer allocated using
+ * the ::NvEncCreateBitstreamBuffer() function. The client must release the output
+ * bitstreamBuffer using this function before destroying the encoder session.
+ *
+ * \param [in] encoder
+ *   Pointer to the NvEncodeAPI interface.
+ * \param [in] bitstreamBuffer
+ *   Pointer to the bitstream buffer being released.
+ *
+ * \return
+ * ::NV_ENC_SUCCESS \n
+ * ::NV_ENC_ERR_INVALID_PTR \n
+ * ::NV_ENC_ERR_INVALID_ENCODERDEVICE \n
+ * ::NV_ENC_ERR_DEVICE_NOT_EXIST \n
+ * ::NV_ENC_ERR_UNSUPPORTED_PARAM \n
+ * ::NV_ENC_ERR_OUT_OF_MEMORY \n
+ * ::NV_ENC_ERR_INVALID_PARAM \n
+ * ::NV_ENC_ERR_INVALID_VERSION \n
+ * ::NV_ENC_ERR_ENCODER_NOT_INITIALIZED \n
+ * ::NV_ENC_ERR_GENERIC \n
+ *
+ */
+NVENCSTATUS NVENCAPI NvEncDestroyBitstreamBuffer                (void* encoder, NV_ENC_OUTPUT_PTR bitstreamBuffer);
+
+// NvEncEncodePicture
+/**
+ * \brief Submit an input picture for encoding.
+ *
+ * This function is used to submit an input picture buffer for encoding. The 
+ * encoding parameters are passed using \p *encodePicParams which is a pointer
+ * to the ::_NV_ENC_PIC_PARAMS structure.
+ *
+ * If the client has set NV_ENC_INITIALIZE_PARAMS::enablePTD to 0, then it must
+ * send a valid value for the following fields.
+ * - NV_ENC_PIC_PARAMS::pictureType
+ * - NV_ENC_PIC_PARAMS_H264::displayPOCSyntax (H264 only)
+ * - NV_ENC_PIC_PARAMS_H264::frameNumSyntax(H264 only)
+ * - NV_ENC_PIC_PARAMS_H264::refPicFlag(H264 only)
+ *
+ *
+ *\par Asynchronous Encoding
+ * If the client has enabled asynchronous mode of encoding by setting 
+ * NV_ENC_INITIALIZE_PARAMS::enableEncodeAsync to 1 in the ::NvEncInitializeEncoder()
+ * API ,then the client must send a valid NV_ENC_PIC_PARAMS::completionEvent.
+ * Incase of asynchronous mode of operation, client can queue the ::NvEncEncodePicture()
+ * API commands from the main thread and then queue output buffers to be processed 
+ * to a secondary worker thread. Before the locking the output buffers in the 
+ * secondary thread , the client must wait on NV_ENC_PIC_PARAMS::completionEvent
+ * it has queued in ::NvEncEncodePicture() API call. The client must always process
+ * completion event and the output buffer in the same order in which they have been
+ * submitted for encoding. The NvEncodeAPI interface is responsible for any 
+ * re-ordering required for B frames and will always ensure that encoded bitstream
+ * data is written in the same order in which output buffer is submitted.
+ *\code
+  The below example shows how  asynchronous encoding in case of 1 B frames
+  ------------------------------------------------------------------------
+  Suppose the client allocated 4 input buffers(I1,I2..), 4 output buffers(O1,O2..) 
+  and 4 completion events(E1, E2, ...). The NvEncodeAPI interface will need to 
+  keep a copy of the input buffers for re-ordering and it allocates following 
+  internal buffers (NvI1, NvI2...). These internal buffers are managed by NvEncodeAPI
+  and the client is not responsible for the allocating or freeing the memory of 
+  the internal buffers.
+
+  a) The client main thread will queue the following encode frame calls. 
+  Note the picture type is unknown to the client, the decision is being taken by 
+  NvEncodeAPI interface. The client should pass ::_NV_ENC_PIC_PARAMS parameter  
+  consisting of allocated input buffer, output buffer and output events in successive 
+  ::NvEncEncodePicture() API calls along with other required encode picture params.
+  For example:
+  1st EncodePicture parameters - (I1, O1, E1)
+  2nd EncodePicture parameters - (I2, O2, E2)
+  3rd EncodePicture parameters - (I3, O3, E3)
+
+  b) NvEncodeAPI SW will receive the following encode Commands from the client. 
+  The left side shows input from client in the form (Input buffer, Output Buffer, 
+  Output Event). The right hand side shows a possible picture type decision take by
+  the NvEncodeAPI interface.
+  (I1, O1, E1)    ---P1 Frame
+  (I2, O2, E2)    ---B2 Frame
+  (I3, O3, E3)    ---P3 Frame
+
+  c) NvEncodeAPI interface will make a copy of the input buffers to its internal  
+   buffersfor re-ordering. These copies are done as part of nvEncEncodePicture  
+   function call from the client and NvEncodeAPI interface is responsible for  
+   synchronization of copy operation with the actual encoding operation.
+   I1 --> NvI1  
+   I2 --> NvI2 
+   I3 --> NvI3
+
+  d) After returning from ::NvEncEncodePicture() call , the client must queue the output
+   bitstream  processing work to the secondary thread. The output bitstream processing
+   for asynchronous mode consist of first waiting on completion event(E1, E2..)
+   and then locking the output bitstream buffer(O1, O2..) for reading the encoded
+   data. The work queued to the secondary thread by the client is in the following order
+   (I1, O1, E1)
+   (I2, O2, E2)
+   (I3, O3, E3)
+   Note they are in the same order in which client calls ::NvEncEncodePicture() API 
+   in \p step a).
+
+  e) NvEncodeAPI interface  will do the re-ordering such that Encoder HW will receive 
+  the following encode commands:
+  (NvI1, O1, E1)   ---P1 Frame
+  (NvI3, O2, E2)   ---P3 Frame
+  (NvI2, O3, E3)   ---B2 frame
+
+  f) After the encoding operations are completed, the events will be signalled 
+  by NvEncodeAPI interface in the following order :
+  (O1, E1) ---P1 Frame ,output bitstream copied to O1 and event E1 signalled.
+  (O2, E2) ---P3 Frame ,output bitstream copied to O2 and event E2 signalled.
+  (O3, E3) ---B2 Frame ,output bitstream copied to O3 and event E3 signalled.
+
+  g) The client must lock the bitstream data using ::NvEncLockBitstream() API in 
+   the order O1,O2,O3  to read the encoded data, after waiting for the events
+   to be signalled in the same order i.e E1, E2 and E3.The output processing is
+   done in the secondary thread in the following order:
+   Waits on E1, copies encoded bitstream from O1
+   Waits on E2, copies encoded bitstream from O2
+   Waits on E3, copies encoded bitstream from O3
+
+  -Note the client will receive the events signalling and output buffer in the 
+   same order in which they have submitted for encoding.
+  -Note the LockBitstream will have picture type field which will notify the 
+   output picture type to the clients.
+  -Note the input, output buffer and the output completion event are free to be 
+   reused once NvEncodeAPI interfaced has signalled the event and the client has
+   copied the data from the output buffer.
+
+ * \endcode
+ *
+ *\par Synchronous Encoding
+ * The client can enable synchronous mode of encoding by setting 
+ * NV_ENC_INITIALIZE_PARAMS::enableEncodeAsync to 0 in ::NvEncInitializeEncoder() API.
+ * The NvEncodeAPI interface may return ::NV_ENC_ERR_NEED_MORE_INPUT error code for
+ * some ::NvEncEncodePicture() API calls when NV_ENC_INITIALIZE_PARAMS::enablePTD 
+ * is set to 1, but the client must not treat it as a fatal error. The NvEncodeAPI 
+ * interface might not be able to submit an input picture buffer for encoding 
+ * immediately due to re-ordering for B frames. The NvEncodeAPI interface cannot 
+ * submit the input picture which is decided to be encoded as B frame as it waits 
+ * for backward reference from  temporally subsequent frames. This input picture
+ * is buffered internally and waits for more input picture to arrive. The client
+ * must not call ::NvEncLockBitstream() API on the output buffers whose 
+ * ::NvEncEncodePicture() API returns ::NV_ENC_ERR_NEED_MORE_INPUT. The client must 
+ * wait for the NvEncodeAPI interface to return ::NV_ENC_SUCCESS before locking the 
+ * output bitstreams to read the encoded bitstream data. The following example
+ * explains the scenario with synchronous encoding with 2 B frames.
+ *\code
+ The below example shows how  synchronous encoding works in case of 1 B frames
+ -----------------------------------------------------------------------------
+ Suppose the client allocated 4 input buffers(I1,I2..), 4 output buffers(O1,O2..) 
+ and 4 completion events(E1, E2, ...). The NvEncodeAPI interface will need to 
+ keep a copy of the input buffers for re-ordering and it allocates following 
+ internal buffers (NvI1, NvI2...). These internal buffers are managed by NvEncodeAPI
+ and the client is not responsible for the allocating or freeing the memory of 
+ the internal buffers.
+
+ The client calls ::NvEncEncodePicture() API with input buffer I1 and output buffer O1.
+ The NvEncodeAPI decides to encode I1 as P frame and submits it to encoder
+ HW and returns ::NV_ENC_SUCCESS. 
+ The client can now read the encoded data by locking the output O1 by calling
+ NvEncLockBitstream API.
+
+ The client calls ::NvEncEncodePicture() API with input buffer I2 and output buffer O2.
+ The NvEncodeAPI decides to encode I2 as B frame and buffers I2 by copying it
+ to internal buffer and returns ::NV_ENC_ERR_NEED_MORE_INPUT.
+ The error is not fatal and it notifies client that it cannot read the encoded 
+ data by locking the output O2 by calling ::NvEncLockBitstream() API without submitting
+ more work to the NvEncodeAPI interface.
+  
+ The client calls ::NvEncEncodePicture() with input buffer I3 and output buffer O3.
+ The NvEncodeAPI decides to encode I3 as P frame and it first submits I3 for 
+ encoding which will be used as backward reference frame for I2.
+ The NvEncodeAPI then submits I2 for encoding and returns ::NV_ENC_SUCESS. Both
+ the submission are part of the same ::NvEncEncodePicture() function call.
+ The client can now read the encoded data for both the frames by locking the output
+ O2 followed by  O3 ,by calling ::NvEncLockBitstream() API.
+
+ The client must always lock the output in the same order in which it has submitted
+ to receive the encoded bitstream in correct encoding order.
+
+ * \endcode
+ *
+ * \param [in] encoder
+ *   Pointer to the NvEncodeAPI interface.
+ * \param [in,out] encodePicParams
+ *   Pointer to the ::_NV_ENC_PIC_PARAMS structure.
+ *
+ * \return
+ * ::NV_ENC_SUCCESS \n
+ * ::NV_ENC_ERR_INVALID_PTR \n
+ * ::NV_ENC_ERR_INVALID_ENCODERDEVICE \n
+ * ::NV_ENC_ERR_DEVICE_NOT_EXIST \n
+ * ::NV_ENC_ERR_UNSUPPORTED_PARAM \n
+ * ::NV_ENC_ERR_OUT_OF_MEMORY \n
+ * ::NV_ENC_ERR_INVALID_PARAM \n
+ * ::NV_ENC_ERR_INVALID_VERSION \n
+ * ::NV_ENC_ERR_ENCODER_BUSY \n
+ * ::NV_ENC_ERR_NEED_MORE_INPUT \n
+ * ::NV_ENC_ERR_ENCODER_NOT_INITIALIZED \n
+ * ::NV_ENC_ERR_GENERIC \n
+ *
+ */
+NVENCSTATUS NVENCAPI NvEncEncodePicture                         (void* encoder, NV_ENC_PIC_PARAMS* encodePicParams);
+
+
+// NvEncLockBitstream
+/**
+ * \brief Lock output bitstream buffer
+ *
+ * This function is used to lock the bitstream buffer to read the encoded data.
+ * The client can only access the encoded data by calling this function. 
+ * The pointer to client accessible encoded data is returned in the 
+ * NV_ENC_LOCK_BITSTREAM::bitstreamBufferPtr field. The size of the encoded data
+ * in the output buffer is returned in the NV_ENC_LOCK_BITSTREAM::bitstreamSizeInBytes
+ * The NvEncodeAPI interface also returns the output picture type and picture structure 
+ * of the encoded frame in NV_ENC_LOCK_BITSTREAM::pictureType and
+ * NV_ENC_LOCK_BITSTREAM::pictureStruct fields respectively. If the client has
+ * set NV_ENC_LOCK_BITSTREAM::doNotWait to 1, the function might return
+ * ::NV_ENC_ERR_LOCK_BUSY if client is operating in synchronous mode. This is not 
+ * a fatal failure if NV_ENC_LOCK_BITSTREAM::doNotWait is set to 1. In the above case the client can 
+ * retry the function after few milliseconds.
+ *
+ * \param [in] encoder
+ *   Pointer to the NvEncodeAPI interface.
+ * \param [in,out] lockBitstreamBufferParams
+ *   Pointer to the ::_NV_ENC_LOCK_BITSTREAM structure.
+ *
+ * \return
+ * ::NV_ENC_SUCCESS \n
+ * ::NV_ENC_ERR_INVALID_PTR \n
+ * ::NV_ENC_ERR_INVALID_ENCODERDEVICE \n
+ * ::NV_ENC_ERR_DEVICE_NOT_EXIST \n
+ * ::NV_ENC_ERR_UNSUPPORTED_PARAM \n
+ * ::NV_ENC_ERR_OUT_OF_MEMORY \n
+ * ::NV_ENC_ERR_INVALID_PARAM \n
+ * ::NV_ENC_ERR_INVALID_VERSION \n
+ * ::NV_ENC_ERR_LOCK_BUSY \n
+ * ::NV_ENC_ERR_ENCODER_NOT_INITIALIZED \n
+ * ::NV_ENC_ERR_GENERIC \n
+ *
+ */
+NVENCSTATUS NVENCAPI NvEncLockBitstream                         (void* encoder, NV_ENC_LOCK_BITSTREAM* lockBitstreamBufferParams);
+
+
+// NvEncUnlockBitstream
+/**
+ * \brief Unlock the output bitstream buffer
+ *
+ * This function is used to unlock the output bitstream buffer after the client
+ * has read the encoded data from output buffer. The client must call this function
+ * to unlock the output buffer which it has previously locked using ::NvEncLockBitstream()
+ * function. Using a locked bitstream buffer in ::NvEncEncodePicture() API will cause
+ * the function to fail.
+ *
+ * \param [in] encoder
+ *   Pointer to the NvEncodeAPI interface.
+ * \param [in,out] bitstreamBuffer
+ *   bitstream buffer pointer being unlocked
+ *
+ * \return
+ * ::NV_ENC_SUCCESS \n
+ * ::NV_ENC_ERR_INVALID_PTR \n
+ * ::NV_ENC_ERR_INVALID_ENCODERDEVICE \n
+ * ::NV_ENC_ERR_DEVICE_NOT_EXIST \n
+ * ::NV_ENC_ERR_UNSUPPORTED_PARAM \n
+ * ::NV_ENC_ERR_OUT_OF_MEMORY \n
+ * ::NV_ENC_ERR_INVALID_PARAM \n
+ * ::NV_ENC_ERR_ENCODER_NOT_INITIALIZED \n
+ * ::NV_ENC_ERR_GENERIC \n
+ *
+ */
+NVENCSTATUS NVENCAPI NvEncUnlockBitstream                       (void* encoder, NV_ENC_OUTPUT_PTR bitstreamBuffer);
+
+
+// NvLockInputBuffer
+/**
+ * \brief Locks an input buffer
+ *
+ * This function is used to lock the input buffer to load the uncompressed YUV
+ * pixel data into input buffer memory. The client must pass the NV_ENC_INPUT_PTR
+ * it had previously allocated using ::NvEncCreateInputBuffer()in the
+ * NV_ENC_LOCK_INPUT_BUFFER::inputBuffer field. 
+ * The NvEncodeAPI interface returns pointer to client accessible input buffer 
+ * memory in NV_ENC_LOCK_INPUT_BUFFER::bufferDataPtr field. 
+ *
+ * \param [in] encoder
+ *   Pointer to the NvEncodeAPI interface.
+ * \param [in,out] lockInputBufferParams
+ *   Pointer to the ::_NV_ENC_LOCK_INPUT_BUFFER structure
+ *
+ * \return
+ * \return
+ * ::NV_ENC_SUCCESS \n
+ * ::NV_ENC_ERR_INVALID_PTR \n
+ * ::NV_ENC_ERR_INVALID_ENCODERDEVICE \n
+ * ::NV_ENC_ERR_DEVICE_NOT_EXIST \n
+ * ::NV_ENC_ERR_UNSUPPORTED_PARAM \n
+ * ::NV_ENC_ERR_OUT_OF_MEMORY \n
+ * ::NV_ENC_ERR_INVALID_PARAM \n
+ * ::NV_ENC_ERR_INVALID_VERSION \n
+ * ::NV_ENC_ERR_LOCK_BUSY \n
+ * ::NV_ENC_ERR_ENCODER_NOT_INITIALIZED \n
+ * ::NV_ENC_ERR_GENERIC \n
+ *
+ */
+NVENCSTATUS NVENCAPI NvEncLockInputBuffer                      (void* encoder, NV_ENC_LOCK_INPUT_BUFFER* lockInputBufferParams);
+
+
+// NvUnlockInputBuffer
+/**
+ * \brief Unlocks the input buffer
+ *
+ * This function is used to unlock the input buffer memory previously locked for
+ * uploading YUV pixel data. The input buffer must be unlocked before being used
+ * again for encoding, otherwise NvEncodeAPI will fail the ::NvEncEncodePicture()
+ *
+  * \param [in] encoder
+ *   Pointer to the NvEncodeAPI interface.
+ * \param [in] inputBuffer
+ *   Pointer to the input buffer that is being unlocked.
+ *
+ * \return
+ * ::NV_ENC_SUCCESS \n
+ * ::NV_ENC_ERR_INVALID_PTR \n
+ * ::NV_ENC_ERR_INVALID_ENCODERDEVICE \n
+ * ::NV_ENC_ERR_DEVICE_NOT_EXIST \n
+ * ::NV_ENC_ERR_UNSUPPORTED_PARAM \n
+ * ::NV_ENC_ERR_OUT_OF_MEMORY \n
+ * ::NV_ENC_ERR_INVALID_VERSION \n
+ * ::NV_ENC_ERR_INVALID_PARAM \n
+ * ::NV_ENC_ERR_ENCODER_NOT_INITIALIZED \n
+ * ::NV_ENC_ERR_GENERIC \n
+ *
+ *
+ */
+NVENCSTATUS NVENCAPI NvEncUnlockInputBuffer                     (void* encoder, NV_ENC_INPUT_PTR inputBuffer);
+
+
+// NvEncGetEncodeStats
+/**
+ * \brief Get encoding statistics.
+ *
+ * This function is used to retrieve the encoding statistics.
+ * This API is not supported when encode device type is CUDA.
+ *
+ * \param [in] encoder
+ *   Pointer to the NvEncodeAPI interface.
+ * \param [in,out] encodeStats
+ *   Pointer to the ::_NV_ENC_STAT structure.
+ *
+ * \return
+ * ::NV_ENC_SUCCESS \n
+ * ::NV_ENC_ERR_INVALID_PTR \n
+ * ::NV_ENC_ERR_INVALID_ENCODERDEVICE \n
+ * ::NV_ENC_ERR_DEVICE_NOT_EXIST \n
+ * ::NV_ENC_ERR_UNSUPPORTED_PARAM \n
+ * ::NV_ENC_ERR_OUT_OF_MEMORY \n
+ * ::NV_ENC_ERR_INVALID_PARAM \n
+ * ::NV_ENC_ERR_ENCODER_NOT_INITIALIZED \n
+ * ::NV_ENC_ERR_GENERIC \n
+ *
+ */
+NVENCSTATUS NVENCAPI NvEncGetEncodeStats                        (void* encoder, NV_ENC_STAT* encodeStats);
+
+
+// NvEncGetSequenceParams
+/**
+ * \brief Get encoded sequence and picture header.
+ *
+ * This function can be used to retrieve the sequence and picture header out of 
+ * band. The client must call this function only after the encoder has been 
+ * initialized using ::NvEncInitializeEncoder() function. The client must 
+ * allocate the memory where the NvEncodeAPI interface can copy the bitstream
+ * header and pass the pointer to the memory in NV_ENC_SEQUENCE_PARAM_PAYLOAD::spsppsBuffer. 
+ * The size of buffer is passed in the field  NV_ENC_SEQUENCE_PARAM_PAYLOAD::inBufferSize.
+ * The NvEncodeAPI interface will copy the bitstream header payload and returns 
+ * the actual size of the bitstream header in the field
+ * NV_ENC_SEQUENCE_PARAM_PAYLOAD::outSPSPPSPayloadSize.
+ * The client must call  ::NvEncGetSequenceParams() function from the same thread which is 
+ * being used to call ::NvEncEncodePicture() function.
+ *
+ * \param [in] encoder
+ *   Pointer to the NvEncodeAPI interface.
+ * \param [in,out] sequenceParamPayload
+ *   Pointer to the ::_NV_ENC_SEQUENCE_PARAM_PAYLOAD structure.
+ *
+ * \return
+ * ::NV_ENC_SUCCESS \n
+ * ::NV_ENC_ERR_INVALID_PTR \n
+ * ::NV_ENC_ERR_INVALID_ENCODERDEVICE \n
+ * ::NV_ENC_ERR_DEVICE_NOT_EXIST \n
+ * ::NV_ENC_ERR_UNSUPPORTED_PARAM \n
+ * ::NV_ENC_ERR_OUT_OF_MEMORY \n
+ * ::NV_ENC_ERR_INVALID_VERSION \n
+ * ::NV_ENC_ERR_INVALID_PARAM \n
+ * ::NV_ENC_ERR_ENCODER_NOT_INITIALIZED \n
+ * ::NV_ENC_ERR_GENERIC \n 
+ *
+ */
+NVENCSTATUS NVENCAPI NvEncGetSequenceParams                     (void* encoder, NV_ENC_SEQUENCE_PARAM_PAYLOAD* sequenceParamPayload);
+
+
+// NvEncRegisterAsyncEvent
+/**
+ * \brief Register event for notification to encoding completion.
+ *
+ * This function is used to register the completion event with NvEncodeAPI 
+ * interface. The event is required when the client has configured the encoder to 
+ * work in asynchronous mode. In this mode the client needs to send a completion
+ * event with every output buffer. The NvEncodeAPI interface will signal the 
+ * completion of the encoding process using this event. Only after the event is 
+ * signalled the client can get the encoded data using ::NvEncLockBitstream() function.
+ *
+ * \param [in] encoder
+ *   Pointer to the NvEncodeAPI interface.
+ * \param [in] eventParams
+ *   Pointer to the ::_NV_ENC_EVENT_PARAMS structure.
+ *
+ * \return
+ * ::NV_ENC_SUCCESS \n
+ * ::NV_ENC_ERR_INVALID_PTR \n
+ * ::NV_ENC_ERR_INVALID_ENCODERDEVICE \n
+ * ::NV_ENC_ERR_DEVICE_NOT_EXIST \n
+ * ::NV_ENC_ERR_UNSUPPORTED_PARAM \n
+ * ::NV_ENC_ERR_OUT_OF_MEMORY \n
+ * ::NV_ENC_ERR_INVALID_VERSION \n
+ * ::NV_ENC_ERR_INVALID_PARAM \n
+ * ::NV_ENC_ERR_ENCODER_NOT_INITIALIZED \n
+ * ::NV_ENC_ERR_GENERIC \n
+ *
+ */
+NVENCSTATUS NVENCAPI NvEncRegisterAsyncEvent                    (void* encoder, NV_ENC_EVENT_PARAMS* eventParams);
+
+
+// NvEncUnregisterAsyncEvent
+/**
+ * \brief Unregister completion event.
+ *
+ * This function is used to unregister completion event which has been previously
+ * registered using ::NvEncRegisterAsyncEvent() function. The client must unregister
+ * all events before destroying the encoder using ::NvEncDestroyEncoder() function.
+ *
+  * \param [in] encoder
+ *   Pointer to the NvEncodeAPI interface.
+ * \param [in] eventParams
+ *   Pointer to the ::_NV_ENC_EVENT_PARAMS structure.
+ *
+ * \return
+ * ::NV_ENC_SUCCESS \n
+ * ::NV_ENC_ERR_INVALID_PTR \n
+ * ::NV_ENC_ERR_INVALID_ENCODERDEVICE \n
+ * ::NV_ENC_ERR_DEVICE_NOT_EXIST \n
+ * ::NV_ENC_ERR_UNSUPPORTED_PARAM \n
+ * ::NV_ENC_ERR_OUT_OF_MEMORY \n
+ * ::NV_ENC_ERR_INVALID_VERSION \n
+ * ::NV_ENC_ERR_INVALID_PARAM \n
+ * ::NV_ENC_ERR_ENCODER_NOT_INITIALIZED \n
+ * ::NV_ENC_ERR_GENERIC \n
+ *
+ */
+NVENCSTATUS NVENCAPI NvEncUnregisterAsyncEvent                  (void* encoder, NV_ENC_EVENT_PARAMS* eventParams);
+
+
+// NvEncMapInputResource 
+/**
+ * \brief Map an externally created input resource pointer for encoding.
+ *
+ * Maps an externally allocated input resource [using and returns a NV_ENC_INPUT_PTR
+ * which can be used for encoding in the ::NvEncEncodePicture() function. The
+ * mapped resource is returned in the field NV_ENC_MAP_INPUT_RESOURCE::outputResourcePtr.
+ * The NvEncodeAPI interface also returns the buffer format of the mapped resource
+ * in the field NV_ENC_MAP_INPUT_RESOURCE::outbufferFmt.
+ * This function provides synchronization guarantee that any direct3d or cuda
+ * work submitted on the input buffer is completed before the buffer is used for encoding.
+ * The client should not access any input buffer while they are mapped by the encoder.
+ *
+ * \param [in] encoder
+ *   Pointer to the NvEncodeAPI interface.
+ * \param [in,out] mapInputResParams
+ *   Pointer to the ::_NV_ENC_MAP_INPUT_RESOURCE structure.
+ *
+ * \return
+ * ::NV_ENC_SUCCESS \n
+ * ::NV_ENC_ERR_INVALID_PTR \n
+ * ::NV_ENC_ERR_INVALID_ENCODERDEVICE \n
+ * ::NV_ENC_ERR_DEVICE_NOT_EXIST \n
+ * ::NV_ENC_ERR_UNSUPPORTED_PARAM \n
+ * ::NV_ENC_ERR_OUT_OF_MEMORY \n
+ * ::NV_ENC_ERR_INVALID_VERSION \n
+ * ::NV_ENC_ERR_INVALID_PARAM \n
+ * ::NV_ENC_ERR_ENCODER_NOT_INITIALIZED \n
+ * ::NV_ENC_ERR_RESOURCE_NOT_REGISTERED \n
+ * ::NV_ENC_ERR_MAP_FAILED \n
+ * ::NV_ENC_ERR_GENERIC \n
+ *
+ */
+NVENCSTATUS NVENCAPI NvEncMapInputResource                         (void* encoder, NV_ENC_MAP_INPUT_RESOURCE* mapInputResParams);
+
+
+// NvEncUnmapInputResource 
+/**
+ * \brief  UnMaps a NV_ENC_INPUT_PTR  which was mapped for encoding
+ *
+ *
+ * UnMaps an input buffer which was previously mapped using ::NvEncMapInputResource()
+ * API. The mapping created using ::NvEncMapInputResource() should be invalidated
+ * using this API before the external resource is destroyed by the client. The client
+ * must unmap the buffer after ::NvEncLockBitstream() API returns succuessfully for encode
+ * work submitted using the mapped input buffer.
+ *
+ *
+ * \param [in] encoder
+ *   Pointer to the NvEncodeAPI interface.
+ * \param [in] mappedInputBuffer
+ *   Pointer to the NV_ENC_INPUT_PTR
+ *
+ * \return
+ * ::NV_ENC_SUCCESS \n
+ * ::NV_ENC_ERR_INVALID_PTR \n
+ * ::NV_ENC_ERR_INVALID_ENCODERDEVICE \n
+ * ::NV_ENC_ERR_DEVICE_NOT_EXIST \n
+ * ::NV_ENC_ERR_UNSUPPORTED_PARAM \n
+ * ::NV_ENC_ERR_OUT_OF_MEMORY \n
+ * ::NV_ENC_ERR_INVALID_VERSION \n
+ * ::NV_ENC_ERR_INVALID_PARAM \n
+ * ::NV_ENC_ERR_ENCODER_NOT_INITIALIZED \n
+ * ::NV_ENC_ERR_RESOURCE_NOT_REGISTERED \n
+ * ::NV_ENC_ERR_RESOURCE_NOT_MAPPED \n
+ * ::NV_ENC_ERR_GENERIC \n
+ *
+ */
+NVENCSTATUS NVENCAPI NvEncUnmapInputResource                         (void* encoder, NV_ENC_INPUT_PTR mappedInputBuffer);
+
+// NvEncDestroyEncoder
+/**
+ * \brief Destroy Encoding Session
+ *
+ * Destroys the encoder session previously created using ::NvEncOpenEncodeSession()
+ * function. The client must flush the encoder before freeing any resources. In order 
+ * to flush the encoder the client must pass a NULL encode picture packet and either 
+ * wait for the ::NvEncEncodePicture() function to return in synchronous mode or wait 
+ * for the flush event to be signaled by the encoder in asynchronous mode.
+ * The client must free all the input and output resources created using the
+ * NvEncodeAPI interface before destroying the encoder. If the client is operating
+ * in asynchronous mode, it must also unregister the completion events previously
+ * registered. 
+ *
+ * \param [in] encoder
+ *   Pointer to the NvEncodeAPI interface.
+ *
+ * \return
+ * ::NV_ENC_SUCCESS \n
+ * ::NV_ENC_ERR_INVALID_PTR \n
+ * ::NV_ENC_ERR_INVALID_ENCODERDEVICE \n
+ * ::NV_ENC_ERR_DEVICE_NOT_EXIST \n
+ * ::NV_ENC_ERR_UNSUPPORTED_PARAM \n
+ * ::NV_ENC_ERR_OUT_OF_MEMORY \n
+ * ::NV_ENC_ERR_INVALID_PARAM \n
+ * ::NV_ENC_ERR_GENERIC \n
+ *
+ */
+NVENCSTATUS NVENCAPI NvEncDestroyEncoder                        (void* encoder);
+
+// NvEncInvalidateRefFrames
+/**
+ * \brief Invalidate reference frames 
+ *
+ * Invalidates reference frame based on the time stamp provided by the client. 
+ * The encoder marks any reference frames or any frames which have been reconstructed
+ * using the corrupt frame as invalid for motion estimation and uses older reference
+ * frames for motion estimation. The encoded forces the current frame to be encoded
+ * as an intra frame if no reference frames are left after invalidation process.
+ * This is useful for low latency application for error resiliency. The client 
+ * is recommended to set NV_ENC_CONFIG_H264::maxNumRefFrames to a large value so 
+ * that encoder can keep a backup of older reference frames in the DPB and can use them
+ * for motion estimation when the newer reference frames have been invalidated.
+ * This API can be called multiple times.
+ *
+ * \param [in] encoder
+ *   Pointer to the NvEncodeAPI interface.
+ * \param [in] invalidRefFrameTimeStamp
+ *   Timestamp of the invalid reference frames which needs to be invalidated.
+ *
+ * \return
+ * ::NV_ENC_SUCCESS \n
+ * ::NV_ENC_ERR_INVALID_PTR \n
+ * ::NV_ENC_ERR_INVALID_ENCODERDEVICE \n
+ * ::NV_ENC_ERR_DEVICE_NOT_EXIST \n
+ * ::NV_ENC_ERR_UNSUPPORTED_PARAM \n
+ * ::NV_ENC_ERR_OUT_OF_MEMORY \n
+ * ::NV_ENC_ERR_INVALID_PARAM \n
+ * ::NV_ENC_ERR_GENERIC \n
+ *
+ */
+NVENCSTATUS NVENCAPI NvEncInvalidateRefFrames(void* encoder, uint64_t invalidRefFrameTimeStamp);
+
+// NvEncOpenEncodeSessionEx
+/**
+ * \brief Opens an encoding session.
+ * 
+ * Opens an encoding session and returns a pointer to the encoder interface in
+ * the \p **encoder parameter. The client should start encoding process by calling
+ * this API first. 
+ * The client must pass a pointer to IDirect3DDevice9/CUDA interface in the \p *device parameter.
+ * If the creation of encoder session fails, the client must call ::NvEncDestroyEncoder API 
+ * before exiting.
+ *
+ * \param [in] openSessionExParams
+ *    Pointer to a ::NV_ENC_OPEN_ENCODE_SESSION_EX_PARAMS structure.
+ * \param [out] encoder
+ *    Encode Session pointer to the NvEncodeAPI interface.
+ * \return
+ * ::NV_ENC_SUCCESS \n
+ * ::NV_ENC_ERR_INVALID_PTR \n
+ * ::NV_ENC_ERR_NO_ENCODE_DEVICE \n
+ * ::NV_ENC_ERR_UNSUPPORTED_DEVICE \n
+ * ::NV_ENC_ERR_INVALID_DEVICE \n
+ * ::NV_ENC_ERR_DEVICE_NOT_EXIST \n
+ * ::NV_ENC_ERR_UNSUPPORTED_PARAM \n
+ * ::NV_ENC_ERR_GENERIC \n
+ *
+ */
+NVENCSTATUS NVENCAPI NvEncOpenEncodeSessionEx                   (NV_ENC_OPEN_ENCODE_SESSION_EX_PARAMS *openSessionExParams, void** encoder);
+
+// NvEncRegisterResource
+/**
+ * \brief Registers a resource with the Nvidia Video Encoder Interface.
+ * 
+ * Registers a resource with the Nvidia Video Encoder Interface for book keeping.
+ * The client is expected to pass the registered resource handle as well, while calling ::NvEncMapInputResource API.
+ *
+ * \param [in] encoder
+ *   Pointer to the NVEncodeAPI interface.
+ *
+ * \param [in] registerResParams
+ *   Pointer to a ::_NV_ENC_REGISTER_RESOURCE structure
+ * 
+ * \return
+ * ::NV_ENC_SUCCESS \n
+ * ::NV_ENC_ERR_INVALID_PTR \n
+ * ::NV_ENC_ERR_INVALID_ENCODERDEVICE \n
+ * ::NV_ENC_ERR_DEVICE_NOT_EXIST \n
+ * ::NV_ENC_ERR_UNSUPPORTED_PARAM \n
+ * ::NV_ENC_ERR_OUT_OF_MEMORY \n
+ * ::NV_ENC_ERR_INVALID_VERSION \n
+ * ::NV_ENC_ERR_INVALID_PARAM \n
+ * ::NV_ENC_ERR_ENCODER_NOT_INITIALIZED \n
+ * ::NV_ENC_ERR_RESOURCE_REGISTER_FAILED \n
+ * ::NV_ENC_ERR_GENERIC \n
+ * ::NV_ENC_ERR_UNIMPLEMENTED \n
+ *
+ */
+NVENCSTATUS NVENCAPI NvEncRegisterResource                      (void* encoder, NV_ENC_REGISTER_RESOURCE* registerResParams);
+
+// NvEncUnregisterResource
+/**
+ * \brief Unregisters a resource previously registered with the Nvidia Video Encoder Interface.
+ * 
+ * Unregisters a resource previously registered with the Nvidia Video Encoder Interface.
+ * The client is expected to unregister any resource that it has registered with the 
+ * Nvidia Video Encoder Interface before destroying the resource.
+ *
+ * \param [in] encoder
+ *   Pointer to the NVEncodeAPI interface.
+ *
+ * \param [in] registeredResource
+ *   The registered resource pointer that was returned in ::NvEncRegisterResource.
+ *
+ * \return
+ * ::NV_ENC_SUCCESS \n
+ * ::NV_ENC_ERR_INVALID_PTR \n
+ * ::NV_ENC_ERR_INVALID_ENCODERDEVICE \n
+ * ::NV_ENC_ERR_DEVICE_NOT_EXIST \n
+ * ::NV_ENC_ERR_UNSUPPORTED_PARAM \n
+ * ::NV_ENC_ERR_OUT_OF_MEMORY \n
+ * ::NV_ENC_ERR_INVALID_VERSION \n
+ * ::NV_ENC_ERR_INVALID_PARAM \n
+ * ::NV_ENC_ERR_ENCODER_NOT_INITIALIZED \n
+ * ::NV_ENC_ERR_RESOURCE_NOT_REGISTERED \n
+ * ::NV_ENC_ERR_GENERIC \n
+ * ::NV_ENC_ERR_UNIMPLEMENTED \n
+ *
+ */
+NVENCSTATUS NVENCAPI NvEncUnregisterResource                    (void* encoder, NV_ENC_REGISTERED_PTR registeredResource);
+
+// NvEncReconfigureEncoder
+/**
+ * \brief Reconfigure an existing encoding session.
+ * 
+ * Reconfigure an existing encoding session.
+ * The client should call this API to change/reconfigure the parameter passed during 
+ * NvEncInitializeEncoder API call.
+ * Currently Reconfiguration of following are not supported.
+ * Change in GOP structure.
+ * Change in sync-Async mode.
+ * Change in MaxWidth & MaxHeight.
+ * Change in PTDmode.
+ * 
+ * Resolution change is possible only if maxEncodeWidth & maxEncodeHeight of NV_ENC_INITIALIZE_PARAMS
+ * is set while creating encoder session.
+ *
+ * \param [in] encoder
+ *   Pointer to the NVEncodeAPI interface.
+ *
+ * \param [in] reInitEncodeParams
+ *    Pointer to a ::NV_ENC_RECONFIGURE_PARAMS structure.
+ * \return
+ * ::NV_ENC_SUCCESS \n
+ * ::NV_ENC_ERR_INVALID_PTR \n
+ * ::NV_ENC_ERR_NO_ENCODE_DEVICE \n
+ * ::NV_ENC_ERR_UNSUPPORTED_DEVICE \n
+ * ::NV_ENC_ERR_INVALID_DEVICE \n
+ * ::NV_ENC_ERR_DEVICE_NOT_EXIST \n
+ * ::NV_ENC_ERR_UNSUPPORTED_PARAM \n
+ * ::NV_ENC_ERR_GENERIC \n
+ *
+ */
+NVENCSTATUS NVENCAPI NvEncReconfigureEncoder                   (void *encoder, NV_ENC_RECONFIGURE_PARAMS* reInitEncodeParams);
+
+
+
+// NvEncCreateMVBuffer
+/**
+ * \brief Allocates output MV buffer for ME only mode.
+ *
+ * This function is used to allocate an output MV buffer. The size of the mvBuffer is
+ * dependent on the frame height and width of the last ::NvEncCreateInputBuffer() call.
+ * The NV_ENC_OUTPUT_PTR returned by the NvEncodeAPI interface in the
+ * ::NV_ENC_CREATE_MV_BUFFER::mvBuffer field should be used in
+ * ::NvEncRunMotionEstimationOnly() API.
+ * Client must lock ::NV_ENC_CREATE_MV_BUFFER::mvBuffer using ::NvEncLockBitstream() API to get the motion vector data.
+ *
+ * \param [in] encoder
+ *   Pointer to the NvEncodeAPI interface.
+ * \param [in,out] createMVBufferParams
+ *  Pointer to the ::NV_ENC_CREATE_MV_BUFFER structure.
+ *
+ * \return
+ * ::NV_ENC_SUCCESS \n
+ * ::NV_ENC_ERR_INVALID_PTR \n
+ * ::NV_ENC_ERR_INVALID_ENCODERDEVICE \n
+ * ::NV_ENC_ERR_DEVICE_NOT_EXIST \n
+ * ::NV_ENC_ERR_UNSUPPORTED_PARAM \n
+ * ::NV_ENC_ERR_OUT_OF_MEMORY \n
+ * ::NV_ENC_ERR_INVALID_PARAM \n
+ * ::NV_ENC_ERR_INVALID_VERSION \n
+ * ::NV_ENC_ERR_GENERIC \n
+ */
+NVENCSTATUS NVENCAPI NvEncCreateMVBuffer                        (void* encoder, NV_ENC_CREATE_MV_BUFFER* createMVBufferParams);
+
+
+// NvEncDestroyMVBuffer
+/**
+ * \brief Release an output MV buffer for ME only mode.
+ *
+ * This function is used to release the output MV buffer allocated using
+ * the ::NvEncCreateMVBuffer() function. The client must release the output
+ * mvBuffer using this function before destroying the encoder session.
+ *
+ * \param [in] encoder
+ *   Pointer to the NvEncodeAPI interface.
+ * \param [in] mvBuffer
+ *   Pointer to the mvBuffer being released.
+ *
+ * \return
+ * ::NV_ENC_SUCCESS \n
+ * ::NV_ENC_ERR_INVALID_PTR \n
+ * ::NV_ENC_ERR_INVALID_ENCODERDEVICE \n
+ * ::NV_ENC_ERR_DEVICE_NOT_EXIST \n
+ * ::NV_ENC_ERR_UNSUPPORTED_PARAM \n
+ * ::NV_ENC_ERR_OUT_OF_MEMORY \n
+ * ::NV_ENC_ERR_INVALID_PARAM \n
+ * ::NV_ENC_ERR_INVALID_VERSION \n
+ * ::NV_ENC_ERR_ENCODER_NOT_INITIALIZED \n
+ * ::NV_ENC_ERR_GENERIC \n
+ */
+NVENCSTATUS NVENCAPI NvEncDestroyMVBuffer                       (void* encoder, NV_ENC_OUTPUT_PTR mvBuffer);
+
+
+// NvEncRunMotionEstimationOnly
+/**
+ * \brief Submit an input picture and reference frame for motion estimation in ME only mode.
+ *
+ * This function is used to submit the input frame and reference frame for motion
+ * estimation. The ME parameters are passed using *meOnlyParams which is a pointer
+ * to ::_NV_ENC_MEONLY_PARAMS structure.
+ * Client must lock ::NV_ENC_CREATE_MV_BUFFER::mvBuffer using ::NvEncLockBitstream() API to get the motion vector data.
+ * to get motion vector data.
+ *
+ * \param [in] encoder
+ *   Pointer to the NvEncodeAPI interface.
+ * \param [in] meOnlyParams
+ *   Pointer to the ::_NV_ENC_MEONLY_PARAMS structure.
+ *
+ * \return
+ * ::NV_ENC_SUCCESS \n
+ * ::NV_ENC_ERR_INVALID_PTR \n
+ * ::NV_ENC_ERR_INVALID_ENCODERDEVICE \n
+ * ::NV_ENC_ERR_DEVICE_NOT_EXIST \n
+ * ::NV_ENC_ERR_UNSUPPORTED_PARAM \n
+ * ::NV_ENC_ERR_OUT_OF_MEMORY \n
+ * ::NV_ENC_ERR_INVALID_PARAM \n
+ * ::NV_ENC_ERR_INVALID_VERSION \n
+ * ::NV_ENC_ERR_NEED_MORE_INPUT \n
+ * ::NV_ENC_ERR_ENCODER_NOT_INITIALIZED \n
+ * ::NV_ENC_ERR_GENERIC \n
+ */
+NVENCSTATUS NVENCAPI NvEncRunMotionEstimationOnly               (void* encoder, NV_ENC_MEONLY_PARAMS* meOnlyParams);
+
+// NvEncodeAPIGetMaxSupportedVersion
+/**
+ * \brief Get the largest NvEncodeAPI version supported by the driver.
+ *
+ * This function can be used by clients to determine if the driver supports
+ * the NvEncodeAPI header the application was compiled with.
+ *
+ * \param [out] version
+ *   Pointer to the requested value. The 4 least significant bits in the returned
+ *   indicate the minor version and the rest of the bits indicate the major
+ *   version of the largest supported version.
+ *
+ * \return
+ * ::NV_ENC_SUCCESS \n
+ * ::NV_ENC_ERR_INVALID_PTR \n
+ */
+NVENCSTATUS NVENCAPI NvEncodeAPIGetMaxSupportedVersion          (uint32_t* version);
+
+
+/// \cond API PFN
+/*
+ *  Defines API function pointers 
+ */
+typedef NVENCSTATUS (NVENCAPI* PNVENCOPENENCODESESSION)         (void* device, uint32_t deviceType, void** encoder);
+typedef NVENCSTATUS (NVENCAPI* PNVENCGETENCODEGUIDCOUNT)        (void* encoder, uint32_t* encodeGUIDCount);
+typedef NVENCSTATUS (NVENCAPI* PNVENCGETENCODEGUIDS)            (void* encoder, GUID* GUIDs, uint32_t guidArraySize, uint32_t* GUIDCount);
+typedef NVENCSTATUS (NVENCAPI* PNVENCGETENCODEPROFILEGUIDCOUNT) (void* encoder, GUID encodeGUID, uint32_t* encodeProfileGUIDCount);
+typedef NVENCSTATUS (NVENCAPI* PNVENCGETENCODEPROFILEGUIDS)     (void* encoder, GUID encodeGUID, GUID* profileGUIDs, uint32_t guidArraySize, uint32_t* GUIDCount);
+typedef NVENCSTATUS (NVENCAPI* PNVENCGETINPUTFORMATCOUNT)       (void* encoder, GUID encodeGUID, uint32_t* inputFmtCount);
+typedef NVENCSTATUS (NVENCAPI* PNVENCGETINPUTFORMATS)           (void* encoder, GUID encodeGUID, NV_ENC_BUFFER_FORMAT* inputFmts, uint32_t inputFmtArraySize, uint32_t* inputFmtCount);
+typedef NVENCSTATUS (NVENCAPI* PNVENCGETENCODECAPS)             (void* encoder, GUID encodeGUID, NV_ENC_CAPS_PARAM* capsParam, int* capsVal);
+typedef NVENCSTATUS (NVENCAPI* PNVENCGETENCODEPRESETCOUNT)      (void* encoder, GUID encodeGUID, uint32_t* encodePresetGUIDCount);
+typedef NVENCSTATUS (NVENCAPI* PNVENCGETENCODEPRESETGUIDS)      (void* encoder, GUID encodeGUID, GUID* presetGUIDs, uint32_t guidArraySize, uint32_t* encodePresetGUIDCount);
+typedef NVENCSTATUS (NVENCAPI* PNVENCGETENCODEPRESETCONFIG)     (void* encoder, GUID encodeGUID, GUID  presetGUID, NV_ENC_PRESET_CONFIG* presetConfig);
+typedef NVENCSTATUS (NVENCAPI* PNVENCINITIALIZEENCODER)         (void* encoder, NV_ENC_INITIALIZE_PARAMS* createEncodeParams);
+typedef NVENCSTATUS (NVENCAPI* PNVENCCREATEINPUTBUFFER)         (void* encoder, NV_ENC_CREATE_INPUT_BUFFER* createInputBufferParams);
+typedef NVENCSTATUS (NVENCAPI* PNVENCDESTROYINPUTBUFFER)        (void* encoder, NV_ENC_INPUT_PTR inputBuffer);
+typedef NVENCSTATUS (NVENCAPI* PNVENCCREATEBITSTREAMBUFFER)     (void* encoder, NV_ENC_CREATE_BITSTREAM_BUFFER* createBitstreamBufferParams);
+typedef NVENCSTATUS (NVENCAPI* PNVENCDESTROYBITSTREAMBUFFER)    (void* encoder, NV_ENC_OUTPUT_PTR bitstreamBuffer);
+typedef NVENCSTATUS (NVENCAPI* PNVENCENCODEPICTURE)             (void* encoder, NV_ENC_PIC_PARAMS* encodePicParams);
+typedef NVENCSTATUS (NVENCAPI* PNVENCLOCKBITSTREAM)             (void* encoder, NV_ENC_LOCK_BITSTREAM* lockBitstreamBufferParams);
+typedef NVENCSTATUS (NVENCAPI* PNVENCUNLOCKBITSTREAM)           (void* encoder, NV_ENC_OUTPUT_PTR bitstreamBuffer);
+typedef NVENCSTATUS (NVENCAPI* PNVENCLOCKINPUTBUFFER)           (void* encoder, NV_ENC_LOCK_INPUT_BUFFER* lockInputBufferParams);
+typedef NVENCSTATUS (NVENCAPI* PNVENCUNLOCKINPUTBUFFER)         (void* encoder, NV_ENC_INPUT_PTR inputBuffer);
+typedef NVENCSTATUS (NVENCAPI* PNVENCGETENCODESTATS)            (void* encoder, NV_ENC_STAT* encodeStats);
+typedef NVENCSTATUS (NVENCAPI* PNVENCGETSEQUENCEPARAMS)         (void* encoder, NV_ENC_SEQUENCE_PARAM_PAYLOAD* sequenceParamPayload);
+typedef NVENCSTATUS (NVENCAPI* PNVENCREGISTERASYNCEVENT)        (void* encoder, NV_ENC_EVENT_PARAMS* eventParams);
+typedef NVENCSTATUS (NVENCAPI* PNVENCUNREGISTERASYNCEVENT)      (void* encoder, NV_ENC_EVENT_PARAMS* eventParams);
+typedef NVENCSTATUS (NVENCAPI* PNVENCMAPINPUTRESOURCE)          (void* encoder, NV_ENC_MAP_INPUT_RESOURCE* mapInputResParams);
+typedef NVENCSTATUS (NVENCAPI* PNVENCUNMAPINPUTRESOURCE)        (void* encoder, NV_ENC_INPUT_PTR mappedInputBuffer);
+typedef NVENCSTATUS (NVENCAPI* PNVENCDESTROYENCODER)            (void* encoder);
+typedef NVENCSTATUS (NVENCAPI* PNVENCINVALIDATEREFFRAMES)       (void* encoder, uint64_t invalidRefFrameTimeStamp);
+typedef NVENCSTATUS (NVENCAPI* PNVENCOPENENCODESESSIONEX)       (NV_ENC_OPEN_ENCODE_SESSION_EX_PARAMS *openSessionExParams, void** encoder);
+typedef NVENCSTATUS (NVENCAPI* PNVENCREGISTERRESOURCE)          (void* encoder, NV_ENC_REGISTER_RESOURCE* registerResParams);
+typedef NVENCSTATUS (NVENCAPI* PNVENCUNREGISTERRESOURCE)        (void* encoder, NV_ENC_REGISTERED_PTR registeredRes);
+typedef NVENCSTATUS (NVENCAPI* PNVENCRECONFIGUREENCODER)        (void* encoder, NV_ENC_RECONFIGURE_PARAMS* reInitEncodeParams);
+
+typedef NVENCSTATUS (NVENCAPI* PNVENCCREATEMVBUFFER)            (void* encoder, NV_ENC_CREATE_MV_BUFFER* createMVBufferParams);
+typedef NVENCSTATUS (NVENCAPI* PNVENCDESTROYMVBUFFER)           (void* encoder, NV_ENC_OUTPUT_PTR mvBuffer);
+typedef NVENCSTATUS (NVENCAPI* PNVENCRUNMOTIONESTIMATIONONLY)   (void* encoder, NV_ENC_MEONLY_PARAMS* meOnlyParams);
+
+
+/// \endcond
+
+
+/** @} */ /* END ENCODE_FUNC */
+
+/**
+ * \ingroup ENCODER_STRUCTURE
+ * NV_ENCODE_API_FUNCTION_LIST
+ */
+typedef struct _NV_ENCODE_API_FUNCTION_LIST
+{
+    uint32_t                        version;                           /**< [in]: Client should pass NV_ENCODE_API_FUNCTION_LIST_VER.                               */
+    uint32_t                        reserved;                          /**< [in]: Reserved and should be set to 0.                                                  */
+    PNVENCOPENENCODESESSION         nvEncOpenEncodeSession;            /**< [out]: Client should access ::NvEncOpenEncodeSession() API through this pointer.        */
+    PNVENCGETENCODEGUIDCOUNT        nvEncGetEncodeGUIDCount;           /**< [out]: Client should access ::NvEncGetEncodeGUIDCount() API through this pointer.       */
+    PNVENCGETENCODEPRESETCOUNT      nvEncGetEncodeProfileGUIDCount;    /**< [out]: Client should access ::NvEncGetEncodeProfileGUIDCount() API through this pointer.*/
+    PNVENCGETENCODEPRESETGUIDS      nvEncGetEncodeProfileGUIDs;        /**< [out]: Client should access ::NvEncGetEncodeProfileGUIDs() API through this pointer.    */
+    PNVENCGETENCODEGUIDS            nvEncGetEncodeGUIDs;               /**< [out]: Client should access ::NvEncGetEncodeGUIDs() API through this pointer.           */
+    PNVENCGETINPUTFORMATCOUNT       nvEncGetInputFormatCount;          /**< [out]: Client should access ::NvEncGetInputFormatCount() API through this pointer.      */
+    PNVENCGETINPUTFORMATS           nvEncGetInputFormats;              /**< [out]: Client should access ::NvEncGetInputFormats() API through this pointer.          */
+    PNVENCGETENCODECAPS             nvEncGetEncodeCaps;                /**< [out]: Client should access ::NvEncGetEncodeCaps() API through this pointer.            */
+    PNVENCGETENCODEPRESETCOUNT      nvEncGetEncodePresetCount;         /**< [out]: Client should access ::NvEncGetEncodePresetCount() API through this pointer.     */
+    PNVENCGETENCODEPRESETGUIDS      nvEncGetEncodePresetGUIDs;         /**< [out]: Client should access ::NvEncGetEncodePresetGUIDs() API through this pointer.     */
+    PNVENCGETENCODEPRESETCONFIG     nvEncGetEncodePresetConfig;        /**< [out]: Client should access ::NvEncGetEncodePresetConfig() API through this pointer.    */
+    PNVENCINITIALIZEENCODER         nvEncInitializeEncoder;            /**< [out]: Client should access ::NvEncInitializeEncoder() API through this pointer.        */
+    PNVENCCREATEINPUTBUFFER         nvEncCreateInputBuffer;            /**< [out]: Client should access ::NvEncCreateInputBuffer() API through this pointer.        */
+    PNVENCDESTROYINPUTBUFFER        nvEncDestroyInputBuffer;           /**< [out]: Client should access ::NvEncDestroyInputBuffer() API through this pointer.       */
+    PNVENCCREATEBITSTREAMBUFFER     nvEncCreateBitstreamBuffer;        /**< [out]: Client should access ::NvEncCreateBitstreamBuffer() API through this pointer.    */
+    PNVENCDESTROYBITSTREAMBUFFER    nvEncDestroyBitstreamBuffer;       /**< [out]: Client should access ::NvEncDestroyBitstreamBuffer() API through this pointer.   */
+    PNVENCENCODEPICTURE             nvEncEncodePicture;                /**< [out]: Client should access ::NvEncEncodePicture() API through this pointer.            */
+    PNVENCLOCKBITSTREAM             nvEncLockBitstream;                /**< [out]: Client should access ::NvEncLockBitstream() API through this pointer.            */
+    PNVENCUNLOCKBITSTREAM           nvEncUnlockBitstream;              /**< [out]: Client should access ::NvEncUnlockBitstream() API through this pointer.          */
+    PNVENCLOCKINPUTBUFFER           nvEncLockInputBuffer;              /**< [out]: Client should access ::NvEncLockInputBuffer() API through this pointer.          */
+    PNVENCUNLOCKINPUTBUFFER         nvEncUnlockInputBuffer;            /**< [out]: Client should access ::NvEncUnlockInputBuffer() API through this pointer.        */
+    PNVENCGETENCODESTATS            nvEncGetEncodeStats;               /**< [out]: Client should access ::NvEncGetEncodeStats() API through this pointer.           */
+    PNVENCGETSEQUENCEPARAMS         nvEncGetSequenceParams;            /**< [out]: Client should access ::NvEncGetSequenceParams() API through this pointer.        */
+    PNVENCREGISTERASYNCEVENT        nvEncRegisterAsyncEvent;           /**< [out]: Client should access ::NvEncRegisterAsyncEvent() API through this pointer.       */
+    PNVENCUNREGISTERASYNCEVENT      nvEncUnregisterAsyncEvent;         /**< [out]: Client should access ::NvEncUnregisterAsyncEvent() API through this pointer.     */
+    PNVENCMAPINPUTRESOURCE          nvEncMapInputResource;             /**< [out]: Client should access ::NvEncMapInputResource() API through this pointer.         */
+    PNVENCUNMAPINPUTRESOURCE        nvEncUnmapInputResource;           /**< [out]: Client should access ::NvEncUnmapInputResource() API through this pointer.       */
+    PNVENCDESTROYENCODER            nvEncDestroyEncoder;               /**< [out]: Client should access ::NvEncDestroyEncoder() API through this pointer.           */
+    PNVENCINVALIDATEREFFRAMES       nvEncInvalidateRefFrames;          /**< [out]: Client should access ::NvEncInvalidateRefFrames() API through this pointer.      */
+    PNVENCOPENENCODESESSIONEX       nvEncOpenEncodeSessionEx;          /**< [out]: Client should access ::NvEncOpenEncodeSession() API through this pointer.        */
+    PNVENCREGISTERRESOURCE          nvEncRegisterResource;             /**< [out]: Client should access ::NvEncRegisterResource() API through this pointer.         */
+    PNVENCUNREGISTERRESOURCE        nvEncUnregisterResource;           /**< [out]: Client should access ::NvEncUnregisterResource() API through this pointer.       */
+    PNVENCRECONFIGUREENCODER        nvEncReconfigureEncoder;           /**< [out]: Client should access ::NvEncReconfigureEncoder() API through this pointer.       */
+    void*                           reserved1;
+    PNVENCCREATEMVBUFFER            nvEncCreateMVBuffer;               /**< [out]: Client should access ::NvEncCreateMVBuffer API through this pointer.             */
+    PNVENCDESTROYMVBUFFER           nvEncDestroyMVBuffer;              /**< [out]: Client should access ::NvEncDestroyMVBuffer API through this pointer.            */
+    PNVENCRUNMOTIONESTIMATIONONLY   nvEncRunMotionEstimationOnly;      /**< [out]: Client should access ::NvEncRunMotionEstimationOnly API through this pointer.    */
+    void*                           reserved2[281];                    /**< [in]:  Reserved and must be set to NULL                                                 */
+} NV_ENCODE_API_FUNCTION_LIST;
+
+/** Macro for constructing the version field of ::_NV_ENCODEAPI_FUNCTION_LIST. */
+#define NV_ENCODE_API_FUNCTION_LIST_VER NVENCAPI_STRUCT_VERSION(2)
+
+// NvEncodeAPICreateInstance
+/**
+ * \ingroup ENCODE_FUNC
+ * Entry Point to the NvEncodeAPI interface.
+ * 
+ * Creates an instance of the NvEncodeAPI interface, and populates the
+ * pFunctionList with function pointers to the API routines implemented by the
+ * NvEncodeAPI interface.
+ *
+ * \param [out] functionList
+ *
+ * \return
+ * ::NV_ENC_SUCCESS
+ * ::NV_ENC_ERR_INVALID_PTR
+ */
+NVENCSTATUS NVENCAPI NvEncodeAPICreateInstance(NV_ENCODE_API_FUNCTION_LIST *functionList);
+
+#ifdef __cplusplus
+}
+#endif
+
+
+#endif
+
diff --git a/webrtc/modules/video_coding/codecs/h264/include/nvFileIO.h b/webrtc/modules/video_coding/codecs/h264/include/nvFileIO.h
new file mode 100644
index 0000000..39e5935
--- /dev/null
+++ b/webrtc/modules/video_coding/codecs/h264/include/nvFileIO.h
@@ -0,0 +1,177 @@
+/*
+ * Copyright 1993-2015 NVIDIA Corporation.  All rights reserved.
+ *
+ * Please refer to the NVIDIA end user license agreement (EULA) associated
+ * with this source code for terms and conditions that govern your use of
+ * this software. Any use, reproduction, disclosure, or distribution of
+ * this software and related documentation outside the terms of the EULA
+ * is strictly prohibited.
+ *
+ */
+
+#ifndef NVFILE_IO_H
+#define NVFILE_IO_H
+
+#if defined __linux__
+#include <stdio.h>
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+#include <errno.h>
+#include <string.h>
+#include <unistd.h>
+#include <dlfcn.h>
+#include <stdlib.h>
+
+typedef void * HANDLE;
+typedef void             *HINSTANCE;
+typedef unsigned long     DWORD, *LPDWORD;
+typedef DWORD             FILE_SIZE;
+
+#define FALSE   0
+#define TRUE    1
+#define INFINITE UINT_MAX
+
+#define FILE_BEGIN               SEEK_SET
+#define INVALID_SET_FILE_POINTER (-1)
+#define INVALID_HANDLE_VALUE     ((void *)(-1))
+
+#else
+#include <stdio.h>
+#include <windows.h>
+#endif
+
+#include "nvCPUOPSys.h"
+
+typedef unsigned long long  U64;
+typedef unsigned int        U32;
+
+inline U32 nvSetFilePointer(HANDLE hInputFile, U32 fileOffset, U32 *moveFilePointer, U32 flag)
+{
+#if defined (NV_WINDOWS)
+    return SetFilePointer(hInputFile, fileOffset, NULL, flag);
+#elif defined __linux || defined __APPLE_ || defined __MACOSX
+    return fseek((FILE *)hInputFile, fileOffset, flag);
+#endif
+}
+
+inline U32 nvSetFilePointer64(HANDLE hInputFile, U64 fileOffset, U64 *moveFilePointer, U32 flag)
+{
+#if defined (NV_WINDOWS)
+    return SetFilePointer(hInputFile, ((U32 *)&fileOffset)[0], (PLONG)&((U32 *)&fileOffset)[1], flag);
+#elif defined __linux || defined __APPLE__ || defined __MACOSX
+    return fseek((FILE *)hInputFile, (long int)fileOffset, flag);
+#endif
+}
+
+inline bool nvReadFile(HANDLE hInputFile, void *buf, U32 bytes_to_read, U32 *bytes_read, void *operlapped)
+{
+#if defined (NV_WINDOWS)
+    ReadFile(hInputFile, buf, bytes_to_read, (LPDWORD)bytes_read, NULL);
+    return true;
+#elif defined __linux || defined __APPLE__ || defined __MACOSX
+    U32 elems_read;
+    elems_read = fread(buf, bytes_to_read, 1, (FILE *)hInputFile);
+
+    if (bytes_read)
+    {
+        *bytes_read = elems_read > 0 ? bytes_to_read : 0;
+    }
+    return true;
+#endif
+}
+
+inline void nvGetFileSize(HANDLE hInputFile, DWORD *pFilesize)
+{
+#if defined (NV_WINDOWS)
+    LARGE_INTEGER file_size;
+
+    if (hInputFile != INVALID_HANDLE_VALUE)
+    {
+        file_size.LowPart = GetFileSize(hInputFile, (LPDWORD)&file_size.HighPart);
+        printf("[ Input Filesize] : %lld bytes\n", ((LONGLONG) file_size.HighPart << 32) + (LONGLONG)file_size.LowPart);
+
+        if (pFilesize != NULL) *pFilesize = file_size.LowPart;
+    }
+
+#elif defined __linux || defined __APPLE__ || defined __MACOSX
+    FILE_SIZE file_size;
+
+    if (hInputFile != NULL)
+    {
+        nvSetFilePointer64(hInputFile, 0, NULL, SEEK_END);
+        file_size = ftell((FILE *)hInputFile);
+        nvSetFilePointer64(hInputFile, 0, NULL, SEEK_SET);
+        printf("Input Filesize: %ld bytes\n", file_size);
+
+        if (pFilesize != NULL) *pFilesize = file_size;
+    }
+
+#endif
+}
+
+inline HANDLE nvOpenFile(const char *input_file)
+{
+    HANDLE hInput = NULL;
+
+#if defined (NV_WINDOWS)
+    hInput = CreateFileA(input_file, GENERIC_READ, FILE_SHARE_READ, NULL, OPEN_EXISTING , FILE_ATTRIBUTE_NORMAL, NULL);
+
+    if (hInput == INVALID_HANDLE_VALUE)
+    {
+        fprintf(stderr, "nvOpenFile Failed to open \"%s\"\n", input_file);
+        exit(EXIT_FAILURE);
+    }
+
+#elif defined __linux || defined __APPLE_ || defined __MACOSX
+    hInput = fopen(input_file, "rb");
+
+    if (hInput == NULL)
+    {
+        fprintf(stderr, "nvOpenFile Failed to open \"%s\"\n", input_file);
+        exit(EXIT_FAILURE);
+    }
+
+#endif
+    return hInput;
+}
+
+inline HANDLE nvOpenFileWrite(const char *output_file)
+{
+    HANDLE hOutput = NULL;
+
+#if defined (NV_WINDOWS)
+    hOutput = CreateFileA(output_file, GENERIC_WRITE, FILE_SHARE_WRITE, NULL, OPEN_EXISTING , FILE_ATTRIBUTE_NORMAL, NULL);
+
+    if (hOutput == INVALID_HANDLE_VALUE)
+    {
+        fprintf(stderr, "nvOpenFileWrite Failed to open \"%s\"\n", output_file);
+        exit(EXIT_FAILURE);
+    }
+
+#elif defined __linux || defined __APPLE_ || defined __MACOSX
+    hOutput = fopen(output_file, "wb+");
+
+    if (hOutput == NULL)
+    {
+        fprintf(stderr, "nvOpenFileWrite Failed to open \"%s\"\n", output_file);
+        exit(EXIT_FAILURE);
+    }
+
+#endif
+    return hOutput;
+}
+
+inline void nvCloseFile(HANDLE hFileHandle)
+{
+    if (hFileHandle)
+    {
+#if defined (NV_WINDOWS)
+        CloseHandle(hFileHandle);
+#else
+        fclose((FILE *)hFileHandle);
+#endif
+    }
+}
+
+#endif
diff --git a/webrtc/modules/video_coding/codecs/h264/include/nvUtils.h b/webrtc/modules/video_coding/codecs/h264/include/nvUtils.h
new file mode 100644
index 0000000..08e979d
--- /dev/null
+++ b/webrtc/modules/video_coding/codecs/h264/include/nvUtils.h
@@ -0,0 +1,133 @@
+/*
+ * Copyright 1993-2015 NVIDIA Corporation.  All rights reserved.
+ *
+ * Please refer to the NVIDIA end user license agreement (EULA) associated
+ * with this source code for terms and conditions that govern your use of
+ * this software. Any use, reproduction, disclosure, or distribution of
+ * this software and related documentation outside the terms of the EULA
+ * is strictly prohibited.
+ *
+ */
+
+#ifndef NVUTILS_H
+#define NVUTILS_H
+
+#include "nvCPUOPSys.h"
+#include "nvFileIO.h"
+
+#if defined (NV_WINDOWS)
+#include <windows.h>
+
+#elif defined NV_UNIX
+#include <sys/time.h>
+#include <limits.h>
+
+#define FALSE 0
+#define TRUE  1
+#define S_OK  0
+#define INFINITE UINT_MAX
+#define stricmp strcasecmp
+#define FILE_BEGIN               SEEK_SET
+#define INVALID_SET_FILE_POINTER (-1)
+#define INVALID_HANDLE_VALUE     ((void *)(-1))
+
+typedef void* HANDLE;
+typedef void* HINSTANCE;
+typedef unsigned long DWORD, *LPWORD;
+typedef DWORD FILE_SIZE;
+typedef DWORD HRESULT;
+
+#endif
+
+#define MAX(a, b) ((a) > (b) ? (a) : (b))
+#define MIN(a, b) ((a) < (b) ? (a) : (b))
+#define FABS(a) ((a) >= 0 ? (a) : -(a))
+
+inline bool NvSleep(unsigned int mSec)
+{
+#if defined (NV_WINDOWS)
+    Sleep(mSec);
+#elif defined NV_UNIX
+    usleep(mSec * 1000);
+#else
+#error NvSleep function unknown for this platform.
+#endif
+    return true;
+}
+
+inline bool NvQueryPerformanceFrequency(unsigned long long *freq)
+{
+    *freq = 0;
+#if defined (NV_WINDOWS)
+    LARGE_INTEGER lfreq;
+    if (!QueryPerformanceFrequency(&lfreq)) {
+        return false;
+    }
+    *freq = lfreq.QuadPart;
+#elif defined NV_UNIX
+    // We use system's  gettimeofday() to return timer ticks in uSec
+    *freq = 1000000000;
+#else
+#error NvQueryPerformanceFrequency function not defined for this platform.
+#endif
+
+    return true;
+}
+
+#define SEC_TO_NANO_ULL(sec)    ((unsigned long long)sec * 1000000000)
+#define MICRO_TO_NANO_ULL(sec)  ((unsigned long long)sec * 1000)
+
+inline bool NvQueryPerformanceCounter(unsigned long long *counter)
+{
+    *counter = 0;
+#if defined (NV_WINDOWS)
+    LARGE_INTEGER lcounter;
+    if (!QueryPerformanceCounter(&lcounter)) {
+        return false;
+    }
+    *counter = lcounter.QuadPart;
+#elif defined NV_UNIX
+    struct timeval tv;
+    int ret;
+
+    ret = gettimeofday(&tv, NULL);
+    if (ret != 0) {
+        return false;
+    }
+
+    *counter = SEC_TO_NANO_ULL(tv.tv_sec) + MICRO_TO_NANO_ULL(tv.tv_usec);
+#else
+#error NvQueryPerformanceCounter function not defined for this platform.
+#endif
+    return true;
+}
+
+#if defined NV_UNIX
+__inline bool operator==(const GUID &guid1, const GUID &guid2)
+{
+     if (guid1.Data1    == guid2.Data1 &&
+         guid1.Data2    == guid2.Data2 &&
+         guid1.Data3    == guid2.Data3 &&
+         guid1.Data4[0] == guid2.Data4[0] &&
+         guid1.Data4[1] == guid2.Data4[1] &&
+         guid1.Data4[2] == guid2.Data4[2] &&
+         guid1.Data4[3] == guid2.Data4[3] &&
+         guid1.Data4[4] == guid2.Data4[4] &&
+         guid1.Data4[5] == guid2.Data4[5] &&
+         guid1.Data4[6] == guid2.Data4[6] &&
+         guid1.Data4[7] == guid2.Data4[7])
+    {
+        return true;
+    }
+
+    return false;
+}
+__inline bool operator!=(const GUID &guid1, const GUID &guid2)
+{
+    return !(guid1 == guid2);
+}
+#endif
+#endif
+
+#define PRINTERR(message, ...) \
+    fprintf(stderr, "%s line %d: " message, __FILE__, __LINE__, ##__VA_ARGS__)
diff --git a/webrtc/modules/video_coding/video_sender.cc b/webrtc/modules/video_coding/video_sender.cc
index 0b54d13..d87b9cb 100644
--- a/webrtc/modules/video_coding/video_sender.cc
+++ b/webrtc/modules/video_coding/video_sender.cc
@@ -1,402 +1,404 @@
-/*
- *  Copyright (c) 2013 The WebRTC project authors. All Rights Reserved.
- *
- *  Use of this source code is governed by a BSD-style license
- *  that can be found in the LICENSE file in the root of the source
- *  tree. An additional intellectual property rights grant can be found
- *  in the file PATENTS.  All contributing project authors may
- *  be found in the AUTHORS file in the root of the source tree.
- */
-
-
-#include <algorithm>  // std::max
-
-#include "webrtc/base/checks.h"
-#include "webrtc/base/logging.h"
-#include "webrtc/common_types.h"
-#include "webrtc/common_video/include/video_bitrate_allocator.h"
-#include "webrtc/common_video/libyuv/include/webrtc_libyuv.h"
-#include "webrtc/modules/video_coding/codecs/vp8/temporal_layers.h"
-#include "webrtc/modules/video_coding/include/video_codec_interface.h"
-#include "webrtc/modules/video_coding/encoded_frame.h"
-#include "webrtc/modules/video_coding/utility/default_video_bitrate_allocator.h"
-#include "webrtc/modules/video_coding/utility/quality_scaler.h"
-#include "webrtc/modules/video_coding/video_coding_impl.h"
-#include "webrtc/system_wrappers/include/clock.h"
-
-namespace webrtc {
-namespace vcm {
-
-VideoSender::VideoSender(Clock* clock,
-                         EncodedImageCallback* post_encode_callback,
-                         VCMSendStatisticsCallback* send_stats_callback)
-    : clock_(clock),
-      _encoder(nullptr),
-      _mediaOpt(clock_),
-      _encodedFrameCallback(post_encode_callback, &_mediaOpt),
-      post_encode_callback_(post_encode_callback),
-      send_stats_callback_(send_stats_callback),
-      _codecDataBase(&_encodedFrameCallback),
-      frame_dropper_enabled_(true),
-      _sendStatsTimer(VCMProcessTimer::kDefaultProcessIntervalMs, clock_),
-      current_codec_(),
-      encoder_params_({BitrateAllocation(), 0, 0, 0}),
-      encoder_has_internal_source_(false),
-      next_frame_types_(1, kVideoFrameDelta) {
-  _mediaOpt.Reset();
-  // Allow VideoSender to be created on one thread but used on another, post
-  // construction. This is currently how this class is being used by at least
-  // one external project (diffractor).
-  sequenced_checker_.Detach();
-}
-
-VideoSender::~VideoSender() {}
-
-void VideoSender::Process() {
-  if (_sendStatsTimer.TimeUntilProcess() == 0) {
-    // |_sendStatsTimer.Processed()| must be called. Otherwise
-    // VideoSender::Process() will be called in an infinite loop.
-    _sendStatsTimer.Processed();
-    if (send_stats_callback_) {
-      uint32_t bitRate = _mediaOpt.SentBitRate();
-      uint32_t frameRate = _mediaOpt.SentFrameRate();
-      send_stats_callback_->SendStatistics(bitRate, frameRate);
-    }
-  }
-}
-
-int64_t VideoSender::TimeUntilNextProcess() {
-  return _sendStatsTimer.TimeUntilProcess();
-}
-
-// Register the send codec to be used.
-int32_t VideoSender::RegisterSendCodec(const VideoCodec* sendCodec,
-                                       uint32_t numberOfCores,
-                                       uint32_t maxPayloadSize) {
-  RTC_DCHECK(sequenced_checker_.CalledSequentially());
-  rtc::CritScope lock(&encoder_crit_);
-  if (sendCodec == nullptr) {
-    return VCM_PARAMETER_ERROR;
-  }
-
-  bool ret =
-      _codecDataBase.SetSendCodec(sendCodec, numberOfCores, maxPayloadSize);
-
-  // Update encoder regardless of result to make sure that we're not holding on
-  // to a deleted instance.
-  _encoder = _codecDataBase.GetEncoder();
-  // Cache the current codec here so they can be fetched from this thread
-  // without requiring the _sendCritSect lock.
-  current_codec_ = *sendCodec;
-
-  if (!ret) {
-    LOG(LS_ERROR) << "Failed to initialize set encoder with payload name '"
-                  << sendCodec->plName << "'.";
-    return VCM_CODEC_ERROR;
-  }
-
-  // SetSendCodec succeeded, _encoder should be set.
-  RTC_DCHECK(_encoder);
-
-  int numLayers;
-  if (sendCodec->codecType == kVideoCodecVP8) {
-    numLayers = sendCodec->VP8().numberOfTemporalLayers;
-  } else if (sendCodec->codecType == kVideoCodecVP9) {
-    numLayers = sendCodec->VP9().numberOfTemporalLayers;
-  } else {
-    numLayers = 1;
-  }
-
-  // If we have screensharing and we have layers, we disable frame dropper.
-  bool disable_frame_dropper =
-      numLayers > 1 && sendCodec->mode == kScreensharing;
-  if (disable_frame_dropper) {
-    _mediaOpt.EnableFrameDropper(false);
-  } else if (frame_dropper_enabled_) {
-    _mediaOpt.EnableFrameDropper(true);
-  }
-
-  {
-    rtc::CritScope cs(&params_crit_);
-    next_frame_types_.clear();
-    next_frame_types_.resize(VCM_MAX(sendCodec->numberOfSimulcastStreams, 1),
-                             kVideoFrameKey);
-    // Cache InternalSource() to have this available from IntraFrameRequest()
-    // without having to acquire encoder_crit_ (avoid blocking on encoder use).
-    encoder_has_internal_source_ = _encoder->InternalSource();
-  }
-
-  LOG(LS_VERBOSE) << " max bitrate " << sendCodec->maxBitrate
-                  << " start bitrate " << sendCodec->startBitrate
-                  << " max frame rate " << sendCodec->maxFramerate
-                  << " max payload size " << maxPayloadSize;
-  _mediaOpt.SetEncodingData(sendCodec->maxBitrate * 1000,
-                            sendCodec->startBitrate * 1000, sendCodec->width,
-                            sendCodec->height, sendCodec->maxFramerate,
-                            numLayers, maxPayloadSize);
-  return VCM_OK;
-}
-
-// Register an external decoder object.
-// This can not be used together with external decoder callbacks.
-void VideoSender::RegisterExternalEncoder(VideoEncoder* externalEncoder,
-                                          uint8_t payloadType,
-                                          bool internalSource /*= false*/) {
-  RTC_DCHECK(sequenced_checker_.CalledSequentially());
-
-  rtc::CritScope lock(&encoder_crit_);
-
-  if (externalEncoder == nullptr) {
-    bool wasSendCodec = false;
-    RTC_CHECK(
-        _codecDataBase.DeregisterExternalEncoder(payloadType, &wasSendCodec));
-    if (wasSendCodec) {
-      // Make sure the VCM doesn't use the de-registered codec
-      rtc::CritScope params_lock(&params_crit_);
-      _encoder = nullptr;
-      encoder_has_internal_source_ = false;
-    }
-    return;
-  }
-  _codecDataBase.RegisterExternalEncoder(externalEncoder, payloadType,
-                                         internalSource);
-}
-
-// Get encode bitrate
-int VideoSender::Bitrate(unsigned int* bitrate) const {
-  RTC_DCHECK(sequenced_checker_.CalledSequentially());
-  // Since we're running on the thread that's the only thread known to modify
-  // the value of _encoder, we don't need to grab the lock here.
-
-  if (!_encoder)
-    return VCM_UNINITIALIZED;
-  *bitrate = _encoder->GetEncoderParameters().target_bitrate.get_sum_bps();
-  return 0;
-}
-
-// Get encode frame rate
-int VideoSender::FrameRate(unsigned int* framerate) const {
-  RTC_DCHECK(sequenced_checker_.CalledSequentially());
-  // Since we're running on the thread that's the only thread known to modify
-  // the value of _encoder, we don't need to grab the lock here.
-
-  if (!_encoder)
-    return VCM_UNINITIALIZED;
-
-  *framerate = _encoder->GetEncoderParameters().input_frame_rate;
-  return 0;
-}
-
-EncoderParameters VideoSender::UpdateEncoderParameters(
-    const EncoderParameters& params,
-    VideoBitrateAllocator* bitrate_allocator,
-    uint32_t target_bitrate_bps) {
-  uint32_t video_target_rate_bps = _mediaOpt.SetTargetRates(target_bitrate_bps);
-  uint32_t input_frame_rate = _mediaOpt.InputFrameRate();
-  if (input_frame_rate == 0)
-    input_frame_rate = current_codec_.maxFramerate;
-
-  BitrateAllocation bitrate_allocation;
-  if (bitrate_allocator) {
-    bitrate_allocation = bitrate_allocator->GetAllocation(video_target_rate_bps,
-                                                          input_frame_rate);
-  } else {
-    DefaultVideoBitrateAllocator default_allocator(current_codec_);
-    bitrate_allocation = default_allocator.GetAllocation(video_target_rate_bps,
-                                                         input_frame_rate);
-  }
-  EncoderParameters new_encoder_params = {bitrate_allocation, params.loss_rate,
-                                          params.rtt, input_frame_rate};
-  return new_encoder_params;
-}
-
-void VideoSender::UpdateChannelParemeters(
-    VideoBitrateAllocator* bitrate_allocator,
-    VideoBitrateAllocationObserver* bitrate_updated_callback) {
-  BitrateAllocation target_rate;
-  {
-    rtc::CritScope cs(&params_crit_);
-    encoder_params_ =
-        UpdateEncoderParameters(encoder_params_, bitrate_allocator,
-                                encoder_params_.target_bitrate.get_sum_bps());
-    target_rate = encoder_params_.target_bitrate;
-  }
-  if (bitrate_updated_callback)
-    bitrate_updated_callback->OnBitrateAllocationUpdated(target_rate);
-}
-
-int32_t VideoSender::SetChannelParameters(
-    uint32_t target_bitrate_bps,
-    uint8_t loss_rate,
-    int64_t rtt,
-    VideoBitrateAllocator* bitrate_allocator,
-    VideoBitrateAllocationObserver* bitrate_updated_callback) {
-  EncoderParameters encoder_params;
-  encoder_params.loss_rate = loss_rate;
-  encoder_params.rtt = rtt;
-  encoder_params = UpdateEncoderParameters(encoder_params, bitrate_allocator,
-                                           target_bitrate_bps);
-  if (bitrate_updated_callback) {
-    bitrate_updated_callback->OnBitrateAllocationUpdated(
-        encoder_params.target_bitrate);
-  }
-
-  bool encoder_has_internal_source;
-  {
-    rtc::CritScope cs(&params_crit_);
-    encoder_params_ = encoder_params;
-    encoder_has_internal_source = encoder_has_internal_source_;
-  }
-
-  // For encoders with internal sources, we need to tell the encoder directly,
-  // instead of waiting for an AddVideoFrame that will never come (internal
-  // source encoders don't get input frames).
-  if (encoder_has_internal_source) {
-    rtc::CritScope cs(&encoder_crit_);
-    if (_encoder) {
-      SetEncoderParameters(encoder_params, encoder_has_internal_source);
-    }
-  }
-
-  return VCM_OK;
-}
-
-void VideoSender::SetEncoderParameters(EncoderParameters params,
-                                       bool has_internal_source) {
-  // |target_bitrate == 0 | means that the network is down or the send pacer is
-  // full. We currently only report this if the encoder has an internal source.
-  // If the encoder does not have an internal source, higher levels are expected
-  // to not call AddVideoFrame. We do this since its unclear how current
-  // encoder implementations behave when given a zero target bitrate.
-  // TODO(perkj): Make sure all known encoder implementations handle zero
-  // target bitrate and remove this check.
-  if (!has_internal_source && params.target_bitrate.get_sum_bps() == 0)
-    return;
-
-  if (params.input_frame_rate == 0) {
-    // No frame rate estimate available, use default.
-    params.input_frame_rate = current_codec_.maxFramerate;
-  }
-  if (_encoder != nullptr)
-    _encoder->SetEncoderParameters(params);
-}
-
-// Deprecated:
-// TODO(perkj): Remove once no projects call this method. It currently do
-// nothing.
-int32_t VideoSender::RegisterProtectionCallback(
-    VCMProtectionCallback* protection_callback) {
-  // Deprecated:
-  // TODO(perkj): Remove once no projects call this method. It currently do
-  // nothing.
-  return VCM_OK;
-}
-
-// Add one raw video frame to the encoder, blocking.
-int32_t VideoSender::AddVideoFrame(const VideoFrame& videoFrame,
-                                   const CodecSpecificInfo* codecSpecificInfo) {
-  EncoderParameters encoder_params;
-  std::vector<FrameType> next_frame_types;
-  bool encoder_has_internal_source = false;
-  {
-    rtc::CritScope lock(&params_crit_);
-    encoder_params = encoder_params_;
-    next_frame_types = next_frame_types_;
-    encoder_has_internal_source = encoder_has_internal_source_;
-  }
-  rtc::CritScope lock(&encoder_crit_);
-  if (_encoder == nullptr)
-    return VCM_UNINITIALIZED;
-  SetEncoderParameters(encoder_params, encoder_has_internal_source);
-  if (_mediaOpt.DropFrame()) {
-    LOG(LS_VERBOSE) << "Drop Frame "
-                    << "target bitrate "
-                    << encoder_params.target_bitrate.get_sum_bps()
-                    << " loss rate " << encoder_params.loss_rate << " rtt "
-                    << encoder_params.rtt << " input frame rate "
-                    << encoder_params.input_frame_rate;
-    post_encode_callback_->OnDroppedFrame();
-    return VCM_OK;
-  }
-  // TODO(pbos): Make sure setting send codec is synchronized with video
-  // processing so frame size always matches.
-  if (!_codecDataBase.MatchesCurrentResolution(videoFrame.width(),
-                                               videoFrame.height())) {
-    LOG(LS_ERROR) << "Incoming frame doesn't match set resolution. Dropping.";
-    return VCM_PARAMETER_ERROR;
-  }
-  VideoFrame converted_frame = videoFrame;
-  if (converted_frame.video_frame_buffer()->native_handle() &&
-      !_encoder->SupportsNativeHandle()) {
-    // This module only supports software encoding.
-    // TODO(pbos): Offload conversion from the encoder thread.
-    rtc::scoped_refptr<VideoFrameBuffer> converted_buffer(
-        converted_frame.video_frame_buffer()->NativeToI420Buffer());
-
-    if (!converted_buffer) {
-      LOG(LS_ERROR) << "Frame conversion failed, dropping frame.";
-      return VCM_PARAMETER_ERROR;
-    }
-    converted_frame = VideoFrame(converted_buffer,
-                                 converted_frame.timestamp(),
-                                 converted_frame.render_time_ms(),
-                                 converted_frame.rotation());
-  }
-  int32_t ret =
-      _encoder->Encode(converted_frame, codecSpecificInfo, next_frame_types);
-  if (ret < 0) {
-    LOG(LS_ERROR) << "Failed to encode frame. Error code: " << ret;
-    return ret;
-  }
-
-  {
-    rtc::CritScope lock(&params_crit_);
-    // Change all keyframe requests to encode delta frames the next time.
-    for (size_t i = 0; i < next_frame_types_.size(); ++i) {
-      // Check for equality (same requested as before encoding) to not
-      // accidentally drop a keyframe request while encoding.
-      if (next_frame_types[i] == next_frame_types_[i])
-        next_frame_types_[i] = kVideoFrameDelta;
-    }
-  }
-  return VCM_OK;
-}
-
-int32_t VideoSender::IntraFrameRequest(size_t stream_index) {
-  {
-    rtc::CritScope lock(&params_crit_);
-    if (stream_index >= next_frame_types_.size()) {
-      return -1;
-    }
-    next_frame_types_[stream_index] = kVideoFrameKey;
-    if (!encoder_has_internal_source_)
-      return VCM_OK;
-  }
-  // TODO(pbos): Remove when InternalSource() is gone. Both locks have to be
-  // held here for internal consistency, since _encoder could be removed while
-  // not holding encoder_crit_. Checks have to be performed again since
-  // params_crit_ was dropped to not cause lock-order inversions with
-  // encoder_crit_.
-  rtc::CritScope lock(&encoder_crit_);
-  rtc::CritScope params_lock(&params_crit_);
-  if (stream_index >= next_frame_types_.size())
-    return -1;
-  if (_encoder != nullptr && _encoder->InternalSource()) {
-    // Try to request the frame if we have an external encoder with
-    // internal source since AddVideoFrame never will be called.
-    if (_encoder->RequestFrame(next_frame_types_) == WEBRTC_VIDEO_CODEC_OK) {
-      // Try to remove just-performed keyframe request, if stream still exists.
-      next_frame_types_[stream_index] = kVideoFrameDelta;
-    }
-  }
-  return VCM_OK;
-}
-
-int32_t VideoSender::EnableFrameDropper(bool enable) {
-  rtc::CritScope lock(&encoder_crit_);
-  frame_dropper_enabled_ = enable;
-  _mediaOpt.EnableFrameDropper(enable);
-  return VCM_OK;
-}
-}  // namespace vcm
-}  // namespace webrtc
+/*
+ *  Copyright (c) 2013 The WebRTC project authors. All Rights Reserved.
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ */
+
+
+#include <algorithm>  // std::max
+
+#include "webrtc/base/checks.h"
+#include "webrtc/base/logging.h"
+#include "webrtc/common_types.h"
+#include "webrtc/common_video/include/video_bitrate_allocator.h"
+#include "webrtc/common_video/libyuv/include/webrtc_libyuv.h"
+#include "webrtc/modules/video_coding/codecs/vp8/temporal_layers.h"
+#include "webrtc/modules/video_coding/include/video_codec_interface.h"
+#include "webrtc/modules/video_coding/encoded_frame.h"
+#include "webrtc/modules/video_coding/utility/default_video_bitrate_allocator.h"
+#include "webrtc/modules/video_coding/utility/quality_scaler.h"
+#include "webrtc/modules/video_coding/video_coding_impl.h"
+#include "webrtc/system_wrappers/include/clock.h"
+
+namespace webrtc {
+namespace vcm {
+
+VideoSender::VideoSender(Clock* clock,
+                         EncodedImageCallback* post_encode_callback,
+                         VCMSendStatisticsCallback* send_stats_callback)
+    : clock_(clock),
+      _encoder(nullptr),
+      _mediaOpt(clock_),
+      _encodedFrameCallback(post_encode_callback, &_mediaOpt),
+      post_encode_callback_(post_encode_callback),
+      send_stats_callback_(send_stats_callback),
+      _codecDataBase(&_encodedFrameCallback),
+      frame_dropper_enabled_(true),
+      _sendStatsTimer(VCMProcessTimer::kDefaultProcessIntervalMs, clock_),
+      current_codec_(),
+      encoder_params_({BitrateAllocation(), 0, 0, 0}),
+      encoder_has_internal_source_(false),
+      next_frame_types_(1, kVideoFrameDelta) {
+  _mediaOpt.Reset();
+  // Allow VideoSender to be created on one thread but used on another, post
+  // construction. This is currently how this class is being used by at least
+  // one external project (diffractor).
+  sequenced_checker_.Detach();
+}
+
+VideoSender::~VideoSender() {}
+
+void VideoSender::Process() {
+  if (_sendStatsTimer.TimeUntilProcess() == 0) {
+    // |_sendStatsTimer.Processed()| must be called. Otherwise
+    // VideoSender::Process() will be called in an infinite loop.
+    _sendStatsTimer.Processed();
+    if (send_stats_callback_) {
+      uint32_t bitRate = _mediaOpt.SentBitRate();
+      uint32_t frameRate = _mediaOpt.SentFrameRate();
+      send_stats_callback_->SendStatistics(bitRate, frameRate);
+    }
+  }
+}
+
+int64_t VideoSender::TimeUntilNextProcess() {
+  return _sendStatsTimer.TimeUntilProcess();
+}
+
+// Register the send codec to be used.
+int32_t VideoSender::RegisterSendCodec(const VideoCodec* sendCodec,
+                                       uint32_t numberOfCores,
+                                       uint32_t maxPayloadSize) {
+  RTC_DCHECK(sequenced_checker_.CalledSequentially());
+  rtc::CritScope lock(&encoder_crit_);
+  if (sendCodec == nullptr) {
+    return VCM_PARAMETER_ERROR;
+  }
+
+  bool ret =
+      _codecDataBase.SetSendCodec(sendCodec, numberOfCores, maxPayloadSize);
+
+  // Update encoder regardless of result to make sure that we're not holding on
+  // to a deleted instance.
+  _encoder = _codecDataBase.GetEncoder();
+  // Cache the current codec here so they can be fetched from this thread
+  // without requiring the _sendCritSect lock.
+  current_codec_ = *sendCodec;
+
+  if (!ret) {
+    LOG(LS_ERROR) << "Failed to initialize set encoder with payload name '"
+                  << sendCodec->plName << "'.";
+    return VCM_CODEC_ERROR;
+  }
+
+  // SetSendCodec succeeded, _encoder should be set.
+  RTC_DCHECK(_encoder);
+
+  int numLayers;
+  if (sendCodec->codecType == kVideoCodecVP8) {
+    numLayers = sendCodec->VP8().numberOfTemporalLayers;
+  } else if (sendCodec->codecType == kVideoCodecVP9) {
+    numLayers = sendCodec->VP9().numberOfTemporalLayers;
+  } else {
+    numLayers = 1;
+  }
+
+  // If we have screensharing and we have layers, we disable frame dropper.
+  bool disable_frame_dropper =
+      numLayers > 1 && sendCodec->mode == kScreensharing;
+  if (disable_frame_dropper) {
+    _mediaOpt.EnableFrameDropper(false);
+  } else if (frame_dropper_enabled_) {
+    _mediaOpt.EnableFrameDropper(true);
+  }
+
+  {
+    rtc::CritScope cs(&params_crit_);
+    next_frame_types_.clear();
+    next_frame_types_.resize(VCM_MAX(sendCodec->numberOfSimulcastStreams, 1),
+                             kVideoFrameKey);
+    // Cache InternalSource() to have this available from IntraFrameRequest()
+    // without having to acquire encoder_crit_ (avoid blocking on encoder use).
+    encoder_has_internal_source_ = _encoder->InternalSource();
+  }
+
+  LOG(LS_VERBOSE) << " max bitrate " << sendCodec->maxBitrate
+                  << " start bitrate " << sendCodec->startBitrate
+                  << " max frame rate " << sendCodec->maxFramerate
+                  << " max payload size " << maxPayloadSize;
+  _mediaOpt.SetEncodingData(sendCodec->maxBitrate * 1000,
+                            sendCodec->startBitrate * 1000, sendCodec->width,
+                            sendCodec->height, sendCodec->maxFramerate,
+                            numLayers, maxPayloadSize);
+  return VCM_OK;
+}
+
+// Register an external decoder object.
+// This can not be used together with external decoder callbacks.
+void VideoSender::RegisterExternalEncoder(VideoEncoder* externalEncoder,
+                                          uint8_t payloadType,
+                                          bool internalSource /*= false*/) {
+  RTC_DCHECK(sequenced_checker_.CalledSequentially());
+
+  rtc::CritScope lock(&encoder_crit_);
+
+  if (externalEncoder == nullptr) {
+    bool wasSendCodec = false;
+    RTC_CHECK(
+        _codecDataBase.DeregisterExternalEncoder(payloadType, &wasSendCodec));
+    if (wasSendCodec) {
+      // Make sure the VCM doesn't use the de-registered codec
+      rtc::CritScope params_lock(&params_crit_);
+      _encoder = nullptr;
+      encoder_has_internal_source_ = false;
+    }
+    return;
+  }
+  _codecDataBase.RegisterExternalEncoder(externalEncoder, payloadType,
+                                         internalSource);
+}
+
+// Get encode bitrate
+int VideoSender::Bitrate(unsigned int* bitrate) const {
+  RTC_DCHECK(sequenced_checker_.CalledSequentially());
+  // Since we're running on the thread that's the only thread known to modify
+  // the value of _encoder, we don't need to grab the lock here.
+
+  if (!_encoder)
+    return VCM_UNINITIALIZED;
+  *bitrate = _encoder->GetEncoderParameters().target_bitrate.get_sum_bps();
+  return 0;
+}
+
+// Get encode frame rate
+int VideoSender::FrameRate(unsigned int* framerate) const {
+  RTC_DCHECK(sequenced_checker_.CalledSequentially());
+  // Since we're running on the thread that's the only thread known to modify
+  // the value of _encoder, we don't need to grab the lock here.
+
+  if (!_encoder)
+    return VCM_UNINITIALIZED;
+
+  *framerate = _encoder->GetEncoderParameters().input_frame_rate;
+  return 0;
+}
+
+EncoderParameters VideoSender::UpdateEncoderParameters(
+    const EncoderParameters& params,
+    VideoBitrateAllocator* bitrate_allocator,
+    uint32_t target_bitrate_bps) {
+  uint32_t video_target_rate_bps = _mediaOpt.SetTargetRates(target_bitrate_bps);
+  uint32_t input_frame_rate = _mediaOpt.InputFrameRate();
+  if (input_frame_rate == 0)
+    input_frame_rate = current_codec_.maxFramerate;
+
+  BitrateAllocation bitrate_allocation;
+  if (bitrate_allocator) {
+    bitrate_allocation = bitrate_allocator->GetAllocation(video_target_rate_bps,
+                                                          input_frame_rate);
+  } else {
+    DefaultVideoBitrateAllocator default_allocator(current_codec_);
+    bitrate_allocation = default_allocator.GetAllocation(video_target_rate_bps,
+                                                         input_frame_rate);
+  }
+  EncoderParameters new_encoder_params = {bitrate_allocation, params.loss_rate,
+                                          params.rtt, input_frame_rate};
+  return new_encoder_params;
+}
+
+void VideoSender::UpdateChannelParemeters(
+    VideoBitrateAllocator* bitrate_allocator,
+    VideoBitrateAllocationObserver* bitrate_updated_callback) {
+  BitrateAllocation target_rate;
+  {
+    rtc::CritScope cs(&params_crit_);
+    encoder_params_ =
+        UpdateEncoderParameters(encoder_params_, bitrate_allocator,
+                                encoder_params_.target_bitrate.get_sum_bps());
+    target_rate = encoder_params_.target_bitrate;
+  }
+  if (bitrate_updated_callback)
+    bitrate_updated_callback->OnBitrateAllocationUpdated(target_rate);
+}
+
+int32_t VideoSender::SetChannelParameters(
+    uint32_t target_bitrate_bps,
+    uint8_t loss_rate,
+    int64_t rtt,
+    VideoBitrateAllocator* bitrate_allocator,
+    VideoBitrateAllocationObserver* bitrate_updated_callback) {
+  EncoderParameters encoder_params;
+  encoder_params.loss_rate = loss_rate;
+  encoder_params.rtt = rtt;
+  encoder_params = UpdateEncoderParameters(encoder_params, bitrate_allocator,
+                                           target_bitrate_bps);
+  if (bitrate_updated_callback) {
+    bitrate_updated_callback->OnBitrateAllocationUpdated(
+        encoder_params.target_bitrate);
+  }
+
+  bool encoder_has_internal_source;
+  {
+    rtc::CritScope cs(&params_crit_);
+    encoder_params_ = encoder_params;
+    encoder_has_internal_source = encoder_has_internal_source_;
+  }
+
+  // For encoders with internal sources, we need to tell the encoder directly,
+  // instead of waiting for an AddVideoFrame that will never come (internal
+  // source encoders don't get input frames).
+  if (encoder_has_internal_source) {
+    rtc::CritScope cs(&encoder_crit_);
+    if (_encoder) {
+      SetEncoderParameters(encoder_params, encoder_has_internal_source);
+    }
+  }
+
+  return VCM_OK;
+}
+
+void VideoSender::SetEncoderParameters(EncoderParameters params,
+                                       bool has_internal_source) {
+  // |target_bitrate == 0 | means that the network is down or the send pacer is
+  // full. We currently only report this if the encoder has an internal source.
+  // If the encoder does not have an internal source, higher levels are expected
+  // to not call AddVideoFrame. We do this since its unclear how current
+  // encoder implementations behave when given a zero target bitrate.
+  // TODO(perkj): Make sure all known encoder implementations handle zero
+  // target bitrate and remove this check.
+  if (!has_internal_source && params.target_bitrate.get_sum_bps() == 0)
+    return;
+
+  if (params.input_frame_rate == 0) {
+    // No frame rate estimate available, use default.
+    params.input_frame_rate = current_codec_.maxFramerate;
+  }
+  if (_encoder != nullptr)
+    _encoder->SetEncoderParameters(params);
+}
+
+// Deprecated:
+// TODO(perkj): Remove once no projects call this method. It currently do
+// nothing.
+int32_t VideoSender::RegisterProtectionCallback(
+    VCMProtectionCallback* protection_callback) {
+  // Deprecated:
+  // TODO(perkj): Remove once no projects call this method. It currently do
+  // nothing.
+  return VCM_OK;
+}
+
+// Add one raw video frame to the encoder, blocking.
+int32_t VideoSender::AddVideoFrame(const VideoFrame& videoFrame,
+                                   const CodecSpecificInfo* codecSpecificInfo) {
+  EncoderParameters encoder_params;
+  std::vector<FrameType> next_frame_types;
+  bool encoder_has_internal_source = false;
+  {
+    rtc::CritScope lock(&params_crit_);
+    encoder_params = encoder_params_;
+    next_frame_types = next_frame_types_;
+    encoder_has_internal_source = encoder_has_internal_source_;
+  }
+  rtc::CritScope lock(&encoder_crit_);
+  if (_encoder == nullptr)
+    return VCM_UNINITIALIZED;
+  SetEncoderParameters(encoder_params, encoder_has_internal_source);
+#if 0
+  if (_mediaOpt.DropFrame()) {
+    LOG(LS_VERBOSE) << "Drop Frame "
+                    << "target bitrate "
+                    << encoder_params.target_bitrate.get_sum_bps()
+                    << " loss rate " << encoder_params.loss_rate << " rtt "
+                    << encoder_params.rtt << " input frame rate "
+                    << encoder_params.input_frame_rate;
+    post_encode_callback_->OnDroppedFrame();
+    return VCM_OK;
+  }
+#endif
+  // TODO(pbos): Make sure setting send codec is synchronized with video
+  // processing so frame size always matches.
+  if (!_codecDataBase.MatchesCurrentResolution(videoFrame.width(),
+                                               videoFrame.height())) {
+    LOG(LS_ERROR) << "Incoming frame doesn't match set resolution. Dropping.";
+    return VCM_PARAMETER_ERROR;
+  }
+  VideoFrame converted_frame = videoFrame;
+  if (converted_frame.video_frame_buffer()->native_handle() &&
+      !_encoder->SupportsNativeHandle()) {
+    // This module only supports software encoding.
+    // TODO(pbos): Offload conversion from the encoder thread.
+    rtc::scoped_refptr<VideoFrameBuffer> converted_buffer(
+        converted_frame.video_frame_buffer()->NativeToI420Buffer());
+
+    if (!converted_buffer) {
+      LOG(LS_ERROR) << "Frame conversion failed, dropping frame.";
+      return VCM_PARAMETER_ERROR;
+    }
+    converted_frame = VideoFrame(converted_buffer,
+                                 converted_frame.timestamp(),
+                                 converted_frame.render_time_ms(),
+                                 converted_frame.rotation());
+  }
+  int32_t ret =
+      _encoder->Encode(converted_frame, codecSpecificInfo, next_frame_types);
+  if (ret < 0) {
+    LOG(LS_ERROR) << "Failed to encode frame. Error code: " << ret;
+    return ret;
+  }
+
+  {
+    rtc::CritScope lock(&params_crit_);
+    // Change all keyframe requests to encode delta frames the next time.
+    for (size_t i = 0; i < next_frame_types_.size(); ++i) {
+      // Check for equality (same requested as before encoding) to not
+      // accidentally drop a keyframe request while encoding.
+      if (next_frame_types[i] == next_frame_types_[i])
+        next_frame_types_[i] = kVideoFrameDelta;
+    }
+  }
+  return VCM_OK;
+}
+
+int32_t VideoSender::IntraFrameRequest(size_t stream_index) {
+  {
+    rtc::CritScope lock(&params_crit_);
+    if (stream_index >= next_frame_types_.size()) {
+      return -1;
+    }
+    next_frame_types_[stream_index] = kVideoFrameKey;
+    if (!encoder_has_internal_source_)
+      return VCM_OK;
+  }
+  // TODO(pbos): Remove when InternalSource() is gone. Both locks have to be
+  // held here for internal consistency, since _encoder could be removed while
+  // not holding encoder_crit_. Checks have to be performed again since
+  // params_crit_ was dropped to not cause lock-order inversions with
+  // encoder_crit_.
+  rtc::CritScope lock(&encoder_crit_);
+  rtc::CritScope params_lock(&params_crit_);
+  if (stream_index >= next_frame_types_.size())
+    return -1;
+  if (_encoder != nullptr && _encoder->InternalSource()) {
+    // Try to request the frame if we have an external encoder with
+    // internal source since AddVideoFrame never will be called.
+    if (_encoder->RequestFrame(next_frame_types_) == WEBRTC_VIDEO_CODEC_OK) {
+      // Try to remove just-performed keyframe request, if stream still exists.
+      next_frame_types_[stream_index] = kVideoFrameDelta;
+    }
+  }
+  return VCM_OK;
+}
+
+int32_t VideoSender::EnableFrameDropper(bool enable) {
+  rtc::CritScope lock(&encoder_crit_);
+  frame_dropper_enabled_ = enable;
+  _mediaOpt.EnableFrameDropper(enable);
+  return VCM_OK;
+}
+}  // namespace vcm
+}  // namespace webrtc
diff --git a/webrtc/video/BUILD.gn b/webrtc/video/BUILD.gn
index a5a3b7c..f2d27d0 100644
--- a/webrtc/video/BUILD.gn
+++ b/webrtc/video/BUILD.gn
@@ -77,7 +77,7 @@ rtc_static_library("video") {
   ]
 }
 
-if (rtc_include_tests) {
+if (true) {
   rtc_source_set("video_quality_test") {
     testonly = true
     sources = [
diff --git a/webrtc/video/video_loopback.cc b/webrtc/video/video_loopback.cc
index f86078e..aacf412 100644
--- a/webrtc/video/video_loopback.cc
+++ b/webrtc/video/video_loopback.cc
@@ -1,294 +1,295 @@
-/*
- *  Copyright (c) 2015 The WebRTC project authors. All Rights Reserved.
- *
- *  Use of this source code is governed by a BSD-style license
- *  that can be found in the LICENSE file in the root of the source
- *  tree. An additional intellectual property rights grant can be found
- *  in the file PATENTS.  All contributing project authors may
- *  be found in the AUTHORS file in the root of the source tree.
- */
-
-#include <stdio.h>
-
-#include "gflags/gflags.h"
-#include "webrtc/test/field_trial.h"
-#include "webrtc/test/gtest.h"
-#include "webrtc/test/run_test.h"
-#include "webrtc/video/video_quality_test.h"
-
-namespace webrtc {
-namespace flags {
-
-// Flags common with screenshare loopback, with different default values.
-DEFINE_int32(width, 640, "Video width.");
-size_t Width() {
-  return static_cast<size_t>(FLAGS_width);
-}
-
-DEFINE_int32(height, 480, "Video height.");
-size_t Height() {
-  return static_cast<size_t>(FLAGS_height);
-}
-
-DEFINE_int32(fps, 30, "Frames per second.");
-int Fps() {
-  return static_cast<int>(FLAGS_fps);
-}
-
-DEFINE_int32(min_bitrate, 50, "Call and stream min bitrate in kbps.");
-int MinBitrateKbps() {
-  return static_cast<int>(FLAGS_min_bitrate);
-}
-
-DEFINE_int32(start_bitrate, 300, "Call start bitrate in kbps.");
-int StartBitrateKbps() {
-  return static_cast<int>(FLAGS_start_bitrate);
-}
-
-DEFINE_int32(target_bitrate, 800, "Stream target bitrate in kbps.");
-int TargetBitrateKbps() {
-  return static_cast<int>(FLAGS_target_bitrate);
-}
-
-DEFINE_int32(max_bitrate, 800, "Call and stream max bitrate in kbps.");
-int MaxBitrateKbps() {
-  return static_cast<int>(FLAGS_max_bitrate);
-}
-
-DEFINE_bool(suspend_below_min_bitrate,
-            false,
-            "Suspends video below the configured min bitrate.");
-
-DEFINE_int32(num_temporal_layers,
-             1,
-             "Number of temporal layers. Set to 1-4 to override.");
-int NumTemporalLayers() {
-  return static_cast<int>(FLAGS_num_temporal_layers);
-}
-
-// Flags common with screenshare loopback, with equal default values.
-DEFINE_string(codec, "VP8", "Video codec to use.");
-std::string Codec() {
-  return static_cast<std::string>(FLAGS_codec);
-}
-
-DEFINE_int32(selected_tl,
-             -1,
-             "Temporal layer to show or analyze. -1 to disable filtering.");
-int SelectedTL() {
-  return static_cast<int>(FLAGS_selected_tl);
-}
-
-DEFINE_int32(
-    duration,
-    0,
-    "Duration of the test in seconds. If 0, rendered will be shown instead.");
-int DurationSecs() {
-  return static_cast<int>(FLAGS_duration);
-}
-
-DEFINE_string(output_filename, "", "Target graph data filename.");
-std::string OutputFilename() {
-  return static_cast<std::string>(FLAGS_output_filename);
-}
-
-DEFINE_string(graph_title,
-              "",
-              "If empty, title will be generated automatically.");
-std::string GraphTitle() {
-  return static_cast<std::string>(FLAGS_graph_title);
-}
-
-DEFINE_int32(loss_percent, 0, "Percentage of packets randomly lost.");
-int LossPercent() {
-  return static_cast<int>(FLAGS_loss_percent);
-}
-
-DEFINE_int32(avg_burst_loss_length,
-             -1,
-             "Average burst length of lost packets.");
-int AvgBurstLossLength() {
-  return static_cast<int>(FLAGS_avg_burst_loss_length);
-}
-
-DEFINE_int32(link_capacity,
-             0,
-             "Capacity (kbps) of the fake link. 0 means infinite.");
-int LinkCapacityKbps() {
-  return static_cast<int>(FLAGS_link_capacity);
-}
-
-DEFINE_int32(queue_size, 0, "Size of the bottleneck link queue in packets.");
-int QueueSize() {
-  return static_cast<int>(FLAGS_queue_size);
-}
-
-DEFINE_int32(avg_propagation_delay_ms,
-             0,
-             "Average link propagation delay in ms.");
-int AvgPropagationDelayMs() {
-  return static_cast<int>(FLAGS_avg_propagation_delay_ms);
-}
-
-DEFINE_int32(std_propagation_delay_ms,
-             0,
-             "Link propagation delay standard deviation in ms.");
-int StdPropagationDelayMs() {
-  return static_cast<int>(FLAGS_std_propagation_delay_ms);
-}
-
-DEFINE_int32(selected_stream, 0, "ID of the stream to show or analyze.");
-int SelectedStream() {
-  return static_cast<int>(FLAGS_selected_stream);
-}
-
-DEFINE_int32(num_spatial_layers, 1, "Number of spatial layers to use.");
-int NumSpatialLayers() {
-  return static_cast<int>(FLAGS_num_spatial_layers);
-}
-
-DEFINE_int32(selected_sl,
-             -1,
-             "Spatial layer to show or analyze. -1 to disable filtering.");
-int SelectedSL() {
-  return static_cast<int>(FLAGS_selected_sl);
-}
-
-DEFINE_string(stream0,
-              "",
-              "Comma separated values describing VideoStream for stream #0.");
-std::string Stream0() {
-  return static_cast<std::string>(FLAGS_stream0);
-}
-
-DEFINE_string(stream1,
-              "",
-              "Comma separated values describing VideoStream for stream #1.");
-std::string Stream1() {
-  return static_cast<std::string>(FLAGS_stream1);
-}
-
-DEFINE_string(sl0,
-              "",
-              "Comma separated values describing SpatialLayer for layer #0.");
-std::string SL0() {
-  return static_cast<std::string>(FLAGS_sl0);
-}
-
-DEFINE_string(sl1,
-              "",
-              "Comma separated values describing SpatialLayer for layer #1.");
-std::string SL1() {
-  return static_cast<std::string>(FLAGS_sl1);
-}
-
-DEFINE_string(encoded_frame_path,
-              "",
-              "The base path for encoded frame logs. Created files will have "
-              "the form <encoded_frame_path>.<n>.(recv|send.<m>).ivf");
-std::string EncodedFramePath() {
-  return static_cast<std::string>(FLAGS_encoded_frame_path);
-}
-
-DEFINE_bool(logs, false, "print logs to stderr");
-
-DEFINE_bool(send_side_bwe, true, "Use send-side bandwidth estimation");
-
-DEFINE_bool(allow_reordering, false, "Allow packet reordering to occur");
-
-DEFINE_bool(use_ulpfec, false, "Use RED+ULPFEC forward error correction.");
-
-DEFINE_bool(use_flexfec, false, "Use FlexFEC forward error correction.");
-
-DEFINE_bool(audio, false, "Add audio stream");
-
-DEFINE_bool(audio_video_sync, false, "Sync audio and video stream (no effect if"
-    " audio is false)");
-
-DEFINE_bool(video, true, "Add video stream");
-
-DEFINE_string(
-    force_fieldtrials,
-    "",
-    "Field trials control experimental feature code which can be forced. "
-    "E.g. running with --force_fieldtrials=WebRTC-FooFeature/Enable/"
-    " will assign the group Enable to field trial WebRTC-FooFeature. Multiple "
-    "trials are separated by \"/\"");
-
-// Video-specific flags.
-DEFINE_string(clip,
-              "",
-              "Name of the clip to show. If empty, using chroma generator.");
-std::string Clip() {
-  return static_cast<std::string>(FLAGS_clip);
-}
-
-}  // namespace flags
-
-void Loopback() {
-  FakeNetworkPipe::Config pipe_config;
-  pipe_config.loss_percent = flags::LossPercent();
-  pipe_config.avg_burst_loss_length = flags::AvgBurstLossLength();
-  pipe_config.link_capacity_kbps = flags::LinkCapacityKbps();
-  pipe_config.queue_length_packets = flags::QueueSize();
-  pipe_config.queue_delay_ms = flags::AvgPropagationDelayMs();
-  pipe_config.delay_standard_deviation_ms = flags::StdPropagationDelayMs();
-  pipe_config.allow_reordering = flags::FLAGS_allow_reordering;
-
-  Call::Config::BitrateConfig call_bitrate_config;
-  call_bitrate_config.min_bitrate_bps = flags::MinBitrateKbps() * 1000;
-  call_bitrate_config.start_bitrate_bps = flags::StartBitrateKbps() * 1000;
-  call_bitrate_config.max_bitrate_bps = flags::MaxBitrateKbps() * 1000;
-
-  VideoQualityTest::Params params;
-  params.call = {flags::FLAGS_send_side_bwe, call_bitrate_config};
-  params.video = {flags::FLAGS_video,
-                  flags::Width(),
-                  flags::Height(),
-                  flags::Fps(),
-                  flags::MinBitrateKbps() * 1000,
-                  flags::TargetBitrateKbps() * 1000,
-                  flags::MaxBitrateKbps() * 1000,
-                  flags::FLAGS_suspend_below_min_bitrate,
-                  flags::Codec(),
-                  flags::NumTemporalLayers(),
-                  flags::SelectedTL(),
-                  0,  // No min transmit bitrate.
-                  flags::FLAGS_use_ulpfec,
-                  flags::FLAGS_use_flexfec,
-                  flags::EncodedFramePath(),
-                  flags::Clip()};
-  params.audio = {flags::FLAGS_audio, flags::FLAGS_audio_video_sync};
-  params.screenshare.enabled = false;
-  params.analyzer = {"video", 0.0, 0.0, flags::DurationSecs(),
-      flags::OutputFilename(), flags::GraphTitle()};
-  params.pipe = pipe_config;
-  params.logs = flags::FLAGS_logs;
-
-  std::vector<std::string> stream_descriptors;
-  stream_descriptors.push_back(flags::Stream0());
-  stream_descriptors.push_back(flags::Stream1());
-  std::vector<std::string> SL_descriptors;
-  SL_descriptors.push_back(flags::SL0());
-  SL_descriptors.push_back(flags::SL1());
-  VideoQualityTest::FillScalabilitySettings(
-      &params, stream_descriptors, flags::SelectedStream(),
-      flags::NumSpatialLayers(), flags::SelectedSL(), SL_descriptors);
-
-  VideoQualityTest test;
-  if (flags::DurationSecs()) {
-    test.RunWithAnalyzer(params);
-  } else {
-    test.RunWithRenderers(params);
-  }
-}
-}  // namespace webrtc
-
-int main(int argc, char* argv[]) {
-  ::testing::InitGoogleTest(&argc, argv);
-  google::ParseCommandLineFlags(&argc, &argv, true);
-  webrtc::test::InitFieldTrialsFromString(
-      webrtc::flags::FLAGS_force_fieldtrials);
-  webrtc::test::RunTest(webrtc::Loopback);
-  return 0;
-}
+/*
+ *  Copyright (c) 2015 The WebRTC project authors. All Rights Reserved.
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ */
+
+#include <stdio.h>
+
+#include "gflags/gflags.h"
+#include "webrtc/test/field_trial.h"
+#include "webrtc/test/gtest.h"
+#include "webrtc/test/run_test.h"
+#include "webrtc/video/video_quality_test.h"
+
+namespace webrtc {
+namespace flags {
+
+// Flags common with screenshare loopback, with different default values.
+DEFINE_int32(width, 640, "Video width.");
+size_t Width() {
+  return static_cast<size_t>(FLAGS_width);
+}
+
+DEFINE_int32(height, 480, "Video height.");
+size_t Height() {
+  return static_cast<size_t>(FLAGS_height);
+}
+
+DEFINE_int32(fps, 30, "Frames per second.");
+int Fps() {
+  return static_cast<int>(FLAGS_fps);
+}
+
+DEFINE_int32(min_bitrate, 50, "Call and stream min bitrate in kbps.");
+int MinBitrateKbps() {
+  return static_cast<int>(FLAGS_min_bitrate);
+}
+
+DEFINE_int32(start_bitrate, 300, "Call start bitrate in kbps.");
+int StartBitrateKbps() {
+  return static_cast<int>(FLAGS_start_bitrate);
+}
+
+DEFINE_int32(target_bitrate, 800, "Stream target bitrate in kbps.");
+int TargetBitrateKbps() {
+  return static_cast<int>(FLAGS_target_bitrate);
+}
+
+DEFINE_int32(max_bitrate, 800, "Call and stream max bitrate in kbps.");
+int MaxBitrateKbps() {
+  return static_cast<int>(FLAGS_max_bitrate);
+}
+
+DEFINE_bool(suspend_below_min_bitrate,
+            false,
+            "Suspends video below the configured min bitrate.");
+
+DEFINE_int32(num_temporal_layers,
+             1,
+             "Number of temporal layers. Set to 1-4 to override.");
+int NumTemporalLayers() {
+  return static_cast<int>(FLAGS_num_temporal_layers);
+}
+
+// Flags common with screenshare loopback, with equal default values.
+DEFINE_string(codec, "H264", "Video codec to use.");
+std::string Codec() {
+  return static_cast<std::string>(FLAGS_codec);
+}
+
+DEFINE_int32(selected_tl,
+             -1,
+             "Temporal layer to show or analyze. -1 to disable filtering.");
+int SelectedTL() {
+  return static_cast<int>(FLAGS_selected_tl);
+}
+
+DEFINE_int32(
+    duration,
+    0,
+    "Duration of the test in seconds. If 0, rendered will be shown instead.");
+int DurationSecs() {
+  return static_cast<int>(FLAGS_duration);
+}
+
+DEFINE_string(output_filename, "", "Target graph data filename.");
+std::string OutputFilename() {
+  return static_cast<std::string>(FLAGS_output_filename);
+}
+
+DEFINE_string(graph_title,
+              "",
+              "If empty, title will be generated automatically.");
+std::string GraphTitle() {
+  return static_cast<std::string>(FLAGS_graph_title);
+}
+
+DEFINE_int32(loss_percent, 0, "Percentage of packets randomly lost.");
+int LossPercent() {
+  return static_cast<int>(FLAGS_loss_percent);
+}
+
+DEFINE_int32(avg_burst_loss_length,
+             -1,
+             "Average burst length of lost packets.");
+int AvgBurstLossLength() {
+  return static_cast<int>(FLAGS_avg_burst_loss_length);
+}
+
+DEFINE_int32(link_capacity,
+             0,
+             "Capacity (kbps) of the fake link. 0 means infinite.");
+int LinkCapacityKbps() {
+  return static_cast<int>(FLAGS_link_capacity);
+}
+
+DEFINE_int32(queue_size, 0, "Size of the bottleneck link queue in packets.");
+int QueueSize() {
+  return static_cast<int>(FLAGS_queue_size);
+}
+
+DEFINE_int32(avg_propagation_delay_ms,
+             0,
+             "Average link propagation delay in ms.");
+int AvgPropagationDelayMs() {
+  return static_cast<int>(FLAGS_avg_propagation_delay_ms);
+}
+
+DEFINE_int32(std_propagation_delay_ms,
+             0,
+             "Link propagation delay standard deviation in ms.");
+int StdPropagationDelayMs() {
+  return static_cast<int>(FLAGS_std_propagation_delay_ms);
+}
+
+DEFINE_int32(selected_stream, 0, "ID of the stream to show or analyze.");
+int SelectedStream() {
+  return static_cast<int>(FLAGS_selected_stream);
+}
+
+DEFINE_int32(num_spatial_layers, 1, "Number of spatial layers to use.");
+int NumSpatialLayers() {
+  return static_cast<int>(FLAGS_num_spatial_layers);
+}
+
+DEFINE_int32(selected_sl,
+             -1,
+             "Spatial layer to show or analyze. -1 to disable filtering.");
+int SelectedSL() {
+  return static_cast<int>(FLAGS_selected_sl);
+}
+
+DEFINE_string(stream0,
+              "",
+              "Comma separated values describing VideoStream for stream #0.");
+std::string Stream0() {
+  return static_cast<std::string>(FLAGS_stream0);
+}
+
+DEFINE_string(stream1,
+              "",
+              "Comma separated values describing VideoStream for stream #1.");
+std::string Stream1() {
+  return static_cast<std::string>(FLAGS_stream1);
+}
+
+DEFINE_string(sl0,
+              "",
+              "Comma separated values describing SpatialLayer for layer #0.");
+std::string SL0() {
+  return static_cast<std::string>(FLAGS_sl0);
+}
+
+DEFINE_string(sl1,
+              "",
+              "Comma separated values describing SpatialLayer for layer #1.");
+std::string SL1() {
+  return static_cast<std::string>(FLAGS_sl1);
+}
+
+DEFINE_string(encoded_frame_path,
+              "",
+              "The base path for encoded frame logs. Created files will have "
+              "the form <encoded_frame_path>.<n>.(recv|send.<m>).ivf");
+std::string EncodedFramePath() {
+  return static_cast<std::string>(FLAGS_encoded_frame_path);
+}
+
+DEFINE_bool(logs, false, "print logs to stderr");
+
+DEFINE_bool(send_side_bwe, true, "Use send-side bandwidth estimation");
+
+DEFINE_bool(allow_reordering, false, "Allow packet reordering to occur");
+
+DEFINE_bool(use_ulpfec, false, "Use RED+ULPFEC forward error correction.");
+
+DEFINE_bool(use_flexfec, false, "Use FlexFEC forward error correction.");
+
+DEFINE_bool(audio, false, "Add audio stream");
+
+DEFINE_bool(audio_video_sync, false, "Sync audio and video stream (no effect if"
+    " audio is false)");
+
+DEFINE_bool(video, true, "Add video stream");
+
+DEFINE_string(
+    force_fieldtrials,
+    "",
+    "Field trials control experimental feature code which can be forced. "
+    "E.g. running with --force_fieldtrials=WebRTC-FooFeature/Enable/"
+    " will assign the group Enable to field trial WebRTC-FooFeature. Multiple "
+    "trials are separated by \"/\"");
+
+// Video-specific flags.
+DEFINE_string(clip,
+              "",
+              "Name of the clip to show. If empty, using chroma generator.");
+std::string Clip() {
+  return static_cast<std::string>(FLAGS_clip);
+}
+
+}  // namespace flags
+
+void Loopback() {
+  FakeNetworkPipe::Config pipe_config;
+  pipe_config.loss_percent = flags::LossPercent();
+  pipe_config.avg_burst_loss_length = flags::AvgBurstLossLength();
+  pipe_config.link_capacity_kbps = flags::LinkCapacityKbps();
+  pipe_config.queue_length_packets = flags::QueueSize();
+  pipe_config.queue_delay_ms = flags::AvgPropagationDelayMs();
+  pipe_config.delay_standard_deviation_ms = flags::StdPropagationDelayMs();
+  pipe_config.allow_reordering = flags::FLAGS_allow_reordering;
+
+  Call::Config::BitrateConfig call_bitrate_config;
+  call_bitrate_config.min_bitrate_bps = flags::MinBitrateKbps() * 1000;
+  call_bitrate_config.start_bitrate_bps = flags::StartBitrateKbps() * 1000;
+  call_bitrate_config.max_bitrate_bps = flags::MaxBitrateKbps() * 1000;
+
+  VideoQualityTest::Params params;
+  params.call = {flags::FLAGS_send_side_bwe, call_bitrate_config};
+  params.video = {flags::FLAGS_video,
+                  flags::Width(),
+                  flags::Height(),
+                  flags::Fps(),
+                  flags::MinBitrateKbps() * 1000,
+                  flags::TargetBitrateKbps() * 1000,
+                  flags::MaxBitrateKbps() * 1000,
+                  flags::FLAGS_suspend_below_min_bitrate,
+                  flags::Codec(),
+                  flags::NumTemporalLayers(),
+                  flags::SelectedTL(),
+                  0,  // No min transmit bitrate.
+                  flags::FLAGS_use_ulpfec,
+                  flags::FLAGS_use_flexfec,
+                  flags::EncodedFramePath(),
+                  flags::Clip()};
+  params.audio = {flags::FLAGS_audio, flags::FLAGS_audio_video_sync};
+  params.screenshare.enabled = false;
+  params.analyzer = {"video", 0.0, 0.0, flags::DurationSecs(),
+      flags::OutputFilename(), flags::GraphTitle()};
+  params.pipe = pipe_config;
+  params.logs = flags::FLAGS_logs;
+
+  std::vector<std::string> stream_descriptors;
+  stream_descriptors.push_back(flags::Stream0());
+  stream_descriptors.push_back(flags::Stream1());
+  std::vector<std::string> SL_descriptors;
+  SL_descriptors.push_back(flags::SL0());
+  SL_descriptors.push_back(flags::SL1());
+  VideoQualityTest::FillScalabilitySettings(
+      &params, stream_descriptors, flags::SelectedStream(),
+      flags::NumSpatialLayers(), flags::SelectedSL(), SL_descriptors);
+
+  VideoQualityTest test;
+  if (flags::DurationSecs()) {
+    test.RunWithAnalyzer(params);
+  } else {
+    test.RunWithRenderers(params);
+  }
+}
+}  // namespace webrtc
+#if 1
+int main(int argc, char* argv[]) {
+  ::testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  webrtc::test::InitFieldTrialsFromString(
+      webrtc::flags::FLAGS_force_fieldtrials);
+  webrtc::test::RunTest(webrtc::Loopback);
+  return 0;
+}
+#endif
diff --git a/webrtc/video/vie_encoder.cc b/webrtc/video/vie_encoder.cc
index c0228f9..9bd298a 100644
--- a/webrtc/video/vie_encoder.cc
+++ b/webrtc/video/vie_encoder.cc
@@ -44,7 +44,7 @@ const int kMinPixelsPerFrame = 320 * 180;
 
 // The maximum number of frames to drop at beginning of stream
 // to try and achieve desired bitrate.
-const int kMaxInitialFramedrop = 4;
+const int kMaxInitialFramedrop = 0;
 
 // TODO(pbos): Lower these thresholds (to closer to 100%) when we handle
 // pipelining encoders better (multiple input frames before something comes
