From b6b1f6af3cfe96509dc42f06da79b6fdc385852e Mon Sep 17 00:00:00 2001
From: Andrei Ermilov <anderm@microsoft.com>
Date: Tue, 21 Nov 2017 18:51:44 -0500
Subject: [PATCH 1/2] nvencoder patch

---
 webrtc/api/video/i420_buffer.cc                    |   3 +-
 webrtc/api/video/video_frame.h                     |  12 +
 webrtc/base/BUILD.gn                               |   2 +-
 webrtc/media/base/mediaconstants.cc                |   1 +
 webrtc/media/base/mediaconstants.h                 |   1 +
 webrtc/media/engine/webrtcvideoengine2.cc          |  11 +-
 webrtc/modules/video_coding/BUILD.gn               |   5 +-
 .../video_coding/codecs/h264/h264_encoder_impl.cc  | 867 ++++++++++++++++-----
 .../video_coding/codecs/h264/h264_encoder_impl.h   | 121 ++-
 .../codecs/h264/h264_encoder_impl_unittest.cc      | 402 ++++++++--
 webrtc/modules/video_coding/video_sender.cc        |   2 +
 webrtc/video/BUILD.gn                              |   2 +-
 webrtc/video/video_loopback.cc                     |   5 +-
 webrtc/video/vie_encoder.cc                        |   2 +-
 14 files changed, 1182 insertions(+), 254 deletions(-)

diff --git a/webrtc/api/video/i420_buffer.cc b/webrtc/api/video/i420_buffer.cc
index 031b15940..8c89d3662 100644
--- a/webrtc/api/video/i420_buffer.cc
+++ b/webrtc/api/video/i420_buffer.cc
@@ -37,6 +37,7 @@ I420Buffer::I420Buffer(int width, int height)
     : I420Buffer(width, height, width, (width + 1) / 2, (width + 1) / 2) {
 }
 
+
 I420Buffer::I420Buffer(int width,
                        int height,
                        int stride_y,
@@ -48,7 +49,7 @@ I420Buffer::I420Buffer(int width,
       stride_u_(stride_u),
       stride_v_(stride_v),
       data_(static_cast<uint8_t*>(AlignedMalloc(
-          I420DataSize(height, stride_y, stride_u, stride_v),
+		  I420DataSize(height, stride_y, stride_u, stride_v),
           kBufferAlignment))) {
   RTC_DCHECK_GT(width, 0);
   RTC_DCHECK_GT(height, 0);
diff --git a/webrtc/api/video/video_frame.h b/webrtc/api/video/video_frame.h
index 8840782ca..d990a25e0 100644
--- a/webrtc/api/video/video_frame.h
+++ b/webrtc/api/video/video_frame.h
@@ -13,6 +13,7 @@
 
 #include <stdint.h>
 
+#include <d3d11.h>
 #include "webrtc/api/video/video_rotation.h"
 #include "webrtc/api/video/video_frame_buffer.h"
 
@@ -106,6 +107,16 @@ class VideoFrame {
     return video_frame_buffer()->native_handle() != nullptr;
   }
 
+  void SetID3D11Texture2D(ID3D11Texture2D* texture)
+  {
+	  m_stagingFrameBuffer = texture;
+  }
+
+  ID3D11Texture2D* GetID3D11Texture2D() const
+  {
+	  return m_stagingFrameBuffer;
+  }
+
  private:
   // An opaque reference counted handle that stores the pixel data.
   rtc::scoped_refptr<webrtc::VideoFrameBuffer> video_frame_buffer_;
@@ -113,6 +124,7 @@ class VideoFrame {
   int64_t ntp_time_ms_;
   int64_t timestamp_us_;
   VideoRotation rotation_;
+  ID3D11Texture2D* m_stagingFrameBuffer;
 };
 
 }  // namespace webrtc
diff --git a/webrtc/base/BUILD.gn b/webrtc/base/BUILD.gn
index eb9361bc4..89b2d4bf2 100644
--- a/webrtc/base/BUILD.gn
+++ b/webrtc/base/BUILD.gn
@@ -647,7 +647,7 @@ rtc_source_set("gtest_prod") {
   ]
 }
 
-if (rtc_include_tests) {
+if (true) {
   config("rtc_base_tests_utils_exported_config") {
     defines = [ "GTEST_RELATIVE_PATH" ]
   }
diff --git a/webrtc/media/base/mediaconstants.cc b/webrtc/media/base/mediaconstants.cc
index 0d8512b9f..76dcb1ca7 100644
--- a/webrtc/media/base/mediaconstants.cc
+++ b/webrtc/media/base/mediaconstants.cc
@@ -108,6 +108,7 @@ const char kH264FmtpLevelAsymmetryAllowed[] = "level-asymmetry-allowed";
 const char kH264FmtpPacketizationMode[] = "packetization-mode";
 const char kH264FmtpSpropParameterSets[] = "sprop-parameter-sets";
 const char kH264ProfileLevelConstrainedBaseline[] = "42e01f";
+const char kH264UseHWNvencode[] = "use-hw-nvencode";
 
 const int kDefaultVideoMaxFramerate = 60;
 }  // namespace cricket
diff --git a/webrtc/media/base/mediaconstants.h b/webrtc/media/base/mediaconstants.h
index 44d8c7ee0..b5921982b 100644
--- a/webrtc/media/base/mediaconstants.h
+++ b/webrtc/media/base/mediaconstants.h
@@ -130,6 +130,7 @@ extern const char kH264FmtpLevelAsymmetryAllowed[];
 extern const char kH264FmtpPacketizationMode[];
 extern const char kH264FmtpSpropParameterSets[];
 extern const char kH264ProfileLevelConstrainedBaseline[];
+extern const char kH264UseHWNvencode[];
 
 extern const int kDefaultVideoMaxFramerate;
 }  // namespace cricket
diff --git a/webrtc/media/engine/webrtcvideoengine2.cc b/webrtc/media/engine/webrtcvideoengine2.cc
index 36394725a..8e2c1649f 100644
--- a/webrtc/media/engine/webrtcvideoengine2.cc
+++ b/webrtc/media/engine/webrtcvideoengine2.cc
@@ -575,7 +575,16 @@ static void AppendVideoCodecs(const std::vector<VideoCodec>& input_codecs,
     if (FindMatchingCodec(*unified_codecs, codec))
       continue;
 
-    unified_codecs->push_back(codec);
+	if (CodecNamesEq(codec.name, kH264CodecName))
+	{
+		auto it = unified_codecs->begin();
+		unified_codecs->insert(it, codec);
+	}
+	else
+	{
+		unified_codecs->push_back(codec);
+	}
+
 
     // Add associated RTX codec for recognized codecs.
     // TODO(deadbeef): Should we add RTX codecs for external codecs whose names
diff --git a/webrtc/modules/video_coding/BUILD.gn b/webrtc/modules/video_coding/BUILD.gn
index 643260a94..cc193d9ce 100644
--- a/webrtc/modules/video_coding/BUILD.gn
+++ b/webrtc/modules/video_coding/BUILD.gn
@@ -171,6 +171,9 @@ rtc_static_library("webrtc_h264") {
       "codecs/h264/h264_decoder_impl.h",
       "codecs/h264/h264_encoder_impl.cc",
       "codecs/h264/h264_encoder_impl.h",
+      "codecs/h264/include/NvHWEncoder.h",
+      "codecs/h264/include/nvEncodeAPI.h",
+      "codecs/h264/NvHWEncoder.cc"
     ]
     deps += [
       "../../common_video",
@@ -271,7 +274,7 @@ rtc_static_library("webrtc_vp9") {
   }
 }
 
-if (rtc_include_tests) {
+if (true) {
   rtc_executable("video_quality_measurement") {
     testonly = true
 
diff --git a/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.cc b/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.cc
index 84bfafb8b..0bbbf9cfa 100644
--- a/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.cc
+++ b/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.cc
@@ -10,6 +10,9 @@
  */
 
 #include "webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.h"
+#include "webrtc/modules/video_coding/codecs/h264/h264_decoder_impl.h"
+#include "webrtc/common_video/h264/h264_common.h"
+#include "webrtc/base/win32socketserver.h"
 
 #include <limits>
 #include <string>
@@ -25,12 +28,19 @@
 #include "webrtc/media/base/mediaconstants.h"
 #include "webrtc/system_wrappers/include/metrics.h"
 
+#include <memory>
+#include <utility>
+#include <vector>
+#include <iostream>
+#include <fstream>
+#include "webrtc/base/thread.h"
+#include "webrtc/base/bind.h"
+#include "webrtc/base/asyncinvoker.h"
+
 namespace webrtc {
 
 namespace {
 
-const bool kOpenH264EncoderDetailedLogging = false;
-
 // Used by histograms. Values of entries should not be changed.
 enum H264EncoderImplEvent {
   kH264EncoderEventInit = 0,
@@ -38,6 +48,8 @@ enum H264EncoderImplEvent {
   kH264EncoderEventMax = 16,
 };
 
+const bool kOpenH264EncoderDetailedLogging = false;
+
 int NumberOfThreads(int width, int height, int number_of_cores) {
   // TODO(hbos): In Chromium, multiple threads do not work with sandbox on Mac,
   // see crbug.com/583348. Until further investigated, only use one thread.
@@ -70,7 +82,6 @@ FrameType ConvertToVideoFrameType(EVideoFrameType type) {
   RTC_NOTREACHED() << "Unexpected/invalid frame type: " << type;
   return kEmptyFrame;
 }
-
 }  // namespace
 
 // Helper method used by H264EncoderImpl::Encode.
@@ -152,29 +163,43 @@ static void RtpFragmentize(EncodedImage* encoded_image,
   }
 }
 
+ID3D11Device * webrtc::H264EncoderImpl::m_d3dDevice = nullptr;
+ID3D11DeviceContext * webrtc::H264EncoderImpl::m_d3dContext = nullptr;
+
 H264EncoderImpl::H264EncoderImpl(const cricket::VideoCodec& codec)
-    : openh264_encoder_(nullptr),
-      width_(0),
-      height_(0),
-      max_frame_rate_(0.0f),
-      target_bps_(0),
-      max_bps_(0),
-      mode_(kRealtimeVideo),
-      frame_dropping_on_(false),
-      key_frame_interval_(0),
-      packetization_mode_(H264PacketizationMode::SingleNalUnit),
-      max_payload_size_(0),
-      number_of_cores_(0),
-      encoded_image_callback_(nullptr),
-      has_reported_init_(false),
-      has_reported_error_(false) {
-  RTC_CHECK(cricket::CodecNamesEq(codec.name, cricket::kH264CodecName));
-  std::string packetization_mode_string;
-  if (codec.GetParam(cricket::kH264FmtpPacketizationMode,
-                     &packetization_mode_string) &&
-      packetization_mode_string == "1") {
-    packetization_mode_ = H264PacketizationMode::NonInterleaved;
-  }
+	:
+	encoder_(nullptr),
+	number_of_cores_(0),
+	width_(0),
+	height_(0),
+	max_frame_rate_(0.0f),
+	target_bps_(0),
+	max_bps_(0),
+	mode_(kRealtimeVideo),
+	frame_dropping_on_(false),
+	m_use_software_encoding(true),
+	m_first_frame_sent(false),
+	m_pNvHWEncoder(NULL),
+	key_frame_interval_(0),
+	packetization_mode_(H264PacketizationMode::SingleNalUnit),
+	max_payload_size_(0),
+	encoded_image_callback_(nullptr),
+	has_reported_init_(false),
+	has_reported_error_(false) {
+	RTC_CHECK(cricket::CodecNamesEq(codec.name, cricket::kH264CodecName));
+	std::string packetization_mode_string;
+	if (codec.GetParam(cricket::kH264FmtpPacketizationMode,
+		&packetization_mode_string) &&
+		packetization_mode_string == "1") {
+		packetization_mode_ = H264PacketizationMode::NonInterleaved;
+	}
+
+	int useNvencode;
+	if (codec.GetParam(cricket::kH264UseHWNvencode,
+		&useNvencode) &&
+		useNvencode == 1) {
+		m_use_software_encoding = false;
+	}
 }
 
 H264EncoderImpl::~H264EncoderImpl() {
@@ -199,59 +224,106 @@ int32_t H264EncoderImpl::InitEncode(const VideoCodec* codec_settings,
     return WEBRTC_VIDEO_CODEC_ERR_PARAMETER;
   }
 
+  Json::Reader reader;
+  Json::Value root = NULL;
+  auto encoderConfigPath = ExePath("nvEncConfig.json");
+  std::ifstream file(encoderConfigPath);
+  if (file.good())
+  {
+	  file >> root;
+	  reader.parse(file, root, true);
+
+	  if (root.isMember("useSoftwareEncoding")) {
+		  m_use_software_encoding = root.get("useSoftwareEncoding", false).asBool();
+	  }
+  }
+
   int32_t release_ret = Release();
   if (release_ret != WEBRTC_VIDEO_CODEC_OK) {
     ReportError();
     return release_ret;
   }
-  RTC_DCHECK(!openh264_encoder_);
-
-  // Create encoder.
-  if (WelsCreateSVCEncoder(&openh264_encoder_) != 0) {
-    // Failed to create encoder.
-    LOG(LS_ERROR) << "Failed to create OpenH264 encoder";
-    RTC_DCHECK(!openh264_encoder_);
-    ReportError();
-    return WEBRTC_VIDEO_CODEC_ERROR;
-  }
-  RTC_DCHECK(openh264_encoder_);
-  if (kOpenH264EncoderDetailedLogging) {
-    int trace_level = WELS_LOG_DETAIL;
-    openh264_encoder_->SetOption(ENCODER_OPTION_TRACE_LEVEL,
-                                 &trace_level);
-  }
-  // else WELS_LOG_DEFAULT is used by default.
-
-  number_of_cores_ = number_of_cores;
-  // Set internal settings from codec_settings
-  width_ = codec_settings->width;
-  height_ = codec_settings->height;
-  max_frame_rate_ = static_cast<float>(codec_settings->maxFramerate);
-  mode_ = codec_settings->mode;
-  frame_dropping_on_ = codec_settings->H264().frameDroppingOn;
-  key_frame_interval_ = codec_settings->H264().keyFrameInterval;
-  max_payload_size_ = max_payload_size;
-
-  // Codec_settings uses kbits/second; encoder uses bits/second.
-  max_bps_ = codec_settings->maxBitrate * 1000;
-  if (codec_settings->targetBitrate == 0)
-    target_bps_ = codec_settings->startBitrate * 1000;
-  else
-    target_bps_ = codec_settings->targetBitrate * 1000;
-
-  SEncParamExt encoder_params = CreateEncoderParams();
-
-  // Initialize.
-  if (openh264_encoder_->InitializeExt(&encoder_params) != 0) {
-    LOG(LS_ERROR) << "Failed to initialize OpenH264 encoder";
-    Release();
-    ReportError();
-    return WEBRTC_VIDEO_CODEC_ERROR;
-  }
-  // TODO(pbos): Base init params on these values before submitting.
-  int video_format = EVideoFormatType::videoFormatI420;
-  openh264_encoder_->SetOption(ENCODER_OPTION_DATAFORMAT,
-                               &video_format);
+	RTC_DCHECK(!encoder_);
+
+	if (m_use_software_encoding)
+	{
+		//codec_settings
+		// Create encoder.
+		if (WelsCreateSVCEncoder(&encoder_) != 0) {
+			// Failed to create encoder.
+			LOG(LS_ERROR) << "Failed to create OpenH264 encoder";
+			RTC_DCHECK(!encoder_);
+			ReportError();
+			return WEBRTC_VIDEO_CODEC_ERROR;
+		}
+		RTC_DCHECK(encoder_);
+		if (kOpenH264EncoderDetailedLogging) {
+			int trace_level = WELS_LOG_DETAIL;
+			encoder_->SetOption(ENCODER_OPTION_TRACE_LEVEL,
+				&trace_level);
+		}
+		// else WELS_LOG_DEFAULT is used by default.
+		
+		number_of_cores_ = number_of_cores;
+		// Set internal settings from codec_settings
+		width_ = codec_settings->width;
+		height_ = codec_settings->height;
+		max_frame_rate_ = static_cast<float>(codec_settings->maxFramerate);
+		mode_ = codec_settings->mode;
+		frame_dropping_on_ = codec_settings->H264().frameDroppingOn;
+		key_frame_interval_ = codec_settings->H264().keyFrameInterval;
+		max_payload_size_ = max_payload_size;
+
+		// Codec_settings uses kbits/second; encoder uses bits/second.
+		max_bps_ = codec_settings->maxBitrate * 1000;
+		if (codec_settings->targetBitrate == 0)
+			target_bps_ = codec_settings->startBitrate * 1000;
+		else
+			target_bps_ = codec_settings->targetBitrate * 1000;
+
+		SEncParamExt encoder_params = CreateEncoderParams();
+
+		// Initialize.
+		if (encoder_->InitializeExt(&encoder_params) != 0) {
+			LOG(LS_ERROR) << "Failed to initialize OpenH264 encoder";
+			Release();
+			ReportError();
+			return WEBRTC_VIDEO_CODEC_ERROR;
+		}
+		// TODO(pbos): Base init params on these values before submitting.
+		int video_format = EVideoFormatType::videoFormatI420;
+		encoder_->SetOption(ENCODER_OPTION_DATAFORMAT,
+			&video_format);
+	}
+	else
+	{
+		packetization_mode_ = H264PacketizationMode::NonInterleaved;
+		m_first_frame_sent = false;
+
+		rtc::Win32Thread w32_thread;
+		rtc::ThreadManager::Instance()->SetCurrentThread(&w32_thread);
+
+		memset(&m_encodeConfig, 0, sizeof(EncodeConfig));
+
+		GetDefaultNvencodeConfig(m_encodeConfig, root);
+		m_encodeConfig.width = codec_settings->width;
+		m_encodeConfig.height = codec_settings->height;
+		m_pNvHWEncoder = new CNvHWEncoder();
+
+		m_pNvHWEncoder->Initialize((void*)m_d3dDevice, NV_ENC_DEVICE_TYPE_DIRECTX);
+		m_encodeConfig.presetGUID = m_pNvHWEncoder->GetPresetGUID(m_encodeConfig.encoderPreset, m_encodeConfig.codec);
+
+		m_pNvHWEncoder->m_stEncodeConfig.profileGUID = NV_ENC_PRESET_LOW_LATENCY_HQ_GUID;
+
+		//	//H264 level sets maximum bitrate limits.  4.1 supported by almost all mobile devices.
+		m_pNvHWEncoder->m_stEncodeConfig.encodeCodecConfig.h264Config.level = NV_ENC_LEVEL_H264_41;
+
+		// Creates the encoder.
+		m_pNvHWEncoder->CreateEncoder(&m_encodeConfig);
+		m_uEncodeBufferCount = 4;
+
+		AllocateIOBuffers(m_encodeConfig.width, m_encodeConfig.height);
+	}
 
   // Initialize encoded image. Default buffer size: size of unencoded data.
   encoded_image_._size =
@@ -266,14 +338,72 @@ int32_t H264EncoderImpl::InitEncode(const VideoCodec* codec_settings,
 }
 
 int32_t H264EncoderImpl::Release() {
-  if (openh264_encoder_) {
-    RTC_CHECK_EQ(0, openh264_encoder_->Uninitialize());
-    WelsDestroySVCEncoder(openh264_encoder_);
-    openh264_encoder_ = nullptr;
-  }
-  encoded_image_._buffer = nullptr;
-  encoded_image_buffer_.reset();
-  return WEBRTC_VIDEO_CODEC_OK;
+	if (encoder_) {
+		RTC_CHECK_EQ(0, encoder_->Uninitialize());
+		WelsDestroySVCEncoder(encoder_);
+		encoder_ = nullptr;
+	}
+
+	if (m_pNvHWEncoder)
+	{
+		Deinitialize();
+
+		if (m_pNvHWEncoder)
+		{
+			delete m_pNvHWEncoder;
+			m_pNvHWEncoder = NULL;
+		}
+	}
+
+	encoded_image_._buffer = nullptr;
+	encoded_image_buffer_.reset();
+	return WEBRTC_VIDEO_CODEC_OK;
+}
+
+// Cleanup resources.
+NVENCSTATUS H264EncoderImpl::Deinitialize()
+{
+	NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+
+	if (!m_pNvHWEncoder)
+		return nvStatus;
+
+	FlushEncoder();
+	ReleaseIOBuffers();
+	nvStatus = m_pNvHWEncoder->NvEncDestroyEncoder();
+	return nvStatus;
+}
+
+NVENCSTATUS H264EncoderImpl::FlushEncoder()
+{
+	NVENCSTATUS nvStatus = m_pNvHWEncoder->NvEncFlushEncoderQueue(m_stEOSOutputBfr.hOutputEvent);
+	if (nvStatus != NV_ENC_SUCCESS)
+	{
+		assert(0);
+		return nvStatus;
+	}
+
+	EncodeBuffer *pEncodeBuffer = m_EncodeBufferQueue.GetPending();
+	while (pEncodeBuffer)
+	{
+		m_pNvHWEncoder->ProcessOutput(pEncodeBuffer);
+		pEncodeBuffer = m_EncodeBufferQueue.GetPending();
+
+		// UnMap the input buffer after frame is done.
+		if (pEncodeBuffer && pEncodeBuffer->stInputBfr.hInputSurface)
+		{
+			nvStatus = m_pNvHWEncoder->NvEncUnmapInputResource(pEncodeBuffer->stInputBfr.hInputSurface);
+			pEncodeBuffer->stInputBfr.hInputSurface = NULL;
+		}
+	}
+
+	if (WaitForSingleObject(m_stEOSOutputBfr.hOutputEvent, 500) != WAIT_OBJECT_0)
+	{
+		assert(0);
+		nvStatus = NV_ENC_ERR_GENERIC;
+	}
+
+	return nvStatus;
 }
 
 int32_t H264EncoderImpl::RegisterEncodeCompleteCallback(
@@ -291,105 +421,486 @@ int32_t H264EncoderImpl::SetRateAllocation(
   target_bps_ = bitrate_allocation.get_sum_bps();
   max_frame_rate_ = static_cast<float>(framerate);
 
-  SBitrateInfo target_bitrate;
-  memset(&target_bitrate, 0, sizeof(SBitrateInfo));
-  target_bitrate.iLayer = SPATIAL_LAYER_ALL,
-  target_bitrate.iBitrate = target_bps_;
-  openh264_encoder_->SetOption(ENCODER_OPTION_BITRATE,
-                               &target_bitrate);
-  openh264_encoder_->SetOption(ENCODER_OPTION_FRAME_RATE, &max_frame_rate_);
-  return WEBRTC_VIDEO_CODEC_OK;
-}
+  if (m_use_software_encoding)
+  {
+	  SBitrateInfo target_bitrate;
+	  memset(&target_bitrate, 0, sizeof(SBitrateInfo));
+	  target_bitrate.iLayer = SPATIAL_LAYER_ALL,
+		  target_bitrate.iBitrate = target_bps_;
 
-int32_t H264EncoderImpl::Encode(const VideoFrame& input_frame,
-                                const CodecSpecificInfo* codec_specific_info,
-                                const std::vector<FrameType>* frame_types) {
-  if (!IsInitialized()) {
-    ReportError();
-    return WEBRTC_VIDEO_CODEC_UNINITIALIZED;
+	  encoder_->SetOption(ENCODER_OPTION_BITRATE,
+		  &target_bitrate);
+	  encoder_->SetOption(ENCODER_OPTION_FRAME_RATE, &max_frame_rate_);
   }
-  if (!encoded_image_callback_) {
-    LOG(LS_WARNING) << "InitEncode() has been called, but a callback function "
-                    << "has not been set with RegisterEncodeCompleteCallback()";
-    ReportError();
-    return WEBRTC_VIDEO_CODEC_UNINITIALIZED;
+  else
+  {
+	  m_encodeConfig.fps = max_frame_rate_;
+	  m_encodeConfig.bitrate = target_bps_;
+
+	  if (m_pNvHWEncoder != nullptr && m_encodeConfig.minBitrate < (int)target_bps_)
+	  {
+		  NvEncPictureCommand pEncPicCommand;
+		  pEncPicCommand.bBitrateChangePending = true;
+		  pEncPicCommand.bForceIDR = false;
+		  pEncPicCommand.bResolutionChangePending = false;
+		  pEncPicCommand.bInvalidateRefFrames = false;
+		  pEncPicCommand.bForceIntraRefresh = false;
+
+		  pEncPicCommand.newBitrate = m_encodeConfig.bitrate;
+		  pEncPicCommand.intraRefreshDuration = m_encodeConfig.fps;
+		  pEncPicCommand.newVBVSize = 0;
+
+		  m_pNvHWEncoder->NvEncReconfigureEncoder(&pEncPicCommand);
+	  }
   }
+  return WEBRTC_VIDEO_CODEC_OK;
+}
 
-  bool force_key_frame = false;
-  if (frame_types != nullptr) {
-    // We only support a single stream.
-    RTC_DCHECK_EQ(frame_types->size(), 1);
-    // Skip frame?
-    if ((*frame_types)[0] == kEmptyFrame) {
-      return WEBRTC_VIDEO_CODEC_OK;
-    }
-    // Force key frame?
-    force_key_frame = (*frame_types)[0] == kVideoFrameKey;
-  }
-  if (force_key_frame) {
-    // API doc says ForceIntraFrame(false) does nothing, but calling this
-    // function forces a key frame regardless of the |bIDR| argument's value.
-    // (If every frame is a key frame we get lag/delays.)
-    openh264_encoder_->ForceIntraFrame(true);
-  }
-  rtc::scoped_refptr<const VideoFrameBuffer> frame_buffer =
-      input_frame.video_frame_buffer();
-  // EncodeFrame input.
-  SSourcePicture picture;
-  memset(&picture, 0, sizeof(SSourcePicture));
-  picture.iPicWidth = frame_buffer->width();
-  picture.iPicHeight = frame_buffer->height();
-  picture.iColorFormat = EVideoFormatType::videoFormatI420;
-  picture.uiTimeStamp = input_frame.ntp_time_ms();
-  picture.iStride[0] = frame_buffer->StrideY();
-  picture.iStride[1] = frame_buffer->StrideU();
-  picture.iStride[2] = frame_buffer->StrideV();
-  picture.pData[0] = const_cast<uint8_t*>(frame_buffer->DataY());
-  picture.pData[1] = const_cast<uint8_t*>(frame_buffer->DataU());
-  picture.pData[2] = const_cast<uint8_t*>(frame_buffer->DataV());
-
-  // EncodeFrame output.
-  SFrameBSInfo info;
-  memset(&info, 0, sizeof(SFrameBSInfo));
-
-  // Encode!
-  int enc_ret = openh264_encoder_->EncodeFrame(&picture, &info);
-  if (enc_ret != 0) {
-    LOG(LS_ERROR) << "OpenH264 frame encoding failed, EncodeFrame returned "
-                  << enc_ret << ".";
-    ReportError();
-    return WEBRTC_VIDEO_CODEC_ERROR;
-  }
+NVENCSTATUS H264EncoderImpl::SetNvencodeProfile(int profileIndex)
+{
+	GUID choice;
+	switch (profileIndex)
+	{
+		//Main should be used for most cases
+	case 1:
+		choice = NV_ENC_H264_PROFILE_MAIN_GUID;
+		break;
+
+		//Low latency and HQ
+	case 2:
+		choice = NV_ENC_PRESET_LOW_LATENCY_HQ_GUID;
+		break;
+
+		//Enables stereo frame packing
+	case 3:
+		choice = NV_ENC_H264_PROFILE_STEREO_GUID;
+		break;
+
+		//Fallback to auto, avoid this.
+	case 0:
+	default: choice = NV_ENC_CODEC_PROFILE_AUTOSELECT_GUID;
+		break;
+	}
+	return NV_ENC_SUCCESS;
+}
 
-  encoded_image_._encodedWidth = frame_buffer->width();
-  encoded_image_._encodedHeight = frame_buffer->height();
-  encoded_image_._timeStamp = input_frame.timestamp();
-  encoded_image_.ntp_time_ms_ = input_frame.ntp_time_ms();
-  encoded_image_.capture_time_ms_ = input_frame.render_time_ms();
-  encoded_image_.rotation_ = input_frame.rotation();
-  encoded_image_._frameType = ConvertToVideoFrameType(info.eFrameType);
-
-  // Split encoded image up into fragments. This also updates |encoded_image_|.
-  RTPFragmentationHeader frag_header;
-  RtpFragmentize(&encoded_image_, &encoded_image_buffer_, *frame_buffer, &info,
-                 &frag_header);
-
-  // Encoder can skip frames to save bandwidth in which case
-  // |encoded_image_._length| == 0.
-  if (encoded_image_._length > 0) {
-    // Parse QP.
-    h264_bitstream_parser_.ParseBitstream(encoded_image_._buffer,
-                                          encoded_image_._length);
-    h264_bitstream_parser_.GetLastSliceQp(&encoded_image_.qp_);
-
-    // Deliver encoded image.
-    CodecSpecificInfo codec_specific;
-    codec_specific.codecType = kVideoCodecH264;
-    codec_specific.codecSpecific.H264.packetization_mode = packetization_mode_;
-    encoded_image_callback_->OnEncodedImage(encoded_image_, &codec_specific,
-                                            &frag_header);
-  }
-  return WEBRTC_VIDEO_CODEC_OK;
+void H264EncoderImpl::GetDefaultNvencodeConfig(EncodeConfig &nvEncodeConfig, Json::Value rootValue = NULL)
+{
+	//Populate with default values
+	{
+		nvEncodeConfig.startFrameIdx = 0;
+		nvEncodeConfig.bitrate = 3000000;
+		nvEncodeConfig.minBitrate = 3000000;
+		nvEncodeConfig.endFrameIdx = INT_MAX;
+		nvEncodeConfig.rcMode = NV_ENC_PARAMS_RC_CBR_LOWDELAY_HQ;
+		nvEncodeConfig.encoderPreset = "lowLatencyHQ";
+
+		//Infinite needed for low latency encoding.
+		nvEncodeConfig.gopLength = NVENC_INFINITE_GOPLENGTH;
+
+		//DeviceType 1 = Force CUDA.
+		nvEncodeConfig.deviceType = 0;
+
+		//Only supported codec for WebRTC.
+		nvEncodeConfig.codec = NV_ENC_H264;
+		nvEncodeConfig.fps = 60;
+
+		//Quantization Parameter - must be 0 for lossless.
+		nvEncodeConfig.qp = 5;
+
+		//Must be set to frame.
+		nvEncodeConfig.pictureStruct = NV_ENC_PIC_STRUCT_FRAME;
+
+		//Initial QP factors for bitrate spinup.
+		nvEncodeConfig.i_quant_factor = DEFAULT_I_QFACTOR;
+		nvEncodeConfig.b_quant_factor = DEFAULT_B_QFACTOR;
+		nvEncodeConfig.i_quant_offset = DEFAULT_I_QOFFSET;
+		nvEncodeConfig.b_quant_offset = DEFAULT_B_QOFFSET;
+		nvEncodeConfig.intraRefreshPeriod = 60;
+		nvEncodeConfig.intraRefreshEnableFlag = true;
+		nvEncodeConfig.intraRefreshDuration = 6;
+
+		// Enable temporal Adaptive Quantization
+		// Shifts quantization matrix based on complexity of frame over time
+		nvEncodeConfig.enableTemporalAQ = false;
+
+		//Need this to be able to recover from stream drops
+		//Client needs to send back a last good timestamp, and we call
+		//NvEncInvalidateRefFrames(encoder,timestamp) to reissue I frame
+		nvEncodeConfig.invalidateRefFramesEnableFlag = true;
+		//NV_ENC_PRESET_LOW_LATENCY_HP_GUID
+		SetNvencodeProfile(2);
+	}
+
+	if (rootValue != NULL && rootValue.isMember("NvencodeSettings"))
+	{
+		auto nvencodeRoot = rootValue.get("NvencodeSettings", NULL);
+		if (nvencodeRoot == NULL)
+			return;
+
+		if (nvencodeRoot.isMember("bitrate"))
+		{
+			nvEncodeConfig.bitrate = nvencodeRoot.get("bitrate", nvEncodeConfig.bitrate).asInt();
+		}
+
+		if (nvencodeRoot.isMember("minBitrate"))
+		{
+			nvEncodeConfig.minBitrate = nvencodeRoot.get("minBitrate", nvEncodeConfig.minBitrate).asInt();
+		}
+
+		if (nvencodeRoot.isMember("fps"))
+		{
+			nvEncodeConfig.fps = nvencodeRoot.get("fps", nvEncodeConfig.fps).asInt();
+		}
+
+		if (nvencodeRoot.isMember("qp"))
+		{
+			nvEncodeConfig.qp = nvencodeRoot.get("qp", nvEncodeConfig.qp).asInt();
+		}
+
+		if (nvencodeRoot.isMember("intraRefreshPeriod"))
+		{
+			nvEncodeConfig.intraRefreshPeriod = nvencodeRoot.get("intraRefreshPeriod", nvEncodeConfig.intraRefreshPeriod).asInt();
+		}
+
+		if (nvencodeRoot.isMember("intraRefreshEnableFlag"))
+		{
+			nvEncodeConfig.intraRefreshEnableFlag = nvencodeRoot.get("intraRefreshEnableFlag", nvEncodeConfig.intraRefreshEnableFlag).asBool();
+		}
+
+		if (nvencodeRoot.isMember("intraRefreshDuration"))
+		{
+			nvEncodeConfig.intraRefreshDuration = nvencodeRoot.get("intraRefreshDuration", nvEncodeConfig.intraRefreshDuration).asInt();
+		}
+
+		if (nvencodeRoot.isMember("enableTemporalAQ"))
+		{
+			nvEncodeConfig.enableTemporalAQ = nvencodeRoot.get("enableTemporalAQ", nvEncodeConfig.enableTemporalAQ).asBool();
+		}
+
+		if (nvencodeRoot.isMember("invalidateRefFramesEnableFlag"))
+		{
+			nvEncodeConfig.invalidateRefFramesEnableFlag = nvencodeRoot.get("invalidateRefFramesEnableFlag", nvEncodeConfig.invalidateRefFramesEnableFlag).asBool();
+		}
+
+		if (nvencodeRoot.isMember("nvEncodeProfile"))
+		{
+			auto profile = nvencodeRoot.get("nvEncodeProfile", 2).asInt();
+			SetNvencodeProfile(profile);
+		}
+	}
+}
+
+void H264EncoderImpl::Capture(ID3D11Texture2D* frameBuffer, bool forceIntra)
+{
+	if (!m_d3dContext || !m_pNvHWEncoder)
+		return;
+
+	// Try to process the pending input buffers.
+	NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+	EncodeBuffer* pEncodeBuffer = m_EncodeBufferQueue.GetAvailable();
+
+	if (!pEncodeBuffer)
+	{
+		pEncodeBuffer = m_EncodeBufferQueue.GetPending();
+		m_pNvHWEncoder->ProcessOutput(pEncodeBuffer);
+
+		// UnMap the input buffer after frame done
+		if (pEncodeBuffer->stInputBfr.hInputSurface)
+		{
+			nvStatus = m_pNvHWEncoder->NvEncUnmapInputResource(pEncodeBuffer->stInputBfr.hInputSurface);
+			pEncodeBuffer->stInputBfr.hInputSurface = NULL;
+		}
+
+		pEncodeBuffer = m_EncodeBufferQueue.GetAvailable();
+	}
+
+	// Copies the frame buffer to the encode input buffer.
+	m_d3dContext->CopyResource(pEncodeBuffer->stInputBfr.pARGBSurface, frameBuffer);
+	frameBuffer->Release();
+	nvStatus = m_pNvHWEncoder->NvEncMapInputResource(pEncodeBuffer->stInputBfr.nvRegisteredResource, &pEncodeBuffer->stInputBfr.hInputSurface);
+	if (nvStatus != NV_ENC_SUCCESS)
+	{
+		PRINTERR("Failed to Map input buffer %p\n", pEncodeBuffer->stInputBfr.hInputSurface);
+		return;
+	}
+
+	NvEncPictureCommand pEncPicCommand;
+	pEncPicCommand.bForceIntraRefresh = forceIntra;
+	pEncPicCommand.bForceIDR = forceIntra;
+	pEncPicCommand.intraRefreshDuration = m_encodeConfig.intraRefreshDuration;
+
+	nvStatus = m_pNvHWEncoder->NvEncEncodeFrame(pEncodeBuffer, forceIntra ? &pEncPicCommand : nullptr, m_encodeConfig.width, m_encodeConfig.height);
+	if (nvStatus != NV_ENC_SUCCESS  && nvStatus != NV_ENC_ERR_NEED_MORE_INPUT)
+	{
+		return;
+	}
+}
+
+NVENCSTATUS H264EncoderImpl::AllocateIOBuffers(uint32_t uInputWidth, uint32_t uInputHeight)
+{
+	ID3D11Texture2D* pVPSurfaces[16];
+
+	// Initializes the encode buffer queue.
+	m_EncodeBufferQueue.Initialize(m_stEncodeBuffer, m_uEncodeBufferCount);
+
+	// Finds the suitable format for buffer.
+	DXGI_FORMAT format = DXGI_FORMAT_R8G8B8A8_UNORM;
+
+	for (uint32_t i = 0; i < m_uEncodeBufferCount; i++)
+	{
+		// Initializes the input buffer, backed by ID3D11Texture2D*.
+		D3D11_TEXTURE2D_DESC desc = { 0 };
+		desc.ArraySize = 1;
+		desc.Format = format;
+		desc.Width = uInputWidth;
+		desc.Height = uInputHeight;
+		desc.MipLevels = 1;
+		desc.SampleDesc.Count = 1;
+		desc.Usage = D3D11_USAGE_DEFAULT;
+		m_d3dDevice->CreateTexture2D(&desc, nullptr, &pVPSurfaces[i]);
+
+		// Registers the input buffer with NvEnc.
+		m_pNvHWEncoder->NvEncRegisterResource(
+			NV_ENC_INPUT_RESOURCE_TYPE_DIRECTX,
+			(void*)pVPSurfaces[i],
+			uInputWidth,
+			uInputHeight,
+			m_stEncodeBuffer[i].stInputBfr.uARGBStride,
+			&m_stEncodeBuffer[i].stInputBfr.nvRegisteredResource);
+
+		// Maps the buffer format to the relevant NvEnc encoder format
+		switch (format)
+		{
+		case DXGI_FORMAT_B8G8R8A8_UNORM:
+			m_stEncodeBuffer[i].stInputBfr.bufferFmt = NV_ENC_BUFFER_FORMAT_ARGB;
+			break;
+
+		case DXGI_FORMAT_R10G10B10A2_UNORM:
+			if (m_pNvHWEncoder->m_stEncodeConfig.encodeCodecConfig.h264Config.qpPrimeYZeroTransformBypassFlag == 1)
+				m_stEncodeBuffer[i].stInputBfr.bufferFmt = NV_ENC_BUFFER_FORMAT_YUV444_10BIT;
+			else
+				m_stEncodeBuffer[i].stInputBfr.bufferFmt = NV_ENC_BUFFER_FORMAT_YUV420_10BIT;
+			break;
+
+		default:
+			m_stEncodeBuffer[i].stInputBfr.bufferFmt = NV_ENC_BUFFER_FORMAT_ABGR;
+			break;
+		}
+
+		m_stEncodeBuffer[i].stInputBfr.dwWidth = uInputWidth;
+		m_stEncodeBuffer[i].stInputBfr.dwHeight = uInputHeight;
+		m_stEncodeBuffer[i].stInputBfr.pARGBSurface = pVPSurfaces[i];
+
+		// Initializes the output buffer.
+		m_pNvHWEncoder->NvEncCreateBitstreamBuffer(2 * 1024 * 1024, &m_stEncodeBuffer[i].stOutputBfr.hBitstreamBuffer);
+		m_stEncodeBuffer[i].stOutputBfr.dwBitstreamBufferSize = 2 * 1024 * 1024;
+
+		// Registers for the output event.
+		m_pNvHWEncoder->NvEncRegisterAsyncEvent(&m_stEncodeBuffer[i].stOutputBfr.hOutputEvent);
+		m_stEncodeBuffer[i].stOutputBfr.bWaitOnEvent = true;
+	}
+
+	m_stEOSOutputBfr.bEOSFlag = TRUE;
+
+	// Registers for the output event.
+	m_pNvHWEncoder->NvEncRegisterAsyncEvent(&m_stEOSOutputBfr.hOutputEvent);
+
+	return NV_ENC_SUCCESS;
+}
+
+NVENCSTATUS H264EncoderImpl::ReleaseIOBuffers()
+{
+	for (uint32_t i = 0; i < m_uEncodeBufferCount; i++)
+	{
+		if (m_stEncodeBuffer[i].stInputBfr.pARGBSurface)
+		{
+			m_stEncodeBuffer[i].stInputBfr.pARGBSurface->Release();
+			m_stEncodeBuffer[i].stInputBfr.pARGBSurface = nullptr;
+		}
+
+		m_pNvHWEncoder->NvEncDestroyBitstreamBuffer(m_stEncodeBuffer[i].stOutputBfr.hBitstreamBuffer);
+		m_stEncodeBuffer[i].stOutputBfr.hBitstreamBuffer = NULL;
+
+		m_pNvHWEncoder->NvEncUnregisterAsyncEvent(m_stEncodeBuffer[i].stOutputBfr.hOutputEvent);
+		CloseHandle(m_stEncodeBuffer[i].stOutputBfr.hOutputEvent);
+		m_stEncodeBuffer[i].stOutputBfr.hOutputEvent = NULL;
+	}
+
+	if (m_stEOSOutputBfr.hOutputEvent)
+	{
+		m_pNvHWEncoder->NvEncUnregisterAsyncEvent(m_stEOSOutputBfr.hOutputEvent);
+		CloseHandle(m_stEOSOutputBfr.hOutputEvent);
+		m_stEOSOutputBfr.hOutputEvent = NULL;
+	}
+
+	return NV_ENC_SUCCESS;
+}
+
+// Captures encoded frames from NvEncoder.
+void H264EncoderImpl::GetEncodedFrame(void** buffer, int* size, _NV_ENC_PIC_TYPE* keyFrameType)
+{
+	*buffer = m_pNvHWEncoder->m_lockBitstreamData.bitstreamBufferPtr;
+	*size = m_pNvHWEncoder->m_lockBitstreamData.bitstreamSizeInBytes;
+	*keyFrameType = m_pNvHWEncoder->m_lockBitstreamData.pictureType;
+}
+
+int32_t H264EncoderImpl::Encode(const VideoFrame& input_frame,
+	const CodecSpecificInfo* codec_specific_info,
+	const std::vector<FrameType>* frame_types) {
+
+	rtc::scoped_refptr<const VideoFrameBuffer> frame_buffer = input_frame.video_frame_buffer();
+	SFrameBSInfo info;
+	RTPFragmentationHeader frag_header;
+
+	if (m_use_software_encoding && !IsInitialized())
+	{
+		ReportError();
+		return WEBRTC_VIDEO_CODEC_UNINITIALIZED;
+	}
+
+	if (!encoded_image_callback_) {
+		LOG(LS_WARNING) << "InitEncode() has been called, but a callback function "
+			<< "has not been set with RegisterEncodeCompleteCallback()";
+		ReportError();
+		return WEBRTC_VIDEO_CODEC_UNINITIALIZED;
+	}
+
+	bool force_key_frame = false;
+	if (frame_types != nullptr) {
+		// We only support a single stream.
+		RTC_DCHECK_EQ(frame_types->size(), 1);
+		// Skip frame?
+		if ((*frame_types)[0] == kEmptyFrame) {
+			return WEBRTC_VIDEO_CODEC_OK;
+		}
+		// Force key frame?
+		force_key_frame = (*frame_types)[0] == kVideoFrameKey;
+	}
+	if (force_key_frame) {
+		// API doc says ForceIntraFrame(false) does nothing, but calling this
+		// function forces a key frame regardless of the |bIDR| argument's value.
+		// (If every frame is a key frame we get lag/delays.)
+		if (m_use_software_encoding)
+		{
+			encoder_->ForceIntraFrame(true);
+		}
+	}
+
+	if (m_use_software_encoding)
+	{
+		// EncodeFrame input.
+		SSourcePicture picture;
+		memset(&picture, 0, sizeof(SSourcePicture));
+		picture.iPicWidth = frame_buffer->width();
+		picture.iPicHeight = frame_buffer->height();
+		picture.iColorFormat = EVideoFormatType::videoFormatI420;
+		picture.uiTimeStamp = input_frame.ntp_time_ms();
+		picture.iStride[0] = frame_buffer->StrideY();
+		picture.iStride[1] = frame_buffer->StrideU();
+		picture.iStride[2] = frame_buffer->StrideV();
+		picture.pData[0] = const_cast<uint8_t*>(frame_buffer->DataY());
+		picture.pData[1] = const_cast<uint8_t*>(frame_buffer->DataU());
+		picture.pData[2] = const_cast<uint8_t*>(frame_buffer->DataV());
+
+		// EncodeFrame output.
+		memset(&info, 0, sizeof(SFrameBSInfo));
+
+		// Encode!
+		int enc_ret = encoder_->EncodeFrame(&picture, &info);
+		if (enc_ret != 0) {
+			LOG(LS_ERROR) << "OpenH264 frame encoding failed, EncodeFrame returned "
+				<< enc_ret << ".";
+			ReportError();
+			return WEBRTC_VIDEO_CODEC_ERROR;
+		}
+
+		encoded_image_._frameType = ConvertToVideoFrameType(info.eFrameType);
+	}
+	else
+	{
+		void* pFrameBuffer = nullptr;
+		int frameSizeInBytes = 0;
+		_NV_ENC_PIC_TYPE frameType;
+		auto texture = input_frame.GetID3D11Texture2D();
+		if (texture == nullptr)
+			return WEBRTC_VIDEO_CODEC_OK;
+
+		// Force a key frame until we send the first one.
+		if (!m_first_frame_sent) force_key_frame = true;
+
+		size_t i_nal = 0;
+		Capture(texture, force_key_frame);
+		GetEncodedFrame(&pFrameBuffer, &frameSizeInBytes, &frameType);
+		if (frameSizeInBytes < 1 || frameSizeInBytes >= 100000000 || frameType == NV_ENC_PIC_TYPE_SKIPPED || frameType == NV_ENC_PIC_TYPE_UNKNOWN)
+			return WEBRTC_VIDEO_CODEC_OK;
+
+		if (!m_first_frame_sent) m_first_frame_sent = true;
+
+		auto p_nal = (uint8_t*)pFrameBuffer;
+		std::vector<H264::NaluIndex> NALUidx;
+
+		NALUidx = H264::FindNaluIndices(p_nal, frameSizeInBytes);
+		if (NALUidx.size() < 1)
+			return WEBRTC_VIDEO_CODEC_OK;
+
+		i_nal = NALUidx.size();
+		if (i_nal == 1)
+		{
+			NALUidx[0].payload_size = frameSizeInBytes - NALUidx[0].payload_start_offset;
+		}
+		else for (size_t i = 0; i < i_nal; i++)
+		{
+			NALUidx[i].payload_size = i + 1 >= i_nal ? frameSizeInBytes - NALUidx[i].payload_start_offset : NALUidx[i + 1].start_offset - NALUidx[i].payload_start_offset;
+		}
+
+		frag_header.VerifyAndAllocateFragmentationHeader(i_nal);
+		encoded_image_.qp_ = m_encodeConfig.qp;
+		encoded_image_._frameType = frameType == NV_ENC_PIC_TYPE_IDR ? kVideoFrameKey : kVideoFrameDelta;
+
+		uint32_t totalNaluIndex = 0;
+		for (size_t nal_index = 0; nal_index < i_nal; nal_index++)
+		{
+			size_t currentNaluSize = 0;
+			currentNaluSize = NALUidx[nal_index].payload_size; //i_frame_size
+
+			frag_header.fragmentationOffset[totalNaluIndex] = NALUidx[nal_index].payload_start_offset;
+			frag_header.fragmentationLength[totalNaluIndex] = currentNaluSize;
+			frag_header.fragmentationPlType[totalNaluIndex] = H264::ParseNaluType(p_nal[NALUidx[nal_index].payload_start_offset]);
+			frag_header.fragmentationTimeDiff[totalNaluIndex] = 0;
+			totalNaluIndex++;
+		}
+
+		memcpy(encoded_image_._buffer, p_nal, frameSizeInBytes);
+		encoded_image_._length = frameSizeInBytes;
+	}
+
+	encoded_image_._encodedWidth = frame_buffer->width();
+	encoded_image_._encodedHeight = frame_buffer->height();
+	encoded_image_._timeStamp = input_frame.timestamp();
+	encoded_image_.ntp_time_ms_ = input_frame.ntp_time_ms();
+	encoded_image_.capture_time_ms_ = input_frame.render_time_ms();
+	encoded_image_.rotation_ = input_frame.rotation();
+
+	// Split encoded image up into fragments. This also updates |encoded_image_|.
+	if (m_use_software_encoding)
+	{
+		RtpFragmentize(&encoded_image_, &encoded_image_buffer_, *frame_buffer, &info,
+			&frag_header);
+	}
+
+	// Encoder can skip frames to save bandwidth in which case
+	// |encoded_image_._length| == 0.
+	if (encoded_image_._length > 0) {
+
+		// Deliver encoded image.
+		CodecSpecificInfo codec_specific;
+		codec_specific.codecType = kVideoCodecH264;
+		codec_specific.codecSpecific.H264.packetization_mode = H264PacketizationMode::NonInterleaved;
+		encoded_image_callback_->OnEncodedImage(encoded_image_, &codec_specific,
+			&frag_header);
+	}
+	return WEBRTC_VIDEO_CODEC_OK;
 }
 
 const char* H264EncoderImpl::ImplementationName() const {
@@ -397,7 +908,7 @@ const char* H264EncoderImpl::ImplementationName() const {
 }
 
 bool H264EncoderImpl::IsInitialized() const {
-  return openh264_encoder_ != nullptr;
+	return encoder_ != nullptr;
 }
 
 // Initialization parameters.
@@ -406,9 +917,9 @@ bool H264EncoderImpl::IsInitialized() const {
 // which is a superset of SEncParamBase (cleared with GetDefaultParams) used
 // in InitializeExt.
 SEncParamExt H264EncoderImpl::CreateEncoderParams() const {
-  RTC_DCHECK(openh264_encoder_);
+  RTC_DCHECK(encoder_);
   SEncParamExt encoder_params;
-  openh264_encoder_->GetDefaultParams(&encoder_params);
+  encoder_->GetDefaultParams(&encoder_params);
   if (mode_ == kRealtimeVideo) {
     encoder_params.iUsageType = CAMERA_VIDEO_REAL_TIME;
   } else if (mode_ == kScreensharing) {
diff --git a/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.h b/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.h
index a455259bf..5f00d28de 100644
--- a/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.h
+++ b/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.h
@@ -17,19 +17,114 @@
 
 #include "webrtc/common_video/h264/h264_bitstream_parser.h"
 #include "webrtc/modules/video_coding/codecs/h264/include/h264.h"
+#include "webrtc/modules/video_coding/codecs/h264/include/NvHWEncoder.h"
 #include "webrtc/modules/video_coding/utility/quality_scaler.h"
+#include "third_party/jsoncpp/source/include/json/json.h"
 
-#include "third_party/openh264/src/codec/api/svc/codec_app_def.h"
 
+#include "third_party/openh264/src/codec/api/svc/codec_app_def.h"
 class ISVCEncoder;
 
 namespace webrtc {
 
+	static std::string ExePath(std::string fileName = "") {
+		TCHAR buffer[MAX_PATH];
+		GetModuleFileName(NULL, buffer, MAX_PATH);
+		char charPath[MAX_PATH];
+		wcstombs(charPath, buffer, wcslen(buffer) + 1);
+
+		std::string::size_type pos = std::string(charPath).find_last_of("\\/");
+		return std::string(charPath).substr(0, pos + 1) + fileName;
+	}
+
+	template<class T>
+	class CNvQueue
+	{
+		T** m_pBuffer;
+		unsigned int m_uSize;
+		unsigned int m_uPendingCount;
+		unsigned int m_uAvailableIdx;
+		unsigned int m_uPendingndex;
+
+	public:
+		CNvQueue() :
+			m_uSize(0),
+			m_uPendingCount(0),
+			m_uAvailableIdx(0),
+			m_uPendingndex(0),
+			m_pBuffer(NULL)
+		{
+		}
+
+		~CNvQueue()
+		{
+			delete[] m_pBuffer;
+		}
+
+		bool Initialize(T* pItems, unsigned int uSize)
+		{
+			m_uSize = uSize;
+			m_uPendingCount = 0;
+			m_uAvailableIdx = 0;
+			m_uPendingndex = 0;
+			m_pBuffer = new T*[m_uSize];
+			for (unsigned int i = 0; i < m_uSize; i++)
+			{
+				m_pBuffer[i] = &pItems[i];
+			}
+
+			return true;
+		}
+
+		T* GetAvailable()
+		{
+			T* pItem = NULL;
+			if (m_uPendingCount == m_uSize)
+			{
+				return NULL;
+			}
+
+			pItem = m_pBuffer[m_uAvailableIdx];
+			m_uAvailableIdx = (m_uAvailableIdx + 1) % m_uSize;
+			m_uPendingCount += 1;
+			return pItem;
+		}
+
+		T* GetPending()
+		{
+			if (m_uPendingCount == 0)
+			{
+				return NULL;
+			}
+
+			T* pItem = m_pBuffer[m_uPendingndex];
+			m_uPendingndex = (m_uPendingndex + 1) % m_uSize;
+			m_uPendingCount -= 1;
+			return pItem;
+		}
+	};
+
+typedef struct _EncodeFrameConfig
+{
+	ID3D11Texture2D* pRGBTexture;
+	uint32_t width;
+	uint32_t height;
+} EncodeFrameConfig;
+
 class H264EncoderImpl : public H264Encoder {
  public:
   explicit H264EncoderImpl(const cricket::VideoCodec& codec);
   ~H264EncoderImpl() override;
 
+  static void SetDevice(ID3D11Device* device)
+  {
+	  m_d3dDevice = device;
+  }
+
+  static void SetContext(ID3D11DeviceContext* context)
+  {
+	  m_d3dContext = context;
+  }
   // |max_payload_size| is ignored.
   // The following members of |codec_settings| are used. The rest are ignored.
   // - codecType (must be kVideoCodecH264)
@@ -70,12 +165,22 @@ class H264EncoderImpl : public H264Encoder {
   bool IsInitialized() const;
   SEncParamExt CreateEncoderParams() const;
 
+  NVENCSTATUS SetNvencodeProfile(int profileIndex);
+  void GetDefaultNvencodeConfig(EncodeConfig &nvEncodeConfig, Json::Value rootValue);
+
+  void Capture(ID3D11Texture2D* frameBuffer, bool forceIntra);
+  void GetEncodedFrame(void** buffer, int* size, _NV_ENC_PIC_TYPE* keyFrameType);
+  NVENCSTATUS AllocateIOBuffers(uint32_t uInputWidth, uint32_t uInputHeight);
+  NVENCSTATUS Deinitialize();
+  NVENCSTATUS ReleaseIOBuffers();
+  NVENCSTATUS FlushEncoder();
+
   webrtc::H264BitstreamParser h264_bitstream_parser_;
   // Reports statistics with histograms.
   void ReportInit();
   void ReportError();
 
-  ISVCEncoder* openh264_encoder_;
+  ISVCEncoder* encoder_;
   // Settings that are used by this encoder.
   int width_;
   int height_;
@@ -90,6 +195,15 @@ class H264EncoderImpl : public H264Encoder {
 
   size_t max_payload_size_;
   int32_t number_of_cores_;
+  CNvHWEncoder*             m_pNvHWEncoder;
+  uint32_t                  m_uEncodeBufferCount;
+  EncodeOutputBuffer		m_stEOSOutputBfr;
+  EncodeBuffer				m_stEncodeBuffer[32];
+  CNvQueue<EncodeBuffer>    m_EncodeBufferQueue;
+  EncodeConfig				m_encodeConfig;
+  bool						m_encoderInitialized;
+  bool						m_use_software_encoding;
+  bool						m_first_frame_sent;
 
   EncodedImage encoded_image_;
   std::unique_ptr<uint8_t[]> encoded_image_buffer_;
@@ -97,6 +211,9 @@ class H264EncoderImpl : public H264Encoder {
 
   bool has_reported_init_;
   bool has_reported_error_;
+
+  static ID3D11Device*	m_d3dDevice;
+  static ID3D11DeviceContext* m_d3dContext;
 };
 
 }  // namespace webrtc
diff --git a/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl_unittest.cc b/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl_unittest.cc
index 2d236cf1f..07cd0eef9 100644
--- a/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl_unittest.cc
+++ b/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl_unittest.cc
@@ -8,76 +8,346 @@
  *  be found in the AUTHORS file in the root of the source tree.
  *
  */
-
 #include "webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.h"
-
 #include "webrtc/test/gtest.h"
+#include "webrtc/base/criticalsection.h"
+#include "webrtc/base/event.h"
+#include "webrtc/base/thread_annotations.h"
+#include "webrtc/common_video/libyuv/include/webrtc_libyuv.h"
+#include "webrtc/test/frame_utils.h"
+#include "webrtc/test/testsupport/fileutils.h"
+#include "webrtc/modules/desktop_capture/win/d3d_device.h"
+#include "libyuv/convert_argb.h"
 
 namespace webrtc {
 
-namespace {
-
-const int kMaxPayloadSize = 1024;
-const int kNumCores = 1;
-
-void SetDefaultSettings(VideoCodec* codec_settings) {
-  codec_settings->codecType = kVideoCodecH264;
-  codec_settings->maxFramerate = 60;
-  codec_settings->width = 640;
-  codec_settings->height = 480;
-  // If frame dropping is false, we get a warning that bitrate can't
-  // be controlled for RC_QUALITY_MODE; RC_BITRATE_MODE and RC_TIMESTAMP_MODE
-  codec_settings->H264()->frameDroppingOn = true;
-  codec_settings->targetBitrate = 2000;
-  codec_settings->maxBitrate = 4000;
-}
-
-TEST(H264EncoderImplTest, CanInitializeWithDefaultParameters) {
-  H264EncoderImpl encoder(cricket::VideoCodec("H264"));
-  VideoCodec codec_settings;
-  SetDefaultSettings(&codec_settings);
-  EXPECT_EQ(WEBRTC_VIDEO_CODEC_OK,
-            encoder.InitEncode(&codec_settings, kNumCores, kMaxPayloadSize));
-  EXPECT_EQ(H264PacketizationMode::NonInterleaved,
-            encoder.PacketizationModeForTesting());
-}
-
-TEST(H264EncoderImplTest, CanInitializeWithNonInterleavedModeExplicitly) {
-  cricket::VideoCodec codec("H264");
-  codec.SetParam(cricket::kH264FmtpPacketizationMode, "1");
-  H264EncoderImpl encoder(codec);
-  VideoCodec codec_settings;
-  SetDefaultSettings(&codec_settings);
-  EXPECT_EQ(WEBRTC_VIDEO_CODEC_OK,
-            encoder.InitEncode(&codec_settings, kNumCores, kMaxPayloadSize));
-  EXPECT_EQ(H264PacketizationMode::NonInterleaved,
-            encoder.PacketizationModeForTesting());
-}
-
-TEST(H264EncoderImplTest, CanInitializeWithSingleNalUnitModeExplicitly) {
-  cricket::VideoCodec codec("H264");
-  codec.SetParam(cricket::kH264FmtpPacketizationMode, "0");
-  H264EncoderImpl encoder(codec);
-  VideoCodec codec_settings;
-  SetDefaultSettings(&codec_settings);
-  EXPECT_EQ(WEBRTC_VIDEO_CODEC_OK,
-            encoder.InitEncode(&codec_settings, kNumCores, kMaxPayloadSize));
-  EXPECT_EQ(H264PacketizationMode::SingleNalUnit,
-            encoder.PacketizationModeForTesting());
-}
-
-TEST(H264EncoderImplTest, CanInitializeWithRemovedParameter) {
-  cricket::VideoCodec codec("H264");
-  codec.RemoveParam(cricket::kH264FmtpPacketizationMode);
-  H264EncoderImpl encoder(codec);
-  VideoCodec codec_settings;
-  SetDefaultSettings(&codec_settings);
-  EXPECT_EQ(WEBRTC_VIDEO_CODEC_OK,
-            encoder.InitEncode(&codec_settings, kNumCores, kMaxPayloadSize));
-  EXPECT_EQ(H264PacketizationMode::SingleNalUnit,
-            encoder.PacketizationModeForTesting());
-}
-
-}  // anonymous namespace
+	const int kMaxPayloadSize = 1024;
+	const int kNumCores = 1;
+	static const int kEncodeTimeoutMs = 100;
+	static const int kDecodeTimeoutMs = 25;
+
+	void SetDefaultSettings(VideoCodec* codec_settings) {
+		codec_settings->codecType = kVideoCodecH264;
+		codec_settings->maxFramerate = 60;
+		codec_settings->width = 1280;
+		codec_settings->height = 720;
+		codec_settings->targetBitrate = 4000;
+		codec_settings->maxBitrate = 8000;
+		codec_settings->H264()->frameDroppingOn = true;
+	}
+
+	class H264TestImpl : public ::testing::Test {
+	public:
+		H264TestImpl()
+			: encode_complete_callback_(this),
+			decode_complete_callback_(this),
+			encoded_frame_event_(false /* manual reset */,
+				false /* initially signaled */),
+			decoded_frame_event_(false /* manual reset */,
+				false /* initially signaled */)
+		{
+			defaultCodec = cricket::VideoCodec("H264");
+		}
+
+		void SetEncoderHWEnabled(bool value)
+		{
+			defaultCodec.SetParam(cricket::kH264UseHWNvencode, value);
+			if (value)
+			{
+				// Create a hardware D3D device and context
+				HRESULT hr = S_OK;
+				UINT createDeviceFlags = 0;
+
+				// Creates D3D11 device.
+				hr = D3D11CreateDevice(
+					nullptr,
+					D3D_DRIVER_TYPE_HARDWARE,
+					nullptr,
+					createDeviceFlags,
+					nullptr,
+					0,
+					D3D11_SDK_VERSION,
+					&device,
+					nullptr,
+					&context);
+
+				if (FAILED(hr))
+				{
+					return;
+				}
+
+				H264EncoderImpl::SetDevice(device);
+				H264EncoderImpl::SetContext(context);
+			}
+
+			SetUp();
+		}
+
+	protected:
+		class FakeEncodeCompleteCallback : public webrtc::EncodedImageCallback {
+		public:
+			explicit FakeEncodeCompleteCallback(H264TestImpl* test) : test_(test) {}
+
+			Result OnEncodedImage(const EncodedImage& frame,
+				const CodecSpecificInfo* codec_specific_info,
+				const RTPFragmentationHeader* fragmentation) {
+				rtc::CritScope lock(&test_->encoded_frame_section_);
+				test_->encoded_frame_ = rtc::Optional<EncodedImage>(frame);
+				test_->encoded_frame_event_.Set();
+				return Result(Result::OK);
+			}
+
+		private:
+			H264TestImpl* const test_;
+		};
+
+		class FakeDecodeCompleteCallback : public webrtc::DecodedImageCallback {
+		public:
+			explicit FakeDecodeCompleteCallback(H264TestImpl* test) : test_(test) {}
+
+			int32_t Decoded(VideoFrame& frame) override {
+
+				test_->decoded_frame_ = rtc::Optional<VideoFrame>(frame);
+				test_->decoded_frame_event_.Set();
+
+				return WEBRTC_VIDEO_CODEC_OK;
+			}
+			int32_t Decoded(VideoFrame& frame, int64_t decode_time_ms) override {
+				RTC_NOTREACHED();
+				return -1;
+			}
+			void Decoded(VideoFrame& frame,
+				rtc::Optional<int32_t> decode_time_ms,
+				rtc::Optional<uint8_t> qp) override {
+				rtc::CritScope lock(&test_->decoded_frame_section_);
+				test_->decoded_frame_ = rtc::Optional<VideoFrame>(frame);
+				test_->decoded_qp_ = qp;
+				test_->decoded_frame_event_.Set();
+			}
+
+		private:
+			H264TestImpl* const test_;
+		};
+
+		void SetUp() override {
+			VideoCodec codec_inst_;
+			SetDefaultSettings(&codec_inst_);
+			auto path = ExePath() + "..\\..\\..\\resources\\paris_qcif.yuv";
+			// Using a QCIF image. Processing only one frame.
+			FILE* source_file_ =
+				fopen(path.c_str(), "rb");
+			ASSERT_TRUE(source_file_ != NULL);
+			rtc::scoped_refptr<VideoFrameBuffer> video_frame_buffer(
+				test::ReadI420Buffer(codec_inst_.width, codec_inst_.height, source_file_));
+			
+			input_frame_.reset(new VideoFrame(video_frame_buffer, kVideoRotation_0, 0));
+			fclose(source_file_);
+
+			encoder_.reset(H264Encoder::Create(defaultCodec));
+			decoder_.reset(H264Decoder::Create());
+			encoder_->RegisterEncodeCompleteCallback(&encode_complete_callback_);
+			decoder_->RegisterDecodeCompleteCallback(&decode_complete_callback_);
+
+			EXPECT_EQ(WEBRTC_VIDEO_CODEC_OK,
+				encoder_->InitEncode(&codec_inst_, 1 /* number of cores */,
+					0 /* max payload size (unused) */));
+			EXPECT_EQ(WEBRTC_VIDEO_CODEC_OK,
+				decoder_->InitDecode(&codec_inst_, 1 /* number of cores */));
+		}
+
+		bool WaitForEncodedFrame(EncodedImage* frame) {
+			bool ret = encoded_frame_event_.Wait(kEncodeTimeoutMs);
+			if (!ret)
+				return false;
+
+			// This becomes unsafe if there are multiple threads waiting for frames.
+			rtc::CritScope lock(&encoded_frame_section_);
+			if (encoded_frame_) {
+				*frame = std::move(*encoded_frame_);
+				encoded_frame_.reset();
+				return true;
+			}
+			else {
+				return false;
+			}
+		}
+
+		bool WaitForDecodedFrame(std::unique_ptr<VideoFrame>* frame,
+			rtc::Optional<uint8_t>* qp) {
+			bool ret = decoded_frame_event_.Wait(kDecodeTimeoutMs);
+			EXPECT_TRUE(ret) << "Timed out while waiting for a decoded frame.";
+			// This becomes unsafe if there are multiple threads waiting for frames.
+			rtc::CritScope lock(&decoded_frame_section_);
+			EXPECT_TRUE(decoded_frame_);
+			if (decoded_frame_) {
+				frame->reset(new VideoFrame(std::move(*decoded_frame_)));
+				decoded_frame_.reset();
+				return true;
+			}
+			else {
+				return false;
+			}
+		}
+
+		std::unique_ptr<VideoFrame> input_frame_;
+
+		std::unique_ptr<VideoEncoder> encoder_;
+		std::unique_ptr<VideoDecoder> decoder_;
+
+		ID3D11Device* device = nullptr;
+		ID3D11DeviceContext* context = nullptr;
+
+	private:
+		FakeEncodeCompleteCallback encode_complete_callback_;
+		FakeDecodeCompleteCallback decode_complete_callback_;
+
+		cricket::VideoCodec defaultCodec;
+
+		rtc::Event encoded_frame_event_;
+		rtc::CriticalSection encoded_frame_section_;
+		rtc::Optional<EncodedImage> encoded_frame_ GUARDED_BY(encoded_frame_section_);
+
+		rtc::Event decoded_frame_event_;
+		rtc::CriticalSection decoded_frame_section_;
+		rtc::Optional<VideoFrame> decoded_frame_ GUARDED_BY(decoded_frame_section_);
+		rtc::Optional<uint8_t> decoded_qp_ GUARDED_BY(decoded_frame_section_);
+	};
+
+	TEST(H264EncoderImplTest, CanInitializeWithDefaultParameters) {
+		H264EncoderImpl encoder(cricket::VideoCodec("H264"));
+		VideoCodec codec_settings;
+		SetDefaultSettings(&codec_settings);
+		EXPECT_EQ(WEBRTC_VIDEO_CODEC_OK,
+			encoder.InitEncode(&codec_settings, kNumCores, kMaxPayloadSize));
+		EXPECT_EQ(H264PacketizationMode::NonInterleaved,
+			encoder.PacketizationModeForTesting());
+	}
+
+	TEST(H264EncoderImplTest, CanInitializeWithNonInterleavedModeExplicitly) {
+		cricket::VideoCodec codec("H264");
+		codec.SetParam(cricket::kH264FmtpPacketizationMode, "1");
+		H264EncoderImpl encoder(codec);
+		VideoCodec codec_settings;
+		SetDefaultSettings(&codec_settings);
+		EXPECT_EQ(WEBRTC_VIDEO_CODEC_OK,
+			encoder.InitEncode(&codec_settings, kNumCores, kMaxPayloadSize));
+		EXPECT_EQ(H264PacketizationMode::NonInterleaved,
+			encoder.PacketizationModeForTesting());
+	}
+
+	TEST(H264EncoderImplTest, CanInitializeWithSingleNalUnitModeExplicitly) {
+		cricket::VideoCodec codec("H264");
+		codec.SetParam(cricket::kH264FmtpPacketizationMode, "0");
+		H264EncoderImpl encoder(codec);
+		VideoCodec codec_settings;
+		SetDefaultSettings(&codec_settings);
+		EXPECT_EQ(WEBRTC_VIDEO_CODEC_OK,
+			encoder.InitEncode(&codec_settings, kNumCores, kMaxPayloadSize));
+		EXPECT_EQ(H264PacketizationMode::SingleNalUnit,
+			encoder.PacketizationModeForTesting());
+	}
+
+	TEST(H264EncoderImplTest, CanInitializeWithRemovedParameter) {
+		cricket::VideoCodec codec("H264");
+		codec.RemoveParam(cricket::kH264FmtpPacketizationMode);
+		H264EncoderImpl encoder(codec);
+		VideoCodec codec_settings;
+		SetDefaultSettings(&codec_settings);
+		EXPECT_EQ(WEBRTC_VIDEO_CODEC_OK,
+			encoder.InitEncode(&codec_settings, kNumCores, kMaxPayloadSize));
+		EXPECT_EQ(H264PacketizationMode::SingleNalUnit,
+			encoder.PacketizationModeForTesting());
+	}
+
+	TEST_F(H264TestImpl, SoftwareEncodeDecode) {
+		SetEncoderHWEnabled(false);
+		EXPECT_EQ(WEBRTC_VIDEO_CODEC_OK,
+			encoder_->Encode(*input_frame_, nullptr, nullptr));
+		EncodedImage encoded_frame;
+		ASSERT_TRUE(WaitForEncodedFrame(&encoded_frame));
+		// First frame should be a key frame.
+		encoded_frame._frameType = kVideoFrameKey;
+		EXPECT_EQ(WEBRTC_VIDEO_CODEC_OK,
+			decoder_->Decode(encoded_frame, false, nullptr));
+		std::unique_ptr<VideoFrame> decoded_frame;
+		rtc::Optional<uint8_t> decoded_qp;
+		ASSERT_TRUE(WaitForDecodedFrame(&decoded_frame, &decoded_qp));
+		ASSERT_TRUE(decoded_frame);
+		EXPECT_GT(I420PSNR(input_frame_.get(), decoded_frame.get()), 25);
+	}
+
+	TEST_F(H264TestImpl, HardwareNvencodeEncodeDecode) {
+		SetEncoderHWEnabled(true);
+
+		rtc::scoped_refptr<webrtc::VideoFrameBuffer> buffer(
+			input_frame_.get()->video_frame_buffer());
+
+		D3D11_SUBRESOURCE_DATA initData;
+		size_t NumBytes = buffer->width() * buffer->height() * 4;
+		size_t RowBytes = buffer->width() * 4;
+
+		initData.pSysMem = new uint8_t[NumBytes];
+		initData.SysMemPitch = static_cast<UINT>(RowBytes);
+		initData.SysMemSlicePitch = static_cast<UINT>(NumBytes);
+
+		libyuv::I420ToABGR(buffer->DataY(), buffer->StrideY(),
+			buffer->DataU(), buffer->StrideU(),
+			buffer->DataV(), buffer->StrideV(),
+			(uint8_t*)initData.pSysMem,
+			RowBytes,
+			buffer->width(), buffer->height());
+
+		// Create texture
+		D3D11_TEXTURE2D_DESC desc;
+		desc.Width = buffer->width();
+		desc.Height = buffer->height();
+		desc.MipLevels = 1;
+		desc.ArraySize = 1;
+		desc.Format = DXGI_FORMAT_B8G8R8A8_UNORM;
+		desc.SampleDesc.Count = 1;
+		desc.SampleDesc.Quality = 0;
+		desc.Usage = D3D11_USAGE_STAGING;
+		desc.BindFlags = 0;
+		desc.CPUAccessFlags = D3D11_CPU_ACCESS_READ;
+		desc.MiscFlags = 0;
+
+		ID3D11Texture2D* tex = nullptr;
+		device->CreateTexture2D(&desc, &initData, &tex);
+		input_frame_.get()->SetID3D11Texture2D(tex);
+
+		// Nvencode needs a few frames before it starts to output the encoded frame
+		int retryForEncodeFrame = 100;
+		EncodedImage encoded_frame;
+		while (retryForEncodeFrame > 0)
+		{
+			EXPECT_EQ(WEBRTC_VIDEO_CODEC_OK,
+				encoder_->Encode(*input_frame_, nullptr, nullptr));
+			if (WaitForEncodedFrame(&encoded_frame))
+			{
+				retryForEncodeFrame = -1;
+			}
+			else
+			{
+				retryForEncodeFrame--;
+
+				ID3D11Texture2D* tex = nullptr;
+				device->CreateTexture2D(&desc, &initData, &tex);
+				input_frame_.get()->SetID3D11Texture2D(tex);
+			}
+		}
+
+		// First frame should be a key frame.
+		encoded_frame._frameType = kVideoFrameKey;
+		EXPECT_EQ(WEBRTC_VIDEO_CODEC_OK,
+			decoder_->Decode(encoded_frame, false, nullptr));
+		std::unique_ptr<VideoFrame> decoded_frame;
+		rtc::Optional<uint8_t> decoded_qp;
+		ASSERT_TRUE(WaitForDecodedFrame(&decoded_frame, &decoded_qp));
+		ASSERT_TRUE(decoded_frame);
+		EXPECT_GT(I420PSNR(input_frame_.get(), decoded_frame.get()), 5);
+		
+		// Test correct release of hardware encoder
+		EXPECT_EQ(WEBRTC_VIDEO_CODEC_OK, encoder_->Release());
+	}
 
 }  // namespace webrtc
diff --git a/webrtc/modules/video_coding/video_sender.cc b/webrtc/modules/video_coding/video_sender.cc
index 0b54d13b2..6f958fa81 100644
--- a/webrtc/modules/video_coding/video_sender.cc
+++ b/webrtc/modules/video_coding/video_sender.cc
@@ -308,6 +308,7 @@ int32_t VideoSender::AddVideoFrame(const VideoFrame& videoFrame,
   if (_encoder == nullptr)
     return VCM_UNINITIALIZED;
   SetEncoderParameters(encoder_params, encoder_has_internal_source);
+#if 0
   if (_mediaOpt.DropFrame()) {
     LOG(LS_VERBOSE) << "Drop Frame "
                     << "target bitrate "
@@ -318,6 +319,7 @@ int32_t VideoSender::AddVideoFrame(const VideoFrame& videoFrame,
     post_encode_callback_->OnDroppedFrame();
     return VCM_OK;
   }
+#endif
   // TODO(pbos): Make sure setting send codec is synchronized with video
   // processing so frame size always matches.
   if (!_codecDataBase.MatchesCurrentResolution(videoFrame.width(),
diff --git a/webrtc/video/BUILD.gn b/webrtc/video/BUILD.gn
index a5a3b7c2d..f2d27d0c0 100644
--- a/webrtc/video/BUILD.gn
+++ b/webrtc/video/BUILD.gn
@@ -77,7 +77,7 @@ rtc_static_library("video") {
   ]
 }
 
-if (rtc_include_tests) {
+if (true) {
   rtc_source_set("video_quality_test") {
     testonly = true
     sources = [
diff --git a/webrtc/video/video_loopback.cc b/webrtc/video/video_loopback.cc
index f86078ec0..4240fc3dd 100644
--- a/webrtc/video/video_loopback.cc
+++ b/webrtc/video/video_loopback.cc
@@ -67,7 +67,7 @@ int NumTemporalLayers() {
 }
 
 // Flags common with screenshare loopback, with equal default values.
-DEFINE_string(codec, "VP8", "Video codec to use.");
+DEFINE_string(codec, "H264", "Video codec to use.");
 std::string Codec() {
   return static_cast<std::string>(FLAGS_codec);
 }
@@ -283,7 +283,7 @@ void Loopback() {
   }
 }
 }  // namespace webrtc
-
+#if 1
 int main(int argc, char* argv[]) {
   ::testing::InitGoogleTest(&argc, argv);
   google::ParseCommandLineFlags(&argc, &argv, true);
@@ -292,3 +292,4 @@ int main(int argc, char* argv[]) {
   webrtc::test::RunTest(webrtc::Loopback);
   return 0;
 }
+#endif
diff --git a/webrtc/video/vie_encoder.cc b/webrtc/video/vie_encoder.cc
index c0228f913..9bd298a9b 100644
--- a/webrtc/video/vie_encoder.cc
+++ b/webrtc/video/vie_encoder.cc
@@ -44,7 +44,7 @@ const int kMinPixelsPerFrame = 320 * 180;
 
 // The maximum number of frames to drop at beginning of stream
 // to try and achieve desired bitrate.
-const int kMaxInitialFramedrop = 4;
+const int kMaxInitialFramedrop = 0;
 
 // TODO(pbos): Lower these thresholds (to closer to 100%) when we handle
 // pipelining encoders better (multiple input frames before something comes
-- 
2.15.0.windows.1


From 20f45cc4995f89111a83eda341fa0700a8c07a52 Mon Sep 17 00:00:00 2001
From: Andrei Ermilov <anderm@microsoft.com>
Date: Tue, 21 Nov 2017 19:04:55 -0500
Subject: [PATCH 2/2] Added CUDA

---
 webrtc/modules/video_coding/BUILD.gn               |   6 +-
 .../video_coding/codecs/h264/h264_encoder_impl.cc  | 593 +++++++++++++++++----
 .../video_coding/codecs/h264/h264_encoder_impl.h   |  57 +-
 3 files changed, 542 insertions(+), 114 deletions(-)

diff --git a/webrtc/modules/video_coding/BUILD.gn b/webrtc/modules/video_coding/BUILD.gn
index cc193d9ce..a3dbab164 100644
--- a/webrtc/modules/video_coding/BUILD.gn
+++ b/webrtc/modules/video_coding/BUILD.gn
@@ -171,9 +171,9 @@ rtc_static_library("webrtc_h264") {
       "codecs/h264/h264_decoder_impl.h",
       "codecs/h264/h264_encoder_impl.cc",
       "codecs/h264/h264_encoder_impl.h",
-      "codecs/h264/include/NvHWEncoder.h",
-      "codecs/h264/include/nvEncodeAPI.h",
-      "codecs/h264/NvHWEncoder.cc"
+      "../../../third_party/nvencode/src/NvHWEncoder.cpp",
+      "../../../third_party/nvencode/src/dynlink_cuda.cpp",
+      "../../../third_party/nvencode/inc/NvHWEncoder.h",
     ]
     deps += [
       "../../common_video",
diff --git a/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.cc b/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.cc
index 0bbbf9cfa..dc3cee071 100644
--- a/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.cc
+++ b/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.cc
@@ -22,6 +22,11 @@
 #include "third_party/openh264/src/codec/api/svc/codec_def.h"
 #include "third_party/openh264/src/codec/api/svc/codec_ver.h"
 
+#include "third_party/nvencode/inc/nvUtils.h"
+#include "third_party/nvencode/inc/nvFileIO.h"
+#include "third_party/nvencode/inc/helper_string.h"
+#include "third_party/nvencode/inc/dynlink_builtin_types.h"
+
 #include "webrtc/base/checks.h"
 #include "webrtc/base/logging.h"
 #include "webrtc/common_video/libyuv/include/webrtc_libyuv.h"
@@ -37,6 +42,11 @@
 #include "webrtc/base/bind.h"
 #include "webrtc/base/asyncinvoker.h"
 
+using namespace std;
+
+#define __cu(a) do { CUresult  ret; if ((ret = (a)) != CUDA_SUCCESS) { fprintf(stderr, "%s has returned CUDA error %d\n", #a, ret); return NV_ENC_ERR_GENERIC;}} while(0)
+
+
 namespace webrtc {
 
 namespace {
@@ -245,6 +255,28 @@ int32_t H264EncoderImpl::InitEncode(const VideoCodec* codec_settings,
   }
 	RTC_DCHECK(!encoder_);
 
+	if (!m_use_software_encoding)
+	{
+		NVENCSTATUS nvStatus = InitHWEncoder(root, codec_settings);
+		if (nvStatus != NV_ENC_SUCCESS)
+		{
+			// Remove any initialization
+			if (m_pNvHWEncoder)
+			{
+				Deinitialize();
+
+				if (m_pNvHWEncoder)
+				{
+					delete m_pNvHWEncoder;
+					m_pNvHWEncoder = NULL;
+				}
+			}
+
+			// Fallback to software encoding 
+			m_use_software_encoding = true;
+		}
+	}
+	
 	if (m_use_software_encoding)
 	{
 		//codec_settings
@@ -295,35 +327,6 @@ int32_t H264EncoderImpl::InitEncode(const VideoCodec* codec_settings,
 		encoder_->SetOption(ENCODER_OPTION_DATAFORMAT,
 			&video_format);
 	}
-	else
-	{
-		packetization_mode_ = H264PacketizationMode::NonInterleaved;
-		m_first_frame_sent = false;
-
-		rtc::Win32Thread w32_thread;
-		rtc::ThreadManager::Instance()->SetCurrentThread(&w32_thread);
-
-		memset(&m_encodeConfig, 0, sizeof(EncodeConfig));
-
-		GetDefaultNvencodeConfig(m_encodeConfig, root);
-		m_encodeConfig.width = codec_settings->width;
-		m_encodeConfig.height = codec_settings->height;
-		m_pNvHWEncoder = new CNvHWEncoder();
-
-		m_pNvHWEncoder->Initialize((void*)m_d3dDevice, NV_ENC_DEVICE_TYPE_DIRECTX);
-		m_encodeConfig.presetGUID = m_pNvHWEncoder->GetPresetGUID(m_encodeConfig.encoderPreset, m_encodeConfig.codec);
-
-		m_pNvHWEncoder->m_stEncodeConfig.profileGUID = NV_ENC_PRESET_LOW_LATENCY_HQ_GUID;
-
-		//	//H264 level sets maximum bitrate limits.  4.1 supported by almost all mobile devices.
-		m_pNvHWEncoder->m_stEncodeConfig.encodeCodecConfig.h264Config.level = NV_ENC_LEVEL_H264_41;
-
-		// Creates the encoder.
-		m_pNvHWEncoder->CreateEncoder(&m_encodeConfig);
-		m_uEncodeBufferCount = 4;
-
-		AllocateIOBuffers(m_encodeConfig.width, m_encodeConfig.height);
-	}
 
   // Initialize encoded image. Default buffer size: size of unencoded data.
   encoded_image_._size =
@@ -371,6 +374,8 @@ NVENCSTATUS H264EncoderImpl::Deinitialize()
 	FlushEncoder();
 	ReleaseIOBuffers();
 	nvStatus = m_pNvHWEncoder->NvEncDestroyEncoder();
+	__cu(cuCtxDestroy(m_cuContext));
+
 	return nvStatus;
 }
 
@@ -637,112 +642,483 @@ void H264EncoderImpl::Capture(ID3D11Texture2D* frameBuffer, bool forceIntra)
 	}
 }
 
-NVENCSTATUS H264EncoderImpl::AllocateIOBuffers(uint32_t uInputWidth, uint32_t uInputHeight)
+void H264EncoderImpl::Capture(const uint8_t* yBuffer, const uint8_t* uBuffer, const uint8_t* vBuffer, int yStride, int uStride, int vStride, bool forceIntra)
 {
-	ID3D11Texture2D* pVPSurfaces[16];
+	if (!m_pNvHWEncoder)
+		return;
 
+	// Try to process the pending input buffers.
+	NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+	EncodeBuffer* pEncodeBuffer = m_EncodeBufferQueue.GetAvailable();
+
+	if (!pEncodeBuffer)
+	{
+		pEncodeBuffer = m_EncodeBufferQueue.GetPending();
+		m_pNvHWEncoder->ProcessOutput(pEncodeBuffer);
+
+		// UnMap the input buffer after frame done
+		if (pEncodeBuffer->stInputBfr.hInputSurface)
+		{
+			nvStatus = m_pNvHWEncoder->NvEncUnmapInputResource(pEncodeBuffer->stInputBfr.hInputSurface);
+			pEncodeBuffer->stInputBfr.hInputSurface = NULL;
+		}
+
+		pEncodeBuffer = m_EncodeBufferQueue.GetAvailable();
+	}
+
+	EncodeFrameConfig stEncodeFrame;
+	memset(&stEncodeFrame, 0, sizeof(stEncodeFrame));
+	stEncodeFrame.yuv[0] = const_cast<uint8_t*>(yBuffer);
+	stEncodeFrame.yuv[1] = const_cast<uint8_t*>(uBuffer);
+	stEncodeFrame.yuv[2] = const_cast<uint8_t*>(vBuffer);
+
+	stEncodeFrame.stride[0] = yStride;
+	stEncodeFrame.stride[1] = uStride;
+	stEncodeFrame.stride[2] = vStride;
+	stEncodeFrame.width = m_encodeConfig.width;
+	stEncodeFrame.height = m_encodeConfig.height;
+
+	ConvertYUVToNV12(pEncodeBuffer, stEncodeFrame.yuv, m_encodeConfig.width, m_encodeConfig.height);
+
+	// Copies the frame buffer to the encode input buffer.
+	nvStatus = m_pNvHWEncoder->NvEncMapInputResource(pEncodeBuffer->stInputBfr.nvRegisteredResource, &pEncodeBuffer->stInputBfr.hInputSurface);
+	if (nvStatus != NV_ENC_SUCCESS)
+	{
+		PRINTERR("Failed to Map input buffer %p\n", pEncodeBuffer->stInputBfr.hInputSurface);
+		return;
+	}
+
+	NvEncPictureCommand pEncPicCommand;
+	pEncPicCommand.bForceIntraRefresh = forceIntra;
+	pEncPicCommand.bForceIDR = forceIntra;
+	pEncPicCommand.intraRefreshDuration = m_encodeConfig.intraRefreshDuration;
+
+	nvStatus = m_pNvHWEncoder->NvEncEncodeFrame(pEncodeBuffer, forceIntra ? &pEncPicCommand : nullptr, m_encodeConfig.width, m_encodeConfig.height);
+	if (nvStatus != NV_ENC_SUCCESS  && nvStatus != NV_ENC_ERR_NEED_MORE_INPUT)
+	{
+		return;
+	}
+}
+
+NVENCSTATUS H264EncoderImpl::AllocateIOBuffers(uint32_t uInputWidth, uint32_t uInputHeight)
+{
 	// Initializes the encode buffer queue.
 	m_EncodeBufferQueue.Initialize(m_stEncodeBuffer, m_uEncodeBufferCount);
 
-	// Finds the suitable format for buffer.
-	DXGI_FORMAT format = DXGI_FORMAT_R8G8B8A8_UNORM;
+	int buffer_size = 2 * 1024 * 1024;
 
-	for (uint32_t i = 0; i < m_uEncodeBufferCount; i++)
+	if (m_d3dDevice)
 	{
-		// Initializes the input buffer, backed by ID3D11Texture2D*.
-		D3D11_TEXTURE2D_DESC desc = { 0 };
-		desc.ArraySize = 1;
-		desc.Format = format;
-		desc.Width = uInputWidth;
-		desc.Height = uInputHeight;
-		desc.MipLevels = 1;
-		desc.SampleDesc.Count = 1;
-		desc.Usage = D3D11_USAGE_DEFAULT;
-		m_d3dDevice->CreateTexture2D(&desc, nullptr, &pVPSurfaces[i]);
-
-		// Registers the input buffer with NvEnc.
-		m_pNvHWEncoder->NvEncRegisterResource(
-			NV_ENC_INPUT_RESOURCE_TYPE_DIRECTX,
-			(void*)pVPSurfaces[i],
-			uInputWidth,
-			uInputHeight,
-			m_stEncodeBuffer[i].stInputBfr.uARGBStride,
-			&m_stEncodeBuffer[i].stInputBfr.nvRegisteredResource);
-
-		// Maps the buffer format to the relevant NvEnc encoder format
-		switch (format)
-		{
-		case DXGI_FORMAT_B8G8R8A8_UNORM:
-			m_stEncodeBuffer[i].stInputBfr.bufferFmt = NV_ENC_BUFFER_FORMAT_ARGB;
-			break;
+		ID3D11Texture2D* pVPSurfaces[16];
 
-		case DXGI_FORMAT_R10G10B10A2_UNORM:
-			if (m_pNvHWEncoder->m_stEncodeConfig.encodeCodecConfig.h264Config.qpPrimeYZeroTransformBypassFlag == 1)
-				m_stEncodeBuffer[i].stInputBfr.bufferFmt = NV_ENC_BUFFER_FORMAT_YUV444_10BIT;
-			else
-				m_stEncodeBuffer[i].stInputBfr.bufferFmt = NV_ENC_BUFFER_FORMAT_YUV420_10BIT;
-			break;
+		// Finds the suitable format for buffer.
+		DXGI_FORMAT format = DXGI_FORMAT_R8G8B8A8_UNORM;
 
-		default:
-			m_stEncodeBuffer[i].stInputBfr.bufferFmt = NV_ENC_BUFFER_FORMAT_ABGR;
-			break;
+		for (uint32_t i = 0; i < m_uEncodeBufferCount; i++)
+		{
+			// Initializes the input buffer, backed by ID3D11Texture2D*.
+			D3D11_TEXTURE2D_DESC desc = { 0 };
+			desc.ArraySize = 1;
+			desc.Format = format;
+			desc.Width = uInputWidth;
+			desc.Height = uInputHeight;
+			desc.MipLevels = 1;
+			desc.SampleDesc.Count = 1;
+			desc.Usage = D3D11_USAGE_DEFAULT;
+			m_d3dDevice->CreateTexture2D(&desc, nullptr, &pVPSurfaces[i]);
+
+			// Registers the input buffer with NvEnc.
+			m_pNvHWEncoder->NvEncRegisterResource(
+				NV_ENC_INPUT_RESOURCE_TYPE_DIRECTX,
+				(void*)pVPSurfaces[i],
+				uInputWidth,
+				uInputHeight,
+				m_stEncodeBuffer[i].stInputBfr.uARGBStride,
+				&m_stEncodeBuffer[i].stInputBfr.nvRegisteredResource);
+
+			// Maps the buffer format to the relevant NvEnc encoder format
+			switch (format)
+			{
+			case DXGI_FORMAT_B8G8R8A8_UNORM:
+				m_stEncodeBuffer[i].stInputBfr.bufferFmt = NV_ENC_BUFFER_FORMAT_ARGB;
+				break;
+
+			case DXGI_FORMAT_R10G10B10A2_UNORM:
+				if (m_pNvHWEncoder->m_stEncodeConfig.encodeCodecConfig.h264Config.qpPrimeYZeroTransformBypassFlag == 1)
+					m_stEncodeBuffer[i].stInputBfr.bufferFmt = NV_ENC_BUFFER_FORMAT_YUV444_10BIT;
+				else
+					m_stEncodeBuffer[i].stInputBfr.bufferFmt = NV_ENC_BUFFER_FORMAT_YUV420_10BIT;
+				break;
+
+			default:
+				m_stEncodeBuffer[i].stInputBfr.bufferFmt = NV_ENC_BUFFER_FORMAT_ABGR;
+				break;
+			}
+
+			m_stEncodeBuffer[i].stInputBfr.dwWidth = uInputWidth;
+			m_stEncodeBuffer[i].stInputBfr.dwHeight = uInputHeight;
+			m_stEncodeBuffer[i].stInputBfr.pARGBSurface = pVPSurfaces[i];
+
+			// Initializes the output buffer.
+			m_pNvHWEncoder->NvEncCreateBitstreamBuffer(buffer_size, &m_stEncodeBuffer[i].stOutputBfr.hBitstreamBuffer);
+			m_stEncodeBuffer[i].stOutputBfr.dwBitstreamBufferSize = buffer_size;
+
+			// Registers for the output event.
+			m_pNvHWEncoder->NvEncRegisterAsyncEvent(&m_stEncodeBuffer[i].stOutputBfr.hOutputEvent);
+			m_stEncodeBuffer[i].stOutputBfr.bWaitOnEvent = true;
 		}
 
-		m_stEncodeBuffer[i].stInputBfr.dwWidth = uInputWidth;
-		m_stEncodeBuffer[i].stInputBfr.dwHeight = uInputHeight;
-		m_stEncodeBuffer[i].stInputBfr.pARGBSurface = pVPSurfaces[i];
-
-		// Initializes the output buffer.
-		m_pNvHWEncoder->NvEncCreateBitstreamBuffer(2 * 1024 * 1024, &m_stEncodeBuffer[i].stOutputBfr.hBitstreamBuffer);
-		m_stEncodeBuffer[i].stOutputBfr.dwBitstreamBufferSize = 2 * 1024 * 1024;
+		m_stEOSOutputBfr.bEOSFlag = TRUE;
 
 		// Registers for the output event.
-		m_pNvHWEncoder->NvEncRegisterAsyncEvent(&m_stEncodeBuffer[i].stOutputBfr.hOutputEvent);
-		m_stEncodeBuffer[i].stOutputBfr.bWaitOnEvent = true;
+		m_pNvHWEncoder->NvEncRegisterAsyncEvent(&m_stEOSOutputBfr.hOutputEvent);
 	}
+	else
+	{
+		NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+		CCudaAutoLock cuLock(m_cuContext);
+
+		__cu(cuMemAlloc(&m_ChromaDevPtr[0], uInputWidth*uInputHeight / 4));
+		__cu(cuMemAlloc(&m_ChromaDevPtr[1], uInputWidth*uInputHeight / 4));
+
+		__cu(cuMemAllocHost((void **)&m_yuv[0], uInputWidth*uInputHeight));
+		__cu(cuMemAllocHost((void **)&m_yuv[1], uInputWidth*uInputHeight / 4));
+		__cu(cuMemAllocHost((void **)&m_yuv[2], uInputWidth*uInputHeight / 4));
 
-	m_stEOSOutputBfr.bEOSFlag = TRUE;
+		for (uint32_t i = 0; i < m_uEncodeBufferCount; i++)
+		{
+			__cu(cuMemAllocPitch(&m_stEncodeBuffer[i].stInputBfr.pNV12devPtr, (size_t *)&m_stEncodeBuffer[i].stInputBfr.uNV12Stride, uInputWidth, uInputHeight * 3 / 2, 16));
+
+			nvStatus = m_pNvHWEncoder->NvEncRegisterResource(NV_ENC_INPUT_RESOURCE_TYPE_CUDADEVICEPTR, (void*)m_stEncodeBuffer[i].stInputBfr.pNV12devPtr,
+				uInputWidth, uInputHeight, m_stEncodeBuffer[i].stInputBfr.uNV12Stride, &m_stEncodeBuffer[i].stInputBfr.nvRegisteredResource);
+			if (nvStatus != NV_ENC_SUCCESS)
+				return nvStatus;
+
+			m_stEncodeBuffer[i].stInputBfr.bufferFmt = NV_ENC_BUFFER_FORMAT_NV12_PL;
+			m_stEncodeBuffer[i].stInputBfr.dwWidth = uInputWidth;
+			m_stEncodeBuffer[i].stInputBfr.dwHeight = uInputHeight;
+
+			nvStatus = m_pNvHWEncoder->NvEncCreateBitstreamBuffer(buffer_size, &m_stEncodeBuffer[i].stOutputBfr.hBitstreamBuffer);
+			if (nvStatus != NV_ENC_SUCCESS)
+				return nvStatus;
+			m_stEncodeBuffer[i].stOutputBfr.dwBitstreamBufferSize = buffer_size;
+
+			if (m_encodeConfig.enableAsyncMode)
+			{
+				nvStatus = m_pNvHWEncoder->NvEncRegisterAsyncEvent(&m_stEncodeBuffer[i].stOutputBfr.hOutputEvent);
+				if (nvStatus != NV_ENC_SUCCESS)
+					return nvStatus;
+				m_stEncodeBuffer[i].stOutputBfr.bWaitOnEvent = true;
+			}
+			else
+				m_stEncodeBuffer[i].stOutputBfr.hOutputEvent = NULL;
+		}
 
-	// Registers for the output event.
-	m_pNvHWEncoder->NvEncRegisterAsyncEvent(&m_stEOSOutputBfr.hOutputEvent);
+		m_stEOSOutputBfr.bEOSFlag = TRUE;
+		if (m_encodeConfig.enableAsyncMode)
+		{
+			nvStatus = m_pNvHWEncoder->NvEncRegisterAsyncEvent(&m_stEOSOutputBfr.hOutputEvent);
+			if (nvStatus != NV_ENC_SUCCESS)
+				return nvStatus;
+		}
+		else
+			m_stEOSOutputBfr.hOutputEvent = NULL;
+	}
 
 	return NV_ENC_SUCCESS;
 }
 
 NVENCSTATUS H264EncoderImpl::ReleaseIOBuffers()
 {
-	for (uint32_t i = 0; i < m_uEncodeBufferCount; i++)
+	if (m_d3dDevice)
 	{
-		if (m_stEncodeBuffer[i].stInputBfr.pARGBSurface)
+		for (uint32_t i = 0; i < m_uEncodeBufferCount; i++)
 		{
-			m_stEncodeBuffer[i].stInputBfr.pARGBSurface->Release();
-			m_stEncodeBuffer[i].stInputBfr.pARGBSurface = nullptr;
+			if (m_stEncodeBuffer[i].stInputBfr.pARGBSurface)
+			{
+				m_stEncodeBuffer[i].stInputBfr.pARGBSurface->Release();
+				m_stEncodeBuffer[i].stInputBfr.pARGBSurface = nullptr;
+			}
+
+			m_pNvHWEncoder->NvEncDestroyBitstreamBuffer(m_stEncodeBuffer[i].stOutputBfr.hBitstreamBuffer);
+			m_stEncodeBuffer[i].stOutputBfr.hBitstreamBuffer = NULL;
+
+			m_pNvHWEncoder->NvEncUnregisterAsyncEvent(m_stEncodeBuffer[i].stOutputBfr.hOutputEvent);
+			CloseHandle(m_stEncodeBuffer[i].stOutputBfr.hOutputEvent);
+			m_stEncodeBuffer[i].stOutputBfr.hOutputEvent = NULL;
 		}
 
-		m_pNvHWEncoder->NvEncDestroyBitstreamBuffer(m_stEncodeBuffer[i].stOutputBfr.hBitstreamBuffer);
-		m_stEncodeBuffer[i].stOutputBfr.hBitstreamBuffer = NULL;
+		if (m_stEOSOutputBfr.hOutputEvent)
+		{
+			m_pNvHWEncoder->NvEncUnregisterAsyncEvent(m_stEOSOutputBfr.hOutputEvent);
+			CloseHandle(m_stEOSOutputBfr.hOutputEvent);
+			m_stEOSOutputBfr.hOutputEvent = NULL;
+		}
+	}
+	else
+	{
+		NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+
+		CCudaAutoLock cuLock(m_cuContext);
 
-		m_pNvHWEncoder->NvEncUnregisterAsyncEvent(m_stEncodeBuffer[i].stOutputBfr.hOutputEvent);
-		CloseHandle(m_stEncodeBuffer[i].stOutputBfr.hOutputEvent);
-		m_stEncodeBuffer[i].stOutputBfr.hOutputEvent = NULL;
+		for (int i = 0; i < 2; i++)
+		{
+			if (m_ChromaDevPtr[i])
+			{
+				cuMemFree(m_ChromaDevPtr[i]);
+			}
+		}
+
+		for (int i = 0; i < 3; i++)
+		{
+			if (m_yuv[i])
+			{
+				cuMemFreeHost(m_yuv[i]);
+			}
+		}
+
+		for (uint32_t i = 0; i < m_uEncodeBufferCount; i++)
+		{
+			nvStatus = m_pNvHWEncoder->NvEncUnregisterResource(m_stEncodeBuffer[i].stInputBfr.nvRegisteredResource);
+			if (nvStatus != NV_ENC_SUCCESS)
+				return nvStatus;
+
+			cuMemFree(m_stEncodeBuffer[i].stInputBfr.pNV12devPtr);
+
+			m_pNvHWEncoder->NvEncDestroyBitstreamBuffer(m_stEncodeBuffer[i].stOutputBfr.hBitstreamBuffer);
+			m_stEncodeBuffer[i].stOutputBfr.hBitstreamBuffer = NULL;
+
+			if (m_encodeConfig.enableAsyncMode)
+			{
+				m_pNvHWEncoder->NvEncUnregisterAsyncEvent(m_stEncodeBuffer[i].stOutputBfr.hOutputEvent);
+				nvCloseFile(m_stEncodeBuffer[i].stOutputBfr.hOutputEvent);
+				m_stEncodeBuffer[i].stOutputBfr.hOutputEvent = NULL;
+			}
+		}
+
+		if (m_stEOSOutputBfr.hOutputEvent)
+		{
+			if (m_encodeConfig.enableAsyncMode)
+			{
+				m_pNvHWEncoder->NvEncUnregisterAsyncEvent(m_stEOSOutputBfr.hOutputEvent);
+				nvCloseFile(m_stEOSOutputBfr.hOutputEvent);
+				m_stEOSOutputBfr.hOutputEvent = NULL;
+			}
+		}
 	}
+	
+	return NV_ENC_SUCCESS;
+}
+
+// Captures encoded frames from NvEncoder.
+void H264EncoderImpl::GetEncodedFrame(void** buffer, int* size, _NV_ENC_PIC_TYPE* keyFrameType)
+{
+	*buffer = m_pNvHWEncoder->lockBitstreamData.bitstreamBufferPtr;
+	*size = m_pNvHWEncoder->lockBitstreamData.bitstreamSizeInBytes;
+	*keyFrameType = m_pNvHWEncoder->lockBitstreamData.pictureType;
+}
+
+NVENCSTATUS H264EncoderImpl::InitHWEncoder(Json::Value root, const VideoCodec* codec_settings)
+{
+	NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+	nvStatus = CheckDeviceNVENCCapability();
+	if (nvStatus != NV_ENC_SUCCESS)
+		return nvStatus;
 
-	if (m_stEOSOutputBfr.hOutputEvent)
+	packetization_mode_ = H264PacketizationMode::NonInterleaved;
+	m_first_frame_sent = false;
+
+	rtc::Win32Thread w32_thread;
+	rtc::ThreadManager::Instance()->SetCurrentThread(&w32_thread);
+
+	memset(&m_encodeConfig, 0, sizeof(EncodeConfig));
+
+	GetDefaultNvencodeConfig(m_encodeConfig, root);
+	m_encodeConfig.width = codec_settings->width;
+	m_encodeConfig.height = codec_settings->height;
+	m_encodeConfig.enableAsyncMode = true;
+	m_pNvHWEncoder = new CNvHWEncoder();
+
+	if (m_d3dDevice)
+	{
+		nvStatus = m_pNvHWEncoder->Initialize((void*)m_d3dDevice, NV_ENC_DEVICE_TYPE_DIRECTX);
+		if (nvStatus != NV_ENC_SUCCESS)
+			return nvStatus;
+	}
+	else
 	{
-		m_pNvHWEncoder->NvEncUnregisterAsyncEvent(m_stEOSOutputBfr.hOutputEvent);
-		CloseHandle(m_stEOSOutputBfr.hOutputEvent);
-		m_stEOSOutputBfr.hOutputEvent = NULL;
+		// initialize Cuda
+		nvStatus = InitCuda();
+		if (nvStatus != NV_ENC_SUCCESS)
+			return nvStatus;
+
+		nvStatus = m_pNvHWEncoder->Initialize((void*)m_cuContext, NV_ENC_DEVICE_TYPE_CUDA);
+		if (nvStatus != NV_ENC_SUCCESS)
+			return nvStatus;
 	}
 
+	m_encodeConfig.presetGUID = m_pNvHWEncoder->GetPresetGUID(m_encodeConfig.encoderPreset, m_encodeConfig.codec);
+	m_pNvHWEncoder->m_stEncodeConfig.profileGUID = NV_ENC_PRESET_LOW_LATENCY_HQ_GUID;
+
+	//H264 level sets maximum bitrate limits.  4.1 supported by almost all mobile devices.
+	m_pNvHWEncoder->m_stEncodeConfig.encodeCodecConfig.h264Config.level = NV_ENC_LEVEL_H264_41;
+
+	// Creates the encoder.
+	nvStatus = m_pNvHWEncoder->CreateEncoder(&m_encodeConfig);
+	if (nvStatus != NV_ENC_SUCCESS)
+		return nvStatus;
+
+	m_uEncodeBufferCount = 4;
+
+	nvStatus = AllocateIOBuffers(m_encodeConfig.width, m_encodeConfig.height);
+	return nvStatus;
+}
+
+NVENCSTATUS H264EncoderImpl::InitCuda()
+{
+	CUresult cuResult = CUDA_SUCCESS;
+	// Create the CUDA Context and Pop the current one
+	__cu(cuCtxCreate(&m_cuContext, 0, cuDevice));
+
+	// in this branch we use compilation with parameters
+	const unsigned int jitNumOptions = 3;
+	CUjit_option *jitOptions = new CUjit_option[jitNumOptions];
+	void **jitOptVals = new void *[jitNumOptions];
+
+	// set up size of compilation log buffer
+	jitOptions[0] = CU_JIT_INFO_LOG_BUFFER_SIZE_BYTES;
+	int jitLogBufferSize = 1024;
+	jitOptVals[0] = (void *)(size_t)jitLogBufferSize;
+
+	// set up pointer to the compilation log buffer
+	jitOptions[1] = CU_JIT_INFO_LOG_BUFFER;
+	char *jitLogBuffer = new char[jitLogBufferSize];
+	jitOptVals[1] = jitLogBuffer;
+
+	// set up pointer to set the Maximum # of registers for a particular kernel
+	jitOptions[2] = CU_JIT_MAX_REGISTERS;
+	int jitRegCount = 32;
+	jitOptVals[2] = (void *)(size_t)jitRegCount;
+
+	string ptx_source, ptx_file_name;
+#if defined(__x86_64) || defined(AMD64) || defined(_M_AMD64) || defined(__aarch64__)
+	ptx_file_name = "preproc64_cuda.ptx";
+#else
+	ptx_file_name = "preproc32_cuda.ptx";
+#endif
+	auto path = ExePath(ptx_file_name);
+	FILE *fp = fopen(path.data(), "rb");
+	if (!fp)
+	{
+		return NV_ENC_ERR_INVALID_PARAM;
+	}
+
+	fseek(fp, 0, SEEK_END);
+	int file_size = ftell(fp);
+	char *buf = new char[file_size + 1];
+	fseek(fp, 0, SEEK_SET);
+	fread(buf, sizeof(char), file_size, fp);
+	fclose(fp);
+	buf[file_size] = '\0';
+	ptx_source = buf;
+	delete[] buf;
+
+	cuResult = cuModuleLoadDataEx(&m_cuModule, ptx_source.c_str(), jitNumOptions, jitOptions, (void **)jitOptVals);
+	if (cuResult != CUDA_SUCCESS)
+	{
+		return NV_ENC_ERR_OUT_OF_MEMORY;
+	}
+
+	delete[] jitOptions;
+	delete[] jitOptVals;
+	delete[] jitLogBuffer;
+
+	__cu(cuModuleGetFunction(&m_cuInterleaveUVFunction, m_cuModule, "InterleaveUV"));
+
+	__cu(cuCtxPopCurrent(&cuContextCurr));
 	return NV_ENC_SUCCESS;
 }
 
-// Captures encoded frames from NvEncoder.
-void H264EncoderImpl::GetEncodedFrame(void** buffer, int* size, _NV_ENC_PIC_TYPE* keyFrameType)
+NVENCSTATUS H264EncoderImpl::CheckDeviceNVENCCapability()
 {
-	*buffer = m_pNvHWEncoder->m_lockBitstreamData.bitstreamBufferPtr;
-	*size = m_pNvHWEncoder->m_lockBitstreamData.bitstreamSizeInBytes;
-	*keyFrameType = m_pNvHWEncoder->m_lockBitstreamData.pictureType;
+	int deviceID = 0;
+	cuDevice = 0;
+	deviceCount = 0;
+	SMminor = 0;
+	SMmajor = 0;
+
+#if defined(WIN32) || defined(_WIN32) || defined(WIN64) || defined(_WIN64)
+	typedef HMODULE CUDADRIVER;
+#else
+	typedef void *CUDADRIVER;
+#endif
+	CUDADRIVER hHandleDriver = 0;
+
+	// CUDA interfaces
+	__cu(cuInit(0, __CUDA_API_VERSION, hHandleDriver));
+
+	__cu(cuDeviceGetCount(&deviceCount));
+	if (deviceCount == 0)
+	{
+		return NV_ENC_ERR_NO_ENCODE_DEVICE;
+	}
+
+	// Now we get the actual device
+	__cu(cuDeviceGet(&cuDevice, deviceID));
+
+	__cu(cuDeviceComputeCapability(&SMmajor, &SMminor, deviceID));
+	if (((SMmajor << 4) + SMminor) < 0x30)
+	{
+		PRINTERR("GPU %d does not have NVENC capabilities exiting\n", deviceID);
+		return NV_ENC_ERR_NO_ENCODE_DEVICE;
+	}
+
+	return NV_ENC_SUCCESS;
+}
+
+NVENCSTATUS H264EncoderImpl::ConvertYUVToNV12(EncodeBuffer * pEncodeBuffer, unsigned char * yuv[3], int width, int height)
+{
+	CCudaAutoLock cuLock(m_cuContext);
+	// copy luma
+	CUDA_MEMCPY2D copyParam;
+	memset(&copyParam, 0, sizeof(copyParam));
+	copyParam.dstMemoryType = CU_MEMORYTYPE_DEVICE;
+	copyParam.dstDevice = pEncodeBuffer->stInputBfr.pNV12devPtr;
+	copyParam.dstPitch = pEncodeBuffer->stInputBfr.uNV12Stride;
+	copyParam.srcMemoryType = CU_MEMORYTYPE_HOST;
+	copyParam.srcHost = yuv[0];
+	copyParam.srcPitch = width;
+	copyParam.WidthInBytes = width;
+	copyParam.Height = height;
+	__cu(cuMemcpy2D(&copyParam));
+
+	// copy chroma
+
+	__cu(cuMemcpyHtoD(m_ChromaDevPtr[0], yuv[1], width*height / 4));
+	__cu(cuMemcpyHtoD(m_ChromaDevPtr[1], yuv[2], width*height / 4));
+
+#define BLOCK_X 32
+#define BLOCK_Y 16
+	int chromaHeight = height / 2;
+	int chromaWidth = width / 2;
+	dim3 block(BLOCK_X, BLOCK_Y, 1);
+	dim3 grid((chromaWidth + BLOCK_X - 1) / BLOCK_X, (chromaHeight + BLOCK_Y - 1) / BLOCK_Y, 1);
+#undef BLOCK_Y
+#undef BLOCK_X
+
+	CUdeviceptr dNV12Chroma = (CUdeviceptr)((unsigned char*)pEncodeBuffer->stInputBfr.pNV12devPtr + pEncodeBuffer->stInputBfr.uNV12Stride*height);
+	void *args[8] = { &m_ChromaDevPtr[0], &m_ChromaDevPtr[1], &dNV12Chroma, &chromaWidth, &chromaHeight, &chromaWidth, &chromaWidth, &pEncodeBuffer->stInputBfr.uNV12Stride };
+
+	__cu(cuLaunchKernel(m_cuInterleaveUVFunction, grid.x, grid.y, grid.z,
+		block.x, block.y, block.z,
+		0,
+		NULL, args, NULL));
+	CUresult cuResult = cuStreamQuery(NULL);
+	if (!((cuResult == CUDA_SUCCESS) || (cuResult == CUDA_ERROR_NOT_READY)))
+	{
+		return NV_ENC_ERR_GENERIC;
+	}
+	return NV_ENC_SUCCESS;
 }
 
 int32_t H264EncoderImpl::Encode(const VideoFrame& input_frame,
@@ -822,16 +1198,27 @@ int32_t H264EncoderImpl::Encode(const VideoFrame& input_frame,
 		void* pFrameBuffer = nullptr;
 		int frameSizeInBytes = 0;
 		_NV_ENC_PIC_TYPE frameType;
-		auto texture = input_frame.GetID3D11Texture2D();
-		if (texture == nullptr)
-			return WEBRTC_VIDEO_CODEC_OK;
 
 		// Force a key frame until we send the first one.
 		if (!m_first_frame_sent) force_key_frame = true;
 
 		size_t i_nal = 0;
-		Capture(texture, force_key_frame);
+
+		if (m_d3dDevice)
+		{
+			auto texture = input_frame.GetID3D11Texture2D();
+			if (texture == nullptr)
+				return WEBRTC_VIDEO_CODEC_OK;
+
+			Capture(texture, force_key_frame);
+		}
+		else
+		{
+			Capture(frame_buffer->DataY(), frame_buffer->DataU(), frame_buffer->DataV(), frame_buffer->StrideY(), frame_buffer->StrideU(), frame_buffer->StrideV(), force_key_frame);
+		}
+
 		GetEncodedFrame(&pFrameBuffer, &frameSizeInBytes, &frameType);
+
 		if (frameSizeInBytes < 1 || frameSizeInBytes >= 100000000 || frameType == NV_ENC_PIC_TYPE_SKIPPED || frameType == NV_ENC_PIC_TYPE_UNKNOWN)
 			return WEBRTC_VIDEO_CODEC_OK;
 
diff --git a/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.h b/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.h
index 5f00d28de..efe812f24 100644
--- a/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.h
+++ b/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.h
@@ -17,12 +17,13 @@
 
 #include "webrtc/common_video/h264/h264_bitstream_parser.h"
 #include "webrtc/modules/video_coding/codecs/h264/include/h264.h"
-#include "webrtc/modules/video_coding/codecs/h264/include/NvHWEncoder.h"
+
 #include "webrtc/modules/video_coding/utility/quality_scaler.h"
 #include "third_party/jsoncpp/source/include/json/json.h"
+#include "third_party/openh264/src/codec/api/svc/codec_app_def.h"
 
+#include "third_party/nvencode/inc/NvHWEncoder.h"
 
-#include "third_party/openh264/src/codec/api/svc/codec_app_def.h"
 class ISVCEncoder;
 
 namespace webrtc {
@@ -107,10 +108,29 @@ namespace webrtc {
 typedef struct _EncodeFrameConfig
 {
 	ID3D11Texture2D* pRGBTexture;
+	uint8_t  *yuv[3];
+	uint32_t stride[3];
 	uint32_t width;
 	uint32_t height;
 } EncodeFrameConfig;
 
+class CCudaAutoLock
+{
+private:
+	CUcontext m_pCtx;
+public:
+	CCudaAutoLock(CUcontext pCtx) :m_pCtx(pCtx) { cuCtxPushCurrent(m_pCtx); };
+	~CCudaAutoLock() { CUcontext cuLast = NULL; cuCtxPopCurrent(&cuLast); };
+};
+
+typedef enum
+{
+	NV_ENC_DX9 = 0,
+	NV_ENC_DX11 = 1,
+	NV_ENC_CUDA = 2,
+	NV_ENC_DX10 = 3,
+} NvEncodeDeviceType;
+
 class H264EncoderImpl : public H264Encoder {
  public:
   explicit H264EncoderImpl(const cricket::VideoCodec& codec);
@@ -169,7 +189,13 @@ class H264EncoderImpl : public H264Encoder {
   void GetDefaultNvencodeConfig(EncodeConfig &nvEncodeConfig, Json::Value rootValue);
 
   void Capture(ID3D11Texture2D* frameBuffer, bool forceIntra);
+  void Capture(const uint8_t* yBuffer, const uint8_t* uBuffer, const uint8_t* vBuffer, int yStride, int uStride, int vStride, bool forceIntra);
+
   void GetEncodedFrame(void** buffer, int* size, _NV_ENC_PIC_TYPE* keyFrameType);
+  NVENCSTATUS InitHWEncoder(Json::Value root, const VideoCodec* codec_settings);
+  NVENCSTATUS InitCuda();
+  NVENCSTATUS CheckDeviceNVENCCapability();
+  NVENCSTATUS ConvertYUVToNV12(EncodeBuffer *pEncodeBuffer, unsigned char *yuv[3], int width, int height);
   NVENCSTATUS AllocateIOBuffers(uint32_t uInputWidth, uint32_t uInputHeight);
   NVENCSTATUS Deinitialize();
   NVENCSTATUS ReleaseIOBuffers();
@@ -180,6 +206,26 @@ class H264EncoderImpl : public H264Encoder {
   void ReportInit();
   void ReportError();
 
+  // Nv hw encode
+  CNvHWEncoder*             m_pNvHWEncoder;
+  uint32_t                  m_uEncodeBufferCount;
+  EncodeOutputBuffer		m_stEOSOutputBfr;
+  EncodeBuffer				m_stEncodeBuffer[32];
+  CNvQueue<EncodeBuffer>    m_EncodeBufferQueue;
+  EncodeConfig				m_encodeConfig;
+  bool						m_encoder_hw_capable;
+
+  // CUDA
+  CUcontext             m_cuContext;
+  CUmodule              m_cuModule;
+  CUfunction            m_cuInterleaveUVFunction;
+  CUdeviceptr           m_ChromaDevPtr[2];
+  CUdevice				cuDevice;
+  CUcontext				cuContextCurr;
+  int					deviceCount;
+  int					SMminor;
+  int					SMmajor;
+
   ISVCEncoder* encoder_;
   // Settings that are used by this encoder.
   int width_;
@@ -195,18 +241,13 @@ class H264EncoderImpl : public H264Encoder {
 
   size_t max_payload_size_;
   int32_t number_of_cores_;
-  CNvHWEncoder*             m_pNvHWEncoder;
-  uint32_t                  m_uEncodeBufferCount;
-  EncodeOutputBuffer		m_stEOSOutputBfr;
-  EncodeBuffer				m_stEncodeBuffer[32];
-  CNvQueue<EncodeBuffer>    m_EncodeBufferQueue;
-  EncodeConfig				m_encodeConfig;
   bool						m_encoderInitialized;
   bool						m_use_software_encoding;
   bool						m_first_frame_sent;
 
   EncodedImage encoded_image_;
   std::unique_ptr<uint8_t[]> encoded_image_buffer_;
+  uint8_t *m_yuv[3];
   EncodedImageCallback* encoded_image_callback_;
 
   bool has_reported_init_;
-- 
2.15.0.windows.1

