diff --git a/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.cc b/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.cc
index 5705428a1..9b1710122 100644
--- a/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.cc
+++ b/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.cc
@@ -37,6 +37,8 @@
 #include "webrtc/base/bind.h"
 #include "webrtc/base/asyncinvoker.h"
 
+#define BITSTREAM_BUFFER_SIZE	2 * 1024 * 1024
+
 namespace webrtc {
 
 namespace {
@@ -184,6 +186,7 @@ H264EncoderImpl::H264EncoderImpl(const cricket::VideoCodec& codec)
 	packetization_mode_(H264PacketizationMode::SingleNalUnit),
 	max_payload_size_(0),
 	encoded_image_callback_(nullptr),
+	last_prediction_timestamp_(0),
 	has_reported_init_(false),
 	has_reported_error_(false) {
 	RTC_CHECK(cricket::CodecNamesEq(codec.name, cricket::kH264CodecName));
@@ -320,8 +323,9 @@ int32_t H264EncoderImpl::InitEncode(const VideoCodec* codec_settings,
 
 		// Creates the encoder.
 		m_pNvHWEncoder->CreateEncoder(&m_encodeConfig);
-		m_uEncodeBufferCount = 4;
 
+		// Allocates IO buffers.
+		m_uEncodeBufferCount = 1;
 		AllocateIOBuffers(m_encodeConfig.width, m_encodeConfig.height);
 	}
 
@@ -600,24 +604,8 @@ void H264EncoderImpl::Capture(ID3D11Texture2D* frameBuffer, bool forceIntra)
 	NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
 	EncodeBuffer* pEncodeBuffer = m_EncodeBufferQueue.GetAvailable();
 
-	if (!pEncodeBuffer)
-	{
-		pEncodeBuffer = m_EncodeBufferQueue.GetPending();
-		m_pNvHWEncoder->ProcessOutput(pEncodeBuffer);
-
-		// UnMap the input buffer after frame done
-		if (pEncodeBuffer->stInputBfr.hInputSurface)
-		{
-			nvStatus = m_pNvHWEncoder->NvEncUnmapInputResource(pEncodeBuffer->stInputBfr.hInputSurface);
-			pEncodeBuffer->stInputBfr.hInputSurface = NULL;
-		}
-
-		pEncodeBuffer = m_EncodeBufferQueue.GetAvailable();
-	}
-
 	// Copies the frame buffer to the encode input buffer.
 	m_d3dContext->CopyResource(pEncodeBuffer->stInputBfr.pARGBSurface, frameBuffer);
-	frameBuffer->Release();
 	nvStatus = m_pNvHWEncoder->NvEncMapInputResource(pEncodeBuffer->stInputBfr.nvRegisteredResource, &pEncodeBuffer->stInputBfr.hInputSurface);
 	if (nvStatus != NV_ENC_SUCCESS)
 	{
@@ -635,6 +623,18 @@ void H264EncoderImpl::Capture(ID3D11Texture2D* frameBuffer, bool forceIntra)
 	{
 		return;
 	}
+	else
+	{
+		pEncodeBuffer = m_EncodeBufferQueue.GetPending();
+		m_pNvHWEncoder->ProcessOutput(pEncodeBuffer);
+
+		// UnMap the input buffer after frame done
+		if (pEncodeBuffer->stInputBfr.hInputSurface)
+		{
+			nvStatus = m_pNvHWEncoder->NvEncUnmapInputResource(pEncodeBuffer->stInputBfr.hInputSurface);
+			pEncodeBuffer->stInputBfr.hInputSurface = NULL;
+		}
+	}
 }
 
 NVENCSTATUS H264EncoderImpl::AllocateIOBuffers(uint32_t uInputWidth, uint32_t uInputHeight)
@@ -693,12 +693,12 @@ NVENCSTATUS H264EncoderImpl::AllocateIOBuffers(uint32_t uInputWidth, uint32_t uI
 		m_stEncodeBuffer[i].stInputBfr.pARGBSurface = pVPSurfaces[i];
 
 		// Initializes the output buffer.
-		m_pNvHWEncoder->NvEncCreateBitstreamBuffer(2 * 1024 * 1024, &m_stEncodeBuffer[i].stOutputBfr.hBitstreamBuffer);
-		m_stEncodeBuffer[i].stOutputBfr.dwBitstreamBufferSize = 2 * 1024 * 1024;
+		m_pNvHWEncoder->NvEncCreateBitstreamBuffer(BITSTREAM_BUFFER_SIZE, &m_stEncodeBuffer[i].stOutputBfr.hBitstreamBuffer);
+		m_stEncodeBuffer[i].stOutputBfr.dwBitstreamBufferSize = BITSTREAM_BUFFER_SIZE;
 
 		// Registers for the output event.
 		m_pNvHWEncoder->NvEncRegisterAsyncEvent(&m_stEncodeBuffer[i].stOutputBfr.hOutputEvent);
-		m_stEncodeBuffer[i].stOutputBfr.bWaitOnEvent = true;
+		m_stEncodeBuffer[i].stOutputBfr.bWaitOnEvent = false;
 	}
 
 	m_stEOSOutputBfr.bEOSFlag = TRUE;
@@ -881,7 +881,8 @@ int32_t H264EncoderImpl::Encode(const VideoFrame& input_frame,
 	encoded_image_.ntp_time_ms_ = input_frame.ntp_time_ms();
 	encoded_image_.capture_time_ms_ = input_frame.render_time_ms();
 	encoded_image_.rotation_ = input_frame.rotation();
-	encoded_image_.prediction_timestamp_ = input_frame.prediction_timestamp();
+	encoded_image_.prediction_timestamp_ = last_prediction_timestamp_;
+	last_prediction_timestamp_ = input_frame.prediction_timestamp();
 
 	// Split encoded image up into fragments. This also updates |encoded_image_|.
 	if (m_use_software_encoding)
diff --git a/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.h b/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.h
index 5f00d28de..d94136329 100644
--- a/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.h
+++ b/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.h
@@ -208,6 +208,7 @@ class H264EncoderImpl : public H264Encoder {
   EncodedImage encoded_image_;
   std::unique_ptr<uint8_t[]> encoded_image_buffer_;
   EncodedImageCallback* encoded_image_callback_;
+  int64_t last_prediction_timestamp_;
 
   bool has_reported_init_;
   bool has_reported_error_;
