From fec45bc7ec7147a8151ac59eca1bf4eb32c1be67 Mon Sep 17 00:00:00 2001
From: Andrei Ermilov <anderm@microsoft.com>
Date: Fri, 19 Jan 2018 11:37:17 -0500
Subject: [PATCH] Cuda patch

---
 webrtc/modules/video_coding/BUILD.gn               |   6 +-
 .../video_coding/codecs/h264/h264_encoder_impl.cc  | 664 ++++++++++++++++-----
 .../video_coding/codecs/h264/h264_encoder_impl.h   |  61 +-
 3 files changed, 577 insertions(+), 154 deletions(-)

diff --git a/webrtc/modules/video_coding/BUILD.gn b/webrtc/modules/video_coding/BUILD.gn
index cc193d9ce..58af8bd07 100644
--- a/webrtc/modules/video_coding/BUILD.gn
+++ b/webrtc/modules/video_coding/BUILD.gn
@@ -171,9 +171,9 @@ rtc_static_library("webrtc_h264") {
       "codecs/h264/h264_decoder_impl.h",
       "codecs/h264/h264_encoder_impl.cc",
       "codecs/h264/h264_encoder_impl.h",
-      "codecs/h264/include/NvHWEncoder.h",
-      "codecs/h264/include/nvEncodeAPI.h",
-      "codecs/h264/NvHWEncoder.cc"
+      "../../../third_party/nvencode/inc/NvHWEncoder.h",
+      "../../../third_party/nvencode/src/dynlink_cuda.cpp",
+      "../../../third_party/nvencode/src/NvHWEncoder.cpp"
     ]
     deps += [
       "../../common_video",
diff --git a/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.cc b/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.cc
index 9b1710122..d361b8a4f 100644
--- a/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.cc
+++ b/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.cc
@@ -22,6 +22,11 @@
 #include "third_party/openh264/src/codec/api/svc/codec_def.h"
 #include "third_party/openh264/src/codec/api/svc/codec_ver.h"
 
+#include "third_party/nvencode/inc/nvUtils.h"
+#include "third_party/nvencode/inc/nvFileIO.h"
+#include "third_party/nvencode/inc/helper_string.h"
+#include "third_party/nvencode/inc/dynlink_builtin_types.h"
+
 #include "webrtc/base/checks.h"
 #include "webrtc/base/logging.h"
 #include "webrtc/common_video/libyuv/include/webrtc_libyuv.h"
@@ -38,6 +43,9 @@
 #include "webrtc/base/asyncinvoker.h"
 
 #define BITSTREAM_BUFFER_SIZE	2 * 1024 * 1024
+using namespace std;
+
+#define __cu(a) do { CUresult  ret; if ((ret = (a)) != CUDA_SUCCESS) { fprintf(stderr, "%s has returned CUDA error %d\n", #a, ret); return NV_ENC_ERR_GENERIC;}} while(0)
 
 namespace webrtc {
 
@@ -248,6 +256,28 @@ int32_t H264EncoderImpl::InitEncode(const VideoCodec* codec_settings,
   }
 	RTC_DCHECK(!encoder_);
 
+	if (!m_use_software_encoding)
+	{
+		NVENCSTATUS nvStatus = InitHWEncoder(root, codec_settings);
+		if (nvStatus != NV_ENC_SUCCESS)
+		{
+			// Remove any initialization
+			if (m_pNvHWEncoder)
+			{
+				Deinitialize();
+
+				if (m_pNvHWEncoder)
+				{
+					delete m_pNvHWEncoder;
+					m_pNvHWEncoder = NULL;
+				}
+			}
+
+			// Fallback to software encoding 
+			m_use_software_encoding = true;
+		}
+	}
+	
 	if (m_use_software_encoding)
 	{
 		//codec_settings
@@ -298,36 +328,6 @@ int32_t H264EncoderImpl::InitEncode(const VideoCodec* codec_settings,
 		encoder_->SetOption(ENCODER_OPTION_DATAFORMAT,
 			&video_format);
 	}
-	else
-	{
-		packetization_mode_ = H264PacketizationMode::NonInterleaved;
-		m_first_frame_sent = false;
-
-		rtc::Win32Thread w32_thread;
-		rtc::ThreadManager::Instance()->SetCurrentThread(&w32_thread);
-
-		memset(&m_encodeConfig, 0, sizeof(EncodeConfig));
-
-		GetDefaultNvencodeConfig(m_encodeConfig, root);
-		m_encodeConfig.width = codec_settings->width;
-		m_encodeConfig.height = codec_settings->height;
-		m_pNvHWEncoder = new CNvHWEncoder();
-
-		m_pNvHWEncoder->Initialize((void*)m_d3dDevice, NV_ENC_DEVICE_TYPE_DIRECTX);
-		m_encodeConfig.presetGUID = m_pNvHWEncoder->GetPresetGUID(m_encodeConfig.encoderPreset, m_encodeConfig.codec);
-
-		m_pNvHWEncoder->m_stEncodeConfig.profileGUID = NV_ENC_PRESET_LOW_LATENCY_HQ_GUID;
-
-		//	//H264 level sets maximum bitrate limits.  4.1 supported by almost all mobile devices.
-		m_pNvHWEncoder->m_stEncodeConfig.encodeCodecConfig.h264Config.level = NV_ENC_LEVEL_H264_41;
-
-		// Creates the encoder.
-		m_pNvHWEncoder->CreateEncoder(&m_encodeConfig);
-
-		// Allocates IO buffers.
-		m_uEncodeBufferCount = 1;
-		AllocateIOBuffers(m_encodeConfig.width, m_encodeConfig.height);
-	}
 
   // Initialize encoded image. Default buffer size: size of unencoded data.
   encoded_image_._size =
@@ -375,6 +375,8 @@ NVENCSTATUS H264EncoderImpl::Deinitialize()
 	FlushEncoder();
 	ReleaseIOBuffers();
 	nvStatus = m_pNvHWEncoder->NvEncDestroyEncoder();
+
+	__cu(cuCtxDestroy(m_cuContext));
 	return nvStatus;
 }
 
@@ -637,114 +639,6 @@ void H264EncoderImpl::Capture(ID3D11Texture2D* frameBuffer, bool forceIntra)
 	}
 }
 
-NVENCSTATUS H264EncoderImpl::AllocateIOBuffers(uint32_t uInputWidth, uint32_t uInputHeight)
-{
-	ID3D11Texture2D* pVPSurfaces[16];
-
-	// Initializes the encode buffer queue.
-	m_EncodeBufferQueue.Initialize(m_stEncodeBuffer, m_uEncodeBufferCount);
-
-	// Finds the suitable format for buffer.
-	DXGI_FORMAT format = DXGI_FORMAT_R8G8B8A8_UNORM;
-
-	for (uint32_t i = 0; i < m_uEncodeBufferCount; i++)
-	{
-		// Initializes the input buffer, backed by ID3D11Texture2D*.
-		D3D11_TEXTURE2D_DESC desc = { 0 };
-		desc.ArraySize = 1;
-		desc.Format = format;
-		desc.Width = uInputWidth;
-		desc.Height = uInputHeight;
-		desc.MipLevels = 1;
-		desc.SampleDesc.Count = 1;
-		desc.Usage = D3D11_USAGE_DEFAULT;
-		m_d3dDevice->CreateTexture2D(&desc, nullptr, &pVPSurfaces[i]);
-
-		// Registers the input buffer with NvEnc.
-		m_pNvHWEncoder->NvEncRegisterResource(
-			NV_ENC_INPUT_RESOURCE_TYPE_DIRECTX,
-			(void*)pVPSurfaces[i],
-			uInputWidth,
-			uInputHeight,
-			m_stEncodeBuffer[i].stInputBfr.uARGBStride,
-			&m_stEncodeBuffer[i].stInputBfr.nvRegisteredResource);
-
-		// Maps the buffer format to the relevant NvEnc encoder format
-		switch (format)
-		{
-		case DXGI_FORMAT_B8G8R8A8_UNORM:
-			m_stEncodeBuffer[i].stInputBfr.bufferFmt = NV_ENC_BUFFER_FORMAT_ARGB;
-			break;
-
-		case DXGI_FORMAT_R10G10B10A2_UNORM:
-			if (m_pNvHWEncoder->m_stEncodeConfig.encodeCodecConfig.h264Config.qpPrimeYZeroTransformBypassFlag == 1)
-				m_stEncodeBuffer[i].stInputBfr.bufferFmt = NV_ENC_BUFFER_FORMAT_YUV444_10BIT;
-			else
-				m_stEncodeBuffer[i].stInputBfr.bufferFmt = NV_ENC_BUFFER_FORMAT_YUV420_10BIT;
-			break;
-
-		default:
-			m_stEncodeBuffer[i].stInputBfr.bufferFmt = NV_ENC_BUFFER_FORMAT_ABGR;
-			break;
-		}
-
-		m_stEncodeBuffer[i].stInputBfr.dwWidth = uInputWidth;
-		m_stEncodeBuffer[i].stInputBfr.dwHeight = uInputHeight;
-		m_stEncodeBuffer[i].stInputBfr.pARGBSurface = pVPSurfaces[i];
-
-		// Initializes the output buffer.
-		m_pNvHWEncoder->NvEncCreateBitstreamBuffer(BITSTREAM_BUFFER_SIZE, &m_stEncodeBuffer[i].stOutputBfr.hBitstreamBuffer);
-		m_stEncodeBuffer[i].stOutputBfr.dwBitstreamBufferSize = BITSTREAM_BUFFER_SIZE;
-
-		// Registers for the output event.
-		m_pNvHWEncoder->NvEncRegisterAsyncEvent(&m_stEncodeBuffer[i].stOutputBfr.hOutputEvent);
-		m_stEncodeBuffer[i].stOutputBfr.bWaitOnEvent = false;
-	}
-
-	m_stEOSOutputBfr.bEOSFlag = TRUE;
-
-	// Registers for the output event.
-	m_pNvHWEncoder->NvEncRegisterAsyncEvent(&m_stEOSOutputBfr.hOutputEvent);
-
-	return NV_ENC_SUCCESS;
-}
-
-NVENCSTATUS H264EncoderImpl::ReleaseIOBuffers()
-{
-	for (uint32_t i = 0; i < m_uEncodeBufferCount; i++)
-	{
-		if (m_stEncodeBuffer[i].stInputBfr.pARGBSurface)
-		{
-			m_stEncodeBuffer[i].stInputBfr.pARGBSurface->Release();
-			m_stEncodeBuffer[i].stInputBfr.pARGBSurface = nullptr;
-		}
-
-		m_pNvHWEncoder->NvEncDestroyBitstreamBuffer(m_stEncodeBuffer[i].stOutputBfr.hBitstreamBuffer);
-		m_stEncodeBuffer[i].stOutputBfr.hBitstreamBuffer = NULL;
-
-		m_pNvHWEncoder->NvEncUnregisterAsyncEvent(m_stEncodeBuffer[i].stOutputBfr.hOutputEvent);
-		CloseHandle(m_stEncodeBuffer[i].stOutputBfr.hOutputEvent);
-		m_stEncodeBuffer[i].stOutputBfr.hOutputEvent = NULL;
-	}
-
-	if (m_stEOSOutputBfr.hOutputEvent)
-	{
-		m_pNvHWEncoder->NvEncUnregisterAsyncEvent(m_stEOSOutputBfr.hOutputEvent);
-		CloseHandle(m_stEOSOutputBfr.hOutputEvent);
-		m_stEOSOutputBfr.hOutputEvent = NULL;
-	}
-
-	return NV_ENC_SUCCESS;
-}
-
-// Captures encoded frames from NvEncoder.
-void H264EncoderImpl::GetEncodedFrame(void** buffer, int* size, _NV_ENC_PIC_TYPE* keyFrameType)
-{
-	*buffer = m_pNvHWEncoder->m_lockBitstreamData.bitstreamBufferPtr;
-	*size = m_pNvHWEncoder->m_lockBitstreamData.bitstreamSizeInBytes;
-	*keyFrameType = m_pNvHWEncoder->m_lockBitstreamData.pictureType;
-}
-
 int32_t H264EncoderImpl::Encode(const VideoFrame& input_frame,
 	const CodecSpecificInfo* codec_specific_info,
 	const std::vector<FrameType>* frame_types) {
@@ -822,15 +716,24 @@ int32_t H264EncoderImpl::Encode(const VideoFrame& input_frame,
 		void* pFrameBuffer = nullptr;
 		int frameSizeInBytes = 0;
 		_NV_ENC_PIC_TYPE frameType;
-		auto texture = input_frame.staging_frame_buffer();
-		if (texture == nullptr)
-			return WEBRTC_VIDEO_CODEC_OK;
 
 		// Force a key frame until we send the first one.
 		if (!m_first_frame_sent) force_key_frame = true;
 
+		if (m_d3dDevice)
+		{
+			auto texture = input_frame.staging_frame_buffer();
+			if (texture == nullptr)
+				return WEBRTC_VIDEO_CODEC_OK;
+
+			Capture(texture, force_key_frame);
+		}
+		else
+		{
+			Capture(frame_buffer->DataY(), frame_buffer->DataU(), frame_buffer->DataV(), frame_buffer->StrideY(), frame_buffer->StrideU(), frame_buffer->StrideV(), force_key_frame);
+		}
+
 		size_t i_nal = 0;
-		Capture(texture, force_key_frame);
 		GetEncodedFrame(&pFrameBuffer, &frameSizeInBytes, &frameType);
 		if (frameSizeInBytes < 1 || frameSizeInBytes >= 100000000 || frameType == NV_ENC_PIC_TYPE_SKIPPED || frameType == NV_ENC_PIC_TYPE_UNKNOWN)
 			return WEBRTC_VIDEO_CODEC_OK;
@@ -905,6 +808,483 @@ int32_t H264EncoderImpl::Encode(const VideoFrame& input_frame,
 	return WEBRTC_VIDEO_CODEC_OK;
 }
 
+void H264EncoderImpl::Capture(const uint8_t* yBuffer, const uint8_t* uBuffer, const uint8_t* vBuffer, int yStride, int uStride, int vStride, bool forceIntra)
+{
+	if (!m_pNvHWEncoder)
+		return;
+
+	// Try to process the pending input buffers.
+	NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+	EncodeBuffer* pEncodeBuffer = m_EncodeBufferQueue.GetAvailable();
+
+	if (!pEncodeBuffer)
+	{
+		pEncodeBuffer = m_EncodeBufferQueue.GetPending();
+		m_pNvHWEncoder->ProcessOutput(pEncodeBuffer);
+
+		// UnMap the input buffer after frame done
+		if (pEncodeBuffer->stInputBfr.hInputSurface)
+		{
+			nvStatus = m_pNvHWEncoder->NvEncUnmapInputResource(pEncodeBuffer->stInputBfr.hInputSurface);
+			pEncodeBuffer->stInputBfr.hInputSurface = NULL;
+		}
+
+		pEncodeBuffer = m_EncodeBufferQueue.GetAvailable();
+	}
+
+	EncodeFrameConfig stEncodeFrame;
+	memset(&stEncodeFrame, 0, sizeof(stEncodeFrame));
+	stEncodeFrame.yuv[0] = const_cast<uint8_t*>(yBuffer);
+	stEncodeFrame.yuv[1] = const_cast<uint8_t*>(uBuffer);
+	stEncodeFrame.yuv[2] = const_cast<uint8_t*>(vBuffer);
+
+	stEncodeFrame.stride[0] = yStride;
+	stEncodeFrame.stride[1] = uStride;
+	stEncodeFrame.stride[2] = vStride;
+	stEncodeFrame.width = m_encodeConfig.width;
+	stEncodeFrame.height = m_encodeConfig.height;
+
+	ConvertYUVToNV12(pEncodeBuffer, stEncodeFrame.yuv, m_encodeConfig.width, m_encodeConfig.height);
+
+	// Copies the frame buffer to the encode input buffer.
+	nvStatus = m_pNvHWEncoder->NvEncMapInputResource(pEncodeBuffer->stInputBfr.nvRegisteredResource, &pEncodeBuffer->stInputBfr.hInputSurface);
+	if (nvStatus != NV_ENC_SUCCESS)
+	{
+		PRINTERR("Failed to Map input buffer %p\n", pEncodeBuffer->stInputBfr.hInputSurface);
+		return;
+	}
+
+	NvEncPictureCommand pEncPicCommand;
+	pEncPicCommand.bForceIntraRefresh = forceIntra;
+	pEncPicCommand.bForceIDR = forceIntra;
+	pEncPicCommand.intraRefreshDuration = m_encodeConfig.intraRefreshDuration;
+
+	nvStatus = m_pNvHWEncoder->NvEncEncodeFrame(pEncodeBuffer, forceIntra ? &pEncPicCommand : nullptr, m_encodeConfig.width, m_encodeConfig.height);
+	if (nvStatus != NV_ENC_SUCCESS  && nvStatus != NV_ENC_ERR_NEED_MORE_INPUT)
+	{
+		return;
+	}
+}
+
+NVENCSTATUS H264EncoderImpl::AllocateIOBuffers(uint32_t uInputWidth, uint32_t uInputHeight)
+{
+	// Initializes the encode buffer queue.
+	m_EncodeBufferQueue.Initialize(m_stEncodeBuffer, m_uEncodeBufferCount);
+
+	if (m_d3dDevice)
+	{
+		ID3D11Texture2D* pVPSurfaces[16];
+
+		// Finds the suitable format for buffer.
+		DXGI_FORMAT format = DXGI_FORMAT_R8G8B8A8_UNORM;
+
+		for (uint32_t i = 0; i < m_uEncodeBufferCount; i++)
+		{
+			// Initializes the input buffer, backed by ID3D11Texture2D*.
+			D3D11_TEXTURE2D_DESC desc = { 0 };
+			desc.ArraySize = 1;
+			desc.Format = format;
+			desc.Width = uInputWidth;
+			desc.Height = uInputHeight;
+			desc.MipLevels = 1;
+			desc.SampleDesc.Count = 1;
+			desc.Usage = D3D11_USAGE_DEFAULT;
+			m_d3dDevice->CreateTexture2D(&desc, nullptr, &pVPSurfaces[i]);
+
+			// Registers the input buffer with NvEnc.
+			m_pNvHWEncoder->NvEncRegisterResource(
+				NV_ENC_INPUT_RESOURCE_TYPE_DIRECTX,
+				(void*)pVPSurfaces[i],
+				uInputWidth,
+				uInputHeight,
+				m_stEncodeBuffer[i].stInputBfr.uARGBStride,
+				&m_stEncodeBuffer[i].stInputBfr.nvRegisteredResource);
+
+			// Maps the buffer format to the relevant NvEnc encoder format
+			switch (format)
+			{
+			case DXGI_FORMAT_B8G8R8A8_UNORM:
+				m_stEncodeBuffer[i].stInputBfr.bufferFmt = NV_ENC_BUFFER_FORMAT_ARGB;
+				break;
+
+			case DXGI_FORMAT_R10G10B10A2_UNORM:
+				if (m_pNvHWEncoder->m_stEncodeConfig.encodeCodecConfig.h264Config.qpPrimeYZeroTransformBypassFlag == 1)
+					m_stEncodeBuffer[i].stInputBfr.bufferFmt = NV_ENC_BUFFER_FORMAT_YUV444_10BIT;
+				else
+					m_stEncodeBuffer[i].stInputBfr.bufferFmt = NV_ENC_BUFFER_FORMAT_YUV420_10BIT;
+				break;
+
+			default:
+				m_stEncodeBuffer[i].stInputBfr.bufferFmt = NV_ENC_BUFFER_FORMAT_ABGR;
+				break;
+			}
+
+			m_stEncodeBuffer[i].stInputBfr.dwWidth = uInputWidth;
+			m_stEncodeBuffer[i].stInputBfr.dwHeight = uInputHeight;
+			m_stEncodeBuffer[i].stInputBfr.pARGBSurface = pVPSurfaces[i];
+
+			// Initializes the output buffer.
+			m_pNvHWEncoder->NvEncCreateBitstreamBuffer(BITSTREAM_BUFFER_SIZE, &m_stEncodeBuffer[i].stOutputBfr.hBitstreamBuffer);
+			m_stEncodeBuffer[i].stOutputBfr.dwBitstreamBufferSize = BITSTREAM_BUFFER_SIZE;
+
+			// Registers for the output event.
+			m_pNvHWEncoder->NvEncRegisterAsyncEvent(&m_stEncodeBuffer[i].stOutputBfr.hOutputEvent);
+			m_stEncodeBuffer[i].stOutputBfr.bWaitOnEvent = false;
+		}
+
+		m_stEOSOutputBfr.bEOSFlag = TRUE;
+
+		// Registers for the output event.
+		m_pNvHWEncoder->NvEncRegisterAsyncEvent(&m_stEOSOutputBfr.hOutputEvent);
+	}
+	else
+	{
+		NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+		CCudaAutoLock cuLock(m_cuContext);
+
+		__cu(cuMemAlloc(&m_ChromaDevPtr[0], uInputWidth*uInputHeight / 4));
+		__cu(cuMemAlloc(&m_ChromaDevPtr[1], uInputWidth*uInputHeight / 4));
+
+		__cu(cuMemAllocHost((void **)&m_yuv[0], uInputWidth*uInputHeight));
+		__cu(cuMemAllocHost((void **)&m_yuv[1], uInputWidth*uInputHeight / 4));
+		__cu(cuMemAllocHost((void **)&m_yuv[2], uInputWidth*uInputHeight / 4));
+
+		for (uint32_t i = 0; i < m_uEncodeBufferCount; i++)
+		{
+			__cu(cuMemAllocPitch(&m_stEncodeBuffer[i].stInputBfr.pNV12devPtr, (size_t *)&m_stEncodeBuffer[i].stInputBfr.uNV12Stride, uInputWidth, uInputHeight * 3 / 2, 16));
+
+			nvStatus = m_pNvHWEncoder->NvEncRegisterResource(NV_ENC_INPUT_RESOURCE_TYPE_CUDADEVICEPTR, (void*)m_stEncodeBuffer[i].stInputBfr.pNV12devPtr,
+				uInputWidth, uInputHeight, m_stEncodeBuffer[i].stInputBfr.uNV12Stride, &m_stEncodeBuffer[i].stInputBfr.nvRegisteredResource);
+			if (nvStatus != NV_ENC_SUCCESS)
+				return nvStatus;
+
+			m_stEncodeBuffer[i].stInputBfr.bufferFmt = NV_ENC_BUFFER_FORMAT_NV12_PL;
+			m_stEncodeBuffer[i].stInputBfr.dwWidth = uInputWidth;
+			m_stEncodeBuffer[i].stInputBfr.dwHeight = uInputHeight;
+
+			nvStatus = m_pNvHWEncoder->NvEncCreateBitstreamBuffer(BITSTREAM_BUFFER_SIZE, &m_stEncodeBuffer[i].stOutputBfr.hBitstreamBuffer);
+			if (nvStatus != NV_ENC_SUCCESS)
+				return nvStatus;
+			m_stEncodeBuffer[i].stOutputBfr.dwBitstreamBufferSize = BITSTREAM_BUFFER_SIZE;
+
+			if (m_encodeConfig.enableAsyncMode)
+			{
+				nvStatus = m_pNvHWEncoder->NvEncRegisterAsyncEvent(&m_stEncodeBuffer[i].stOutputBfr.hOutputEvent);
+				if (nvStatus != NV_ENC_SUCCESS)
+					return nvStatus;
+				m_stEncodeBuffer[i].stOutputBfr.bWaitOnEvent = false;
+			}
+			else
+				m_stEncodeBuffer[i].stOutputBfr.hOutputEvent = NULL;
+		}
+
+		m_stEOSOutputBfr.bEOSFlag = TRUE;
+		if (m_encodeConfig.enableAsyncMode)
+		{
+			nvStatus = m_pNvHWEncoder->NvEncRegisterAsyncEvent(&m_stEOSOutputBfr.hOutputEvent);
+			if (nvStatus != NV_ENC_SUCCESS)
+				return nvStatus;
+		}
+		else
+			m_stEOSOutputBfr.hOutputEvent = NULL;
+	}
+
+	return NV_ENC_SUCCESS;
+}
+
+NVENCSTATUS H264EncoderImpl::ReleaseIOBuffers()
+{
+	if (m_d3dDevice)
+	{
+		for (uint32_t i = 0; i < m_uEncodeBufferCount; i++)
+		{
+			if (m_stEncodeBuffer[i].stInputBfr.pARGBSurface)
+			{
+				m_stEncodeBuffer[i].stInputBfr.pARGBSurface->Release();
+				m_stEncodeBuffer[i].stInputBfr.pARGBSurface = nullptr;
+			}
+
+			m_pNvHWEncoder->NvEncDestroyBitstreamBuffer(m_stEncodeBuffer[i].stOutputBfr.hBitstreamBuffer);
+			m_stEncodeBuffer[i].stOutputBfr.hBitstreamBuffer = NULL;
+
+			m_pNvHWEncoder->NvEncUnregisterAsyncEvent(m_stEncodeBuffer[i].stOutputBfr.hOutputEvent);
+			CloseHandle(m_stEncodeBuffer[i].stOutputBfr.hOutputEvent);
+			m_stEncodeBuffer[i].stOutputBfr.hOutputEvent = NULL;
+		}
+
+		if (m_stEOSOutputBfr.hOutputEvent)
+		{
+			m_pNvHWEncoder->NvEncUnregisterAsyncEvent(m_stEOSOutputBfr.hOutputEvent);
+			CloseHandle(m_stEOSOutputBfr.hOutputEvent);
+			m_stEOSOutputBfr.hOutputEvent = NULL;
+		}
+	}
+	else
+	{
+		NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+
+		CCudaAutoLock cuLock(m_cuContext);
+
+		for (int i = 0; i < 2; i++)
+		{
+			if (m_ChromaDevPtr[i])
+			{
+				cuMemFree(m_ChromaDevPtr[i]);
+			}
+		}
+
+		for (int i = 0; i < 3; i++)
+		{
+			if (m_yuv[i])
+			{
+				cuMemFreeHost(m_yuv[i]);
+			}
+		}
+
+		for (uint32_t i = 0; i < m_uEncodeBufferCount; i++)
+		{
+			nvStatus = m_pNvHWEncoder->NvEncUnregisterResource(m_stEncodeBuffer[i].stInputBfr.nvRegisteredResource);
+			if (nvStatus != NV_ENC_SUCCESS)
+				return nvStatus;
+
+			cuMemFree(m_stEncodeBuffer[i].stInputBfr.pNV12devPtr);
+
+			m_pNvHWEncoder->NvEncDestroyBitstreamBuffer(m_stEncodeBuffer[i].stOutputBfr.hBitstreamBuffer);
+			m_stEncodeBuffer[i].stOutputBfr.hBitstreamBuffer = NULL;
+
+			if (m_encodeConfig.enableAsyncMode)
+			{
+				m_pNvHWEncoder->NvEncUnregisterAsyncEvent(m_stEncodeBuffer[i].stOutputBfr.hOutputEvent);
+				nvCloseFile(m_stEncodeBuffer[i].stOutputBfr.hOutputEvent);
+				m_stEncodeBuffer[i].stOutputBfr.hOutputEvent = NULL;
+			}
+		}
+
+		if (m_stEOSOutputBfr.hOutputEvent)
+		{
+			if (m_encodeConfig.enableAsyncMode)
+			{
+				m_pNvHWEncoder->NvEncUnregisterAsyncEvent(m_stEOSOutputBfr.hOutputEvent);
+				nvCloseFile(m_stEOSOutputBfr.hOutputEvent);
+				m_stEOSOutputBfr.hOutputEvent = NULL;
+			}
+		}
+	}
+	
+	return NV_ENC_SUCCESS;
+}
+
+// Captures encoded frames from NvEncoder.
+void H264EncoderImpl::GetEncodedFrame(void** buffer, int* size, _NV_ENC_PIC_TYPE* keyFrameType)
+{
+	*buffer = m_pNvHWEncoder->lockBitstreamData.bitstreamBufferPtr;
+	*size = m_pNvHWEncoder->lockBitstreamData.bitstreamSizeInBytes;
+	*keyFrameType = m_pNvHWEncoder->lockBitstreamData.pictureType;
+}
+
+NVENCSTATUS H264EncoderImpl::InitHWEncoder(Json::Value root, const VideoCodec* codec_settings)
+{
+	NVENCSTATUS nvStatus = NV_ENC_SUCCESS;
+	nvStatus = CheckDeviceNVENCCapability();
+	if (nvStatus != NV_ENC_SUCCESS)
+		return nvStatus;
+
+	packetization_mode_ = H264PacketizationMode::NonInterleaved;
+	m_first_frame_sent = false;
+
+	rtc::Win32Thread w32_thread;
+	rtc::ThreadManager::Instance()->SetCurrentThread(&w32_thread);
+
+	memset(&m_encodeConfig, 0, sizeof(EncodeConfig));
+
+	GetDefaultNvencodeConfig(m_encodeConfig, root);
+	m_encodeConfig.width = codec_settings->width;
+	m_encodeConfig.height = codec_settings->height;
+	m_encodeConfig.enableAsyncMode = true;
+	m_pNvHWEncoder = new CNvHWEncoder();
+
+	if (m_d3dDevice)
+	{
+		nvStatus = m_pNvHWEncoder->Initialize((void*)m_d3dDevice, NV_ENC_DEVICE_TYPE_DIRECTX);
+		if (nvStatus != NV_ENC_SUCCESS)
+			return nvStatus;
+	}
+	else
+	{
+		// initialize Cuda
+		nvStatus = InitCuda();
+		if (nvStatus != NV_ENC_SUCCESS)
+			return nvStatus;
+
+		nvStatus = m_pNvHWEncoder->Initialize((void*)m_cuContext, NV_ENC_DEVICE_TYPE_CUDA);
+		if (nvStatus != NV_ENC_SUCCESS)
+			return nvStatus;
+	}
+
+	m_encodeConfig.presetGUID = m_pNvHWEncoder->GetPresetGUID(m_encodeConfig.encoderPreset, m_encodeConfig.codec);
+	m_pNvHWEncoder->m_stEncodeConfig.profileGUID = NV_ENC_PRESET_LOW_LATENCY_HQ_GUID;
+
+	//H264 level sets maximum bitrate limits.  4.1 supported by almost all mobile devices.
+	m_pNvHWEncoder->m_stEncodeConfig.encodeCodecConfig.h264Config.level = NV_ENC_LEVEL_H264_41;
+
+	// Creates the encoder.
+	nvStatus = m_pNvHWEncoder->CreateEncoder(&m_encodeConfig);
+	if (nvStatus != NV_ENC_SUCCESS)
+		return nvStatus;
+
+	m_uEncodeBufferCount = 1;
+
+	nvStatus = AllocateIOBuffers(m_encodeConfig.width, m_encodeConfig.height);
+	return nvStatus;
+}
+
+NVENCSTATUS H264EncoderImpl::InitCuda()
+{
+	CUresult cuResult = CUDA_SUCCESS;
+	// Create the CUDA Context and Pop the current one
+	__cu(cuCtxCreate(&m_cuContext, 0, cuDevice));
+
+	// in this branch we use compilation with parameters
+	const unsigned int jitNumOptions = 3;
+	CUjit_option *jitOptions = new CUjit_option[jitNumOptions];
+	void **jitOptVals = new void *[jitNumOptions];
+
+	// set up size of compilation log buffer
+	jitOptions[0] = CU_JIT_INFO_LOG_BUFFER_SIZE_BYTES;
+	int jitLogBufferSize = 1024;
+	jitOptVals[0] = (void *)(size_t)jitLogBufferSize;
+
+	// set up pointer to the compilation log buffer
+	jitOptions[1] = CU_JIT_INFO_LOG_BUFFER;
+	char *jitLogBuffer = new char[jitLogBufferSize];
+	jitOptVals[1] = jitLogBuffer;
+
+	// set up pointer to set the Maximum # of registers for a particular kernel
+	jitOptions[2] = CU_JIT_MAX_REGISTERS;
+	int jitRegCount = 32;
+	jitOptVals[2] = (void *)(size_t)jitRegCount;
+
+	string ptx_source, ptx_file_name;
+#if defined(__x86_64) || defined(AMD64) || defined(_M_AMD64) || defined(__aarch64__)
+	ptx_file_name = "preproc64_cuda.ptx";
+#else
+	ptx_file_name = "preproc32_cuda.ptx";
+#endif
+	auto path = ExePath(ptx_file_name);
+	FILE *fp = fopen(path.data(), "rb");
+	if (!fp)
+	{
+		return NV_ENC_ERR_INVALID_PARAM;
+	}
+
+	fseek(fp, 0, SEEK_END);
+	int file_size = ftell(fp);
+	char *buf = new char[file_size + 1];
+	fseek(fp, 0, SEEK_SET);
+	fread(buf, sizeof(char), file_size, fp);
+	fclose(fp);
+	buf[file_size] = '\0';
+	ptx_source = buf;
+	delete[] buf;
+
+	cuResult = cuModuleLoadDataEx(&m_cuModule, ptx_source.c_str(), jitNumOptions, jitOptions, (void **)jitOptVals);
+	if (cuResult != CUDA_SUCCESS)
+	{
+		return NV_ENC_ERR_OUT_OF_MEMORY;
+	}
+
+	delete[] jitOptions;
+	delete[] jitOptVals;
+	delete[] jitLogBuffer;
+
+	__cu(cuModuleGetFunction(&m_cuInterleaveUVFunction, m_cuModule, "InterleaveUV"));
+
+	__cu(cuCtxPopCurrent(&cuContextCurr));
+	return NV_ENC_SUCCESS;
+}
+
+NVENCSTATUS H264EncoderImpl::CheckDeviceNVENCCapability()
+{
+	int deviceID = 0;
+	cuDevice = 0;
+	deviceCount = 0;
+	SMminor = 0;
+	SMmajor = 0;
+
+#if defined(WIN32) || defined(_WIN32) || defined(WIN64) || defined(_WIN64)
+	typedef HMODULE CUDADRIVER;
+#else
+	typedef void *CUDADRIVER;
+#endif
+	CUDADRIVER hHandleDriver = 0;
+
+	// CUDA interfaces
+	__cu(cuInit(0, __CUDA_API_VERSION, hHandleDriver));
+
+	__cu(cuDeviceGetCount(&deviceCount));
+	if (deviceCount == 0)
+	{
+		return NV_ENC_ERR_NO_ENCODE_DEVICE;
+	}
+
+	// Now we get the actual device
+	__cu(cuDeviceGet(&cuDevice, deviceID));
+
+	__cu(cuDeviceComputeCapability(&SMmajor, &SMminor, deviceID));
+	if (((SMmajor << 4) + SMminor) < 0x30)
+	{
+		PRINTERR("GPU %d does not have NVENC capabilities exiting\n", deviceID);
+		return NV_ENC_ERR_NO_ENCODE_DEVICE;
+	}
+
+	return NV_ENC_SUCCESS;
+}
+
+NVENCSTATUS H264EncoderImpl::ConvertYUVToNV12(EncodeBuffer * pEncodeBuffer, unsigned char * yuv[3], int width, int height)
+{
+	CCudaAutoLock cuLock(m_cuContext);
+	// copy luma
+	CUDA_MEMCPY2D copyParam;
+	memset(&copyParam, 0, sizeof(copyParam));
+	copyParam.dstMemoryType = CU_MEMORYTYPE_DEVICE;
+	copyParam.dstDevice = pEncodeBuffer->stInputBfr.pNV12devPtr;
+	copyParam.dstPitch = pEncodeBuffer->stInputBfr.uNV12Stride;
+	copyParam.srcMemoryType = CU_MEMORYTYPE_HOST;
+	copyParam.srcHost = yuv[0];
+	copyParam.srcPitch = width;
+	copyParam.WidthInBytes = width;
+	copyParam.Height = height;
+	__cu(cuMemcpy2D(&copyParam));
+
+	// copy chroma
+
+	__cu(cuMemcpyHtoD(m_ChromaDevPtr[0], yuv[1], width*height / 4));
+	__cu(cuMemcpyHtoD(m_ChromaDevPtr[1], yuv[2], width*height / 4));
+
+#define BLOCK_X 32
+#define BLOCK_Y 16
+	int chromaHeight = height / 2;
+	int chromaWidth = width / 2;
+	dim3 block(BLOCK_X, BLOCK_Y, 1);
+	dim3 grid((chromaWidth + BLOCK_X - 1) / BLOCK_X, (chromaHeight + BLOCK_Y - 1) / BLOCK_Y, 1);
+#undef BLOCK_Y
+#undef BLOCK_X
+
+	CUdeviceptr dNV12Chroma = (CUdeviceptr)((unsigned char*)pEncodeBuffer->stInputBfr.pNV12devPtr + pEncodeBuffer->stInputBfr.uNV12Stride*height);
+	void *args[8] = { &m_ChromaDevPtr[0], &m_ChromaDevPtr[1], &dNV12Chroma, &chromaWidth, &chromaHeight, &chromaWidth, &chromaWidth, &pEncodeBuffer->stInputBfr.uNV12Stride };
+
+	__cu(cuLaunchKernel(m_cuInterleaveUVFunction, grid.x, grid.y, grid.z,
+		block.x, block.y, block.z,
+		0,
+		NULL, args, NULL));
+	CUresult cuResult = cuStreamQuery(NULL);
+	if (!((cuResult == CUDA_SUCCESS) || (cuResult == CUDA_ERROR_NOT_READY)))
+	{
+		return NV_ENC_ERR_GENERIC;
+	}
+	return NV_ENC_SUCCESS;
+}
+
 const char* H264EncoderImpl::ImplementationName() const {
   return "OpenH264";
 }
diff --git a/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.h b/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.h
index d94136329..484c7e27d 100644
--- a/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.h
+++ b/webrtc/modules/video_coding/codecs/h264/h264_encoder_impl.h
@@ -17,12 +17,13 @@
 
 #include "webrtc/common_video/h264/h264_bitstream_parser.h"
 #include "webrtc/modules/video_coding/codecs/h264/include/h264.h"
-#include "webrtc/modules/video_coding/codecs/h264/include/NvHWEncoder.h"
+#include "webrtc/modules/video_coding/utility/quality_scaler.h"
+
 #include "webrtc/modules/video_coding/utility/quality_scaler.h"
 #include "third_party/jsoncpp/source/include/json/json.h"
-
-
 #include "third_party/openh264/src/codec/api/svc/codec_app_def.h"
+
+#include "third_party/nvencode/inc/NvHWEncoder.h"
 class ISVCEncoder;
 
 namespace webrtc {
@@ -107,10 +108,29 @@ namespace webrtc {
 typedef struct _EncodeFrameConfig
 {
 	ID3D11Texture2D* pRGBTexture;
+	uint8_t  *yuv[3];
+	uint32_t stride[3];
 	uint32_t width;
 	uint32_t height;
 } EncodeFrameConfig;
 
+class CCudaAutoLock
+{
+private:
+	CUcontext m_pCtx;
+public:
+	CCudaAutoLock(CUcontext pCtx) :m_pCtx(pCtx) { cuCtxPushCurrent(m_pCtx); };
+	~CCudaAutoLock() { CUcontext cuLast = NULL; cuCtxPopCurrent(&cuLast); };
+};
+
+typedef enum
+{
+	NV_ENC_DX9 = 0,
+	NV_ENC_DX11 = 1,
+	NV_ENC_CUDA = 2,
+	NV_ENC_DX10 = 3,
+} NvEncodeDeviceType;
+
 class H264EncoderImpl : public H264Encoder {
  public:
   explicit H264EncoderImpl(const cricket::VideoCodec& codec);
@@ -169,7 +189,14 @@ class H264EncoderImpl : public H264Encoder {
   void GetDefaultNvencodeConfig(EncodeConfig &nvEncodeConfig, Json::Value rootValue);
 
   void Capture(ID3D11Texture2D* frameBuffer, bool forceIntra);
+
   void GetEncodedFrame(void** buffer, int* size, _NV_ENC_PIC_TYPE* keyFrameType);
+  void Capture(const uint8_t* yBuffer, const uint8_t* uBuffer, const uint8_t* vBuffer, int yStride, int uStride, int vStride, bool forceIntra);
+
+  NVENCSTATUS InitHWEncoder(Json::Value root, const VideoCodec* codec_settings);
+  NVENCSTATUS InitCuda();
+  NVENCSTATUS CheckDeviceNVENCCapability();
+  NVENCSTATUS ConvertYUVToNV12(EncodeBuffer *pEncodeBuffer, unsigned char *yuv[3], int width, int height);
   NVENCSTATUS AllocateIOBuffers(uint32_t uInputWidth, uint32_t uInputHeight);
   NVENCSTATUS Deinitialize();
   NVENCSTATUS ReleaseIOBuffers();
@@ -180,6 +207,26 @@ class H264EncoderImpl : public H264Encoder {
   void ReportInit();
   void ReportError();
 
+  // Nv hw encode
+  CNvHWEncoder*             m_pNvHWEncoder;
+  uint32_t                  m_uEncodeBufferCount;
+  EncodeOutputBuffer		m_stEOSOutputBfr;
+  EncodeBuffer				m_stEncodeBuffer[32];
+  CNvQueue<EncodeBuffer>    m_EncodeBufferQueue;
+  EncodeConfig				m_encodeConfig;
+  bool						m_encoder_hw_capable;
+
+  // CUDA
+  CUcontext             m_cuContext;
+  CUmodule              m_cuModule;
+  CUfunction            m_cuInterleaveUVFunction;
+  CUdeviceptr           m_ChromaDevPtr[2];
+  CUdevice				cuDevice;
+  CUcontext				cuContextCurr;
+  int					deviceCount;
+  int					SMminor;
+  int					SMmajor;
+
   ISVCEncoder* encoder_;
   // Settings that are used by this encoder.
   int width_;
@@ -195,18 +242,14 @@ class H264EncoderImpl : public H264Encoder {
 
   size_t max_payload_size_;
   int32_t number_of_cores_;
-  CNvHWEncoder*             m_pNvHWEncoder;
-  uint32_t                  m_uEncodeBufferCount;
-  EncodeOutputBuffer		m_stEOSOutputBfr;
-  EncodeBuffer				m_stEncodeBuffer[32];
-  CNvQueue<EncodeBuffer>    m_EncodeBufferQueue;
-  EncodeConfig				m_encodeConfig;
+
   bool						m_encoderInitialized;
   bool						m_use_software_encoding;
   bool						m_first_frame_sent;
 
   EncodedImage encoded_image_;
   std::unique_ptr<uint8_t[]> encoded_image_buffer_;
+  uint8_t *m_yuv[3];
   EncodedImageCallback* encoded_image_callback_;
   int64_t last_prediction_timestamp_;
 
-- 
2.11.0.windows.3

